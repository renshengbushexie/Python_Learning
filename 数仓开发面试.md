# 面试宝典


# Oracle基础查询&函数问题

# 1. MySQL与Oracle的区别？

注：主要考察是否真的有用过这两个数据库，以及平常你是否属于善于学习或者总结的，以下可以挑几点去回答。

答：1、本质区别是mysql开源免费的，oracle是收费的  
2、数据库的安全性，mysql使用三个参数来验证用户，即用户名、密码和位置，而oracle多使用了很多功能，增加配置文件、本地身份验证、外部身份验证等  
3. oracle 多了 PL/SQL 的编程语言提供了更大的灵活性，用于报表输出和变量定义。  
4、对事务的提交，mysql是自动提交的，oracle默认不自动提交，需要手动提交或者写commit指令或者点击commit按钮  
5、分页查询，mysql是直接用limit实现分页，oracle则是需要伪列rownum和嵌套查询实现  
6、字符类型的比较，MySQL具有CHAR和VARCHAR，最大长度允许为65535字节（CHAR最多可以为255字节，VARCHAR为65535字节），Oracle支持四种字符类型最大限制是4000个字节  
7、模糊查询MySQL里用字段名like‘%字符串%’，Oracle里也可以用字段名like‘%字符串%’但这种方法不能使用索引，速度不快，用字符串比较函数instr(字段名，‘字符串’）>0会得到更精确的查找结果  
8、MySQL 中有 2 种日期格式 DATE 和 TIME, Oracle 只有一种日期格式 DATE。  
9、空字符的处理，MySQL的非空字段也有空的内容：Oracle里定义了非空字段就不容许有空的内容。按MySQL的NOT NULL来定义Oracle表结构，导数据的时候会产生错误。因此导数据时要对空字符进行判断，如果为NULL或空字符，需要把它改成一个空格的字符串。

# 2. 常用函数有哪些？

注：了解函数掌握情况,来判断代码基础。如果一个一个说,除非自己有顺序。所以,不要说具体的,说分类。每一类说一两个就可以

答：大致用过的函数，一般就那几类。比如聚合函数 sum、avg、max、min、count，字符函数 substr、replace、length、round，日期函数 add Months、last_day，数字函数 mod、round、还有这个 case when 、nvl, 还有分析函数，主要做排序和同比环比的。

# 3. 截取字符串的函数有哪些？去空格的函数有哪些

答：截取substr,去空格：trim（两端）ltrim（左）rtrim（右）替换replace（中间的空格）

# 4. 给一个日期，找到上个季度的最后一天

注：简单的考察日期函数的使用，逻辑关系的判断。上季度最后一天=本季度第一天-1天答：我的思路是这样的，先找到本季度的第一天，再减一天就可以了，然后本季度的第一天就可以用trunc对这个日期按季度截断，具体函数式trunc（日期，‘Q'）

# 5. 把字符串的某一个特定数值或字符的位置找出来

答：instr(str,查找内容，开始位置，第几次出现）

6.union和unionall的区别答：都是将两个结果集合并为一个，Union将会按照字段的顺序进行排序筛选掉重复的记录，unionall则不去重

7.truncate和delete的区别答：truncate是DDL语句，一般用于清空全表，不能回滚，效率比较高，delete是DML语句，一般用于删除部分数据，也可以删全表数据，需要提交，也可以回滚的，效率比较低。（truncate为什么效率高？清空数据的方式是把数据文件和日志文件删除，delete将删除的数据及删除的动作写入日志文件）

8. delete 删除数据，commit 提交后，如何找回？答：delete 删除数据之后，默认会将数据放在 oracle 对应的表格的回收站中。（oracle 的闪回机制 flush）

9.select语句查询顺序答：from》on》join》where》groupby》having》select》orderby

10. Oracle where 子句中多个 and 和 or 时，执行顺序是怎么样的？三个逻辑运算符优先级：NOT>AND>OR

11.降序排序空值排在前面还是后面？最前（空值默认最大）可以在最后加nullslast/first改变

12. where 和 having 的区别
    答：where 是在分组前筛选，主要筛选表中原来的数据，having 在分组后筛选，主要筛选一些统计值

13.知道varchar与varchar2的区别吗？空值怎么转换及区别？  
1)varchar2所有字符都占两字节处理(一般情况下)，varchar只对汉字和全角等字符占两字节，数字，英文字符等都是一个字节；  
2)VARCHAR2把空字符串等同于null处理，而varchar仍按照空字符串处理；

14.笛卡尔积是什么

答：比如A表0条记录，B表5条记录，.笛卡尔积就是50条记录

# 15. 遇到重复数据，你们项目如何去重，去重为什么用 group bay 而不是 distinct？

答：一般需要去重查询的，都是使用groupby，如果需要删除的，一般都是groupby查询出来，然后插入新的表，清空原表，再插入回来。因为groupby效率会高一些。

# 16. 数据库表重复数据删除

答：可以使用ROWID，查询相同记录中最大或最小的ROWID，然后ROWID不在这些中的就都删除，Delete from 表 where rowid not in（select max (rowid) from 表 group by 需求字段）

# 17. 去重的方法，分别有哪几种（4种），哪种性能比较好

答：查询两次用union连接去重、distinct直接去重、groupby去重、row_number排序后取序号为1的。groupby的性能是最好的，union的性能是最差的，因为union是先排序后去重的

# 18. 连接方式有哪些？有什么区别？

答：表的连接方式主要有4种，内连接，左外连接，右外连接，全外连接。都是通过关联条件关联，关联上的数据都会保留，内连接关联不上的数据是都舍去，左外连接关联不上的数据都是保留左表，右表舍去，右外连接关联不上的数据是保留右表，左表舍去，全外连接是关联不上的数据两个表分表保留，不足的值用空值填充。

# 19. 在左连接里面，如果使用 and 筛选和用 where 筛选，区别是什么？

答：（如果是两张表之间关联）最终结果是一样的，但过程有区别，条件放在 and 后面，是进行筛选在进行连接，条件放在 where 后面是连接之后对结果集进行筛选，两者之间 and 性能好一些，因为已经提前过滤了不需要的数据量。

# 20. 非等值连接是什么

答：连接条件中的关系不是等值的。平时工作里没有怎么接触

# 21. 左外连接发散问题

答：数据发散一般都是因为关联条件某一边或两边的值都不唯一，就是关联条件是一对多或者多对多。需要对多的一边进行去重再关联，也有可能是缺少了关联的条件。

# 22. 关于rowid和rownum的理解

答：rownum 是伪列，不是表中真是存在的列，相当于行号，每一行标记一个数字，常用于取前几行，做分页的。rowid 是每一行对应的物理地址，每一行都是唯一的，常用于删除重复数据。

23. 知道怎么授权吗？取消授权呢？

24. 知道怎么授权吗？取消授权呢？注：常用的系统权限集合有以下三个：CONNECT(基本的连接)，RESOURCE(程序开发)，DBA(数据库管理)

答：1)GRANT 授予权限：GRANT CONNECT/RESOURCE TO 用户名；

(2)REVOKE 回收权限：REVOKE CONNECT/RESOURCE FROM用户名；

24. Oracle 分页查询该怎么查、怎么查询任意的第几行

答：使用伪列中的 rownum，rownum 只能<或者<=。也可以用子查询查 rownum，给个别名，外层查询再用别名字段做判断。

25. 可以开窗的函数有哪些

答：聚合max、sum这些，偏移lead、lag，排序row_number、rank、dense_rank这些

26.那你用分析函数都用在自己的项目哪一块里面

答：主要是涉及占比或者排序这块，比如销售进度，或者占到当前交易总额5%的记录，或者用同比环比，用到偏移这样

27. 排序的用法和区别

答：排序函数()over（partition by... order by...）排序函数主要有rownumber，连续不重复34567，rank重复不连续446，dense_rank连续且重复3333444

28. sum() over(partition by *** order by *** desc) 加不加 order by 有什么区别？

答：有 order by 是依次累加，没有 order by 是全量累加

29. 偏移函数的用途

答：偏移函数有向上偏移lead和向下偏移lag，主要用于做同比和环比，其实类似的就是，需要同一列中的值互相比较或者计算的时候，都可以使用

30. 行转列，列转行

答：行转列可以使用连表、偏移、case when、decode；列转行可以使用union all

31. 用过什么类型的表？

答：oracle的普通表，视图，同义词，分区表，hive的，我知道有内部表，外部表，分区表。

32. 数据仓库是用什么方法来保存历史数据的？

数仓仓库保存历史数据通常是在 dw 层或者是 hdw 层创建拉链表（缓慢变化维）来保存历史数据，每次更新数据都会通过 update 进行逻辑删除，然后将最新的数据插入到拉链表中，并将历史数据更新成无效状态，将最新的数据更新为有效状态

33. 拉链表的里核心的几个字段，拉链表的计算原理是什么？

答：跟其他表的区别是多了这三个字段，开始时间结束时间，有效标识。原理的话，在某个时间段内，该数据是有效的，超出时间范围，就是无效的，可以方便的记录和保存历史数据，也可以看出来历史数据的变化记录。一般用于产品信息表，客户信息表等

34. 拉链表设计：假如你 19 年在 A 部门，20 年在 B 部门，21 年在 C 部门，怎么用拉链表来存储这些数据变化？

答：姓名，部门，开始时间，结束时间，有效标识；第一条，姓名，A部门，19-01-01,19-12-31,0；第二条记录，姓名，B部门，20-01-01,20-12-31,0；第三条记录，姓名，C部门，21-01-01,9999-12-31,1；0表示无效，1表示有效。

35. 你们拉链表是怎么创建，实现更新的，具体过程是怎样的？

答：一般是用增量数据更新拉链表，我的思路是：

比如说是今天的数据就是原始数据，我将今天的产品信息表数据进行全量初始化导入到创建的拉链表中；明天跑数的时候，抽取增量的数据，需要把要处理的所有数据分成两部分，处理思路为

(1)一种是新增, 今天增量数据, 结束日期  $= '9999 - 12 - 31"$  
(2)一种是更新, 历史拉链表与增量表进行比对

当增量表中存在记录,开始日期=历史拉链表开始日期,结束日期= date_add('${dt1}', -1)  

- 当增量表中不存在此记录，代表不需要更新最后将结果用union all合并插入到拉链表中
36. 如何查询拉链表最新状态数据，语句怎么写？

select * from 拉链表 where end_date='9999-12-31';

37. 如何查询拉链表某一天所有的数据状态（快照）

select * from 拉链表 where start_date<=‘\\({dt1}’and end_date>=‘\\({dt1}\)';

注：这里日期是个变量，想看哪一天的就输入哪一天的即可。

# 38. 比如说发现某一天拉链表数据有问题，怎么回事？

答：假设在 23 号发现数据有问题,需要回滚 22 号的数据,我的思路是分为几部分进行操作：

①对于结束日期为21号及之前的数据，【保留】；  
②对于22日有效的数据，【更新】，其数据又分两种：

一种是结束日期是22日的，把结束日期  $= 9999 - 12 - 31$  即可；

一种是22日前创建，22日后还有效的数据，把结束日期  $= 19999 - 12 - 31$

③对于22日后产生的数据，【删除】。

# 39.怎么查看数据库用户下的表

答：有一个表记录了当前用户的表，叫user_tables，另外还有一个表user_procedures，记录了当前用户下的自定义函数和存储过程

select * from all_tables a where a.OWNER = upper('数据库用户名');

# 40. 什么是表空间？系统常见的表空间有哪些？

答：表空间就是一个或多个数据文件的集合，ORACLE 所有的数据对象都存放在指定的表空间中，但主要存放的是表，所以称作表空间。系统常见的表空间有系统表空间、TMEP 表空间、用户表自定义空间。

系统表空间：存放系统数据，例如：数据字典、视图；

TMEP表空间：存放临时缓存数据；

用户表定义空间：存放用户创建的表数据；

# 41. 什么是数据字典？

每个数据库包括一个被称为“元数据”的集合，或者说包含用来描述数据库有关数据结构的数据。包含这些元数据的表和视图称为数据库数据字典。可以理解为数据字典就是一个用描述数据库中表的字段名，字段长度，字段说明等信息的文档；是对数据库里字段的一种描述，说明；是存储数据库中基本元素的一个集合！它可以存储基本表的数据结构，存储过程；是一张表，记录某些数据库的信息等等。

# 42. 如何查询系统数据字典？

查看用户下所有的表：select * from user_tables;

查看名称包含log字符的表：

select object_name, object_id from userobjects where instr(object_name.'LOG') > 0;

查看索引个数和类别：

select index_name,index_type,table_name from userindexes order by table_name;

查看函数和存储过程的状态

select object_name, status from user Objects where object_type='FUNCTION'

select object_name, status from user Objects where object_type='PROCEDURE';

查看函数和过程的源代码

select text from all_source where owner  $\equiv$  user and name  $\equiv$  upper('&p|sql_name');

# 43. 对数据库事务了解吗？

答：有自己了解过，主要是访问并可能操作各种数据项的一个数据库操作序列。事务性质 ACID 有：原子性、一致性、隔离性、持久性

# 44. 数据库事务的特点、属性是什么？

答：ACID，原子性（Atomicity）：事务中的全部操作在数据库中是不可分割的，要么全部完成，要么全部不执行。一致性（Consistency）：几个并行执行的事务，其执行结果必须与按某一顺序串执行的结果相一致。隔离性（Isolation）：事务的执行不收其他事务的干扰，事务执行的中甲结果对其他事务必须是透明的。持久性（Durability）：对于任意已提交事务，系统必须保证该事务对数据库的改变不被丢失，即使数据库出现故障。

# 45. 事务数据库的隔离级别

答：有序列化，可重复读、已提交读、未提交读四个级别，默认的是已提交读隔离级别。

1. 读未提交（Read Uncommitted）：该隔离级别允许一个事务读取另一个未提交的事务所做的修改。这会导致脏读（Dirty Read）问题，即读取到未提交的数据。

2. 读已提交 (Read Committed) : 该隔离级别要求一个事务只能读取已经提交的数据。与读未提交相比, 读已提交可以避免脏读问题。但是, 由于一个事务可能在读取数据过程中, 另一个事务进行了更新操作, 导致不可重复读 (Non Repeatable Read) 问题, 即两次读取同一行数据得到的结果不一致。

3. 可重复读 (Repeatable Read) : 该隔离级别要求一个事务在整个过程中多次读取同一行数据的结果是一致的。事务在开始时, 会创建一个快照, 并在整个事务过程中使用这个快照来读取数据。这可以避免脏读和不可重复读问题。

4. 序列化（Serializable）：该隔离级别要求所有事务串行执行，即每个事务都会完全地等待其他事务完成。这是最高的隔离级别，可以避免脏读、不可重复读和幻读（Phantom Read）问题。幻读是指在同一个事务中两次执行相同的查询，得到的结果不一致，这是因为在查询过程中，其他事务插入了新的数据。

隔离级别越高，数据的一致性和安全性越好，但并发性能会下降。根据具体的应用场景，可以选择合适的隔离级别。

# 46. 什么是锁，什么是死锁？

锁就是某个用户在更新、修改数据时，锁定当前表或记录，不允许其他用户修改。死锁就是两个会话，每个会话持有对方想要的资源，因争夺资源而造成的互相等待的现象

# 47. 锁的种类

答：在 Oracle 中最重要的锁是 DML 锁，DML 锁的目的在于保证并发情况下的数据完整性。在 Oracle 数据库中，DML 锁主要包括 TM 锁和 TX 锁，其中 TM 锁称为表级锁，TX 锁称为事务锁或行级锁。锁出现在数据共享的场合，用来保证数据的一致性。当多个会话同事修改一个表时，需要对数据进行相应的锁定。

# 48. 如果发生死锁，如何处理？

首先要有管理员权限，然后在 psqldev 工具中打开：工具-会话窗口找到带锁的会话，点击关掉（kill）这个会话。（或者直接找运维解决这个问题）

# PL/SQL

# 1. 游标如何使用

注：游标的概念+种类+优缺+使用场景

答：游标主要是 psql 中使用，属于变量的一种，将一个结果集读到内存中，然后每次读取一条记录，所以需要使用 loop 循环或者 for 循环来读取。有显示游标和隐式游标，使用游标的优点在于，不需要重复读取某一个表，因为结果集是在内存中的，效率比重复去磁盘读取一个表要快的多。缺点的话，主要是可能表的数据量比较大，无法读取到内存，而且游标会给相关的记录加一个锁，在生产环境可能会影响其他的程序，所以我们之前的项目中一般是不允许使用游标的。

# 2. 游标的属性有哪些？

游标属性：%FOUND 和%NOTFOUND

%FOUND: 用于判断游标是否从结果集中提取数据。如果提取到数据，则返回值为 TRUE，否则返回值为 FALSE；

%%NOTFOUND: 该属性与%FOUND 相反, 如果提取到数据则返回值为 FALSE; 如果没有, 则返回值为 TRUN:

# 3. 包的组成？

一个程序包由两部分组成：包定义和包体。其中包定义部分声明包内数据类型、变量、常量、游标、子程序和函数等元素，这些元素为包的共有元素。包主体则定义了包定义部分的具体实现。

# 4. 什么是全局变量？

在包定义部分声明的变量和在包体里的存储过程或者函数都可使用，这个是全局的，在存储过程或者函数内部定义的只能这个存储过程或者函数使用，是局部的。

# 5. 如何创建存储过程？

包括过程的名称、过程使用的参数以及过程执行的操作

# 6. 存储过程参数的区别？

IN表示传入参数，不可以被赋值，OUT表示传出参数，可以被赋值，INOUT表示传入传出参数，可以传入值，可以被赋值，可以返回值

# 7. 存储过程由哪些代码构成？

create or replace procedure 名字(参数 类型)

as

声明

begin

执行

exception

异常

end;

# 8. 存储过程怎么调用

答：存储过程一般使用 psql 程序去调用，也可以写一个总的存储过程，调用所有的存储过程，开发完以后，一般都是选中存储过程，然后右键点测试，输入参数，看一下能否运行成功。

# 9. 存储过程的优点

答：存储过程，主要是封装一些数据处理的过程，比如删除，插入或者修改，将这些脚本保存在数据库中，方便管理和维护，而且再次执行时，不需要重新进行编译，比我们保存一段 SQL 脚本要方便多。

# 10. 什么时候你会写存储过程？（写存储过实现了哪些功能）

使用存储过程来对sql语句进行优化，减少硬盘的读写和与服务器之间的交互次数；复杂的逻辑

辑使用存储过程；一些数据库的固定操作；表格的数据抽取的ETL操作等等。

# 11. 有用过自定义函数吗？

答：有用过 Oracle 的自定义函数，其实就是将某些查询或者计算方法封装成一个函数，类似于常用的 substr、replace 这样们可以避免重复开发，简化代码，提高工作效率。有些过一些通用的，比较简单的，比如说计算年龄或者性别这种的。

# 12. 自定义函数与存储过程的区别

答：都是封装一段 SQL 脚本，语法上可能差不多，但是自递归函数主要用于封装一些查询计算类的语句，用于传入几个参数，返回某个具体的值；而存储过程主要是用于封装一些过程类的脚本，比如 delete、update、insert 这些，用于数据的处理。自定义函数还要求先声名返回值的类型，要有 return。而存储过程中可以有 return，只能 return null，但是可以使用 out 来传出一个值或者某个参数。

# 13. 如何找到存储过程哪个环节执行的快慢？

答：用日志是用来追溯问题的，记录整个程序的运行情况，知道哪个环节报错了，记录每一步花了多少时间，判断哪一步性能不好，从而对程序进行修改和优化。（其次在存储过程前后加上打印时间戳也可以判断出哪个过程比较缓慢）

# 14. 存储过程的异常怎么捕获？

答：在 psql 中使用 exception，在 exception 中把过程数据记录到错误日志表，然后对主过程的事务做回滚，并且针对异常我们会记录异常的日志，方便后续问题的分析

1）、代码上用EXCEPTION。

例如：EXCEPTION

WHEN OTHERS THEN

DBMS_OUTPUT.put_line('sqlcode：' | |sqlcode);

DBMS_OUTPUT.put_line('sqlerrm：' | |sqlerrm);

END

sqlite是异常编号，sqlerrm是异常的详细信息，如果异常信息太多，可以截取一段显示，如：DBMS_OUTPUT.put_line（‘sqlite:’）；是截取前100个字符显示出来。也可以插入到日志表：insert xxx values （sqlite,sqlerrm）

2）、工作上看报错日志。

15. 常用的异常都有哪些？  

<table><tr><td>异常名称</td><td>异常码</td><td>描述</td></tr><tr><td>DUP_VAL_ON_INDEX</td><td>ORA-00001</td><td>试图向唯一索引列插入重复值</td></tr><tr><td>INVALID_CURSOR</td><td>ORA-01001</td><td>试图进行非法游标操作。</td></tr><tr><td>INVALID_NUMBER</td><td>ORA-01722</td><td>试图将字符串转换为数字</td></tr><tr><td>NO_DATA FOUND</td><td>ORA-01403</td><td>SELECT INTO语句中没有返回任何记录。</td></tr><tr><td>TOO MANY_ROWS</td><td>ORA-01422</td><td>SELECT INTO语句中返回多于1条记录。</td></tr><tr><td>ZERO_DIVIDE</td><td>ORA-01476</td><td>试图用0作为除数。</td></tr><tr><td>CURSOR_ALREADY_OPEN</td><td>ORA-06511</td><td>试图打开一个已经打开的游标</td></tr></table>

# 表分区

# 1. 分区有没有用过

答：分区表是有用过的，一般都是按照日期，每年一个分区的，之前项目有几个表是比较大的，几亿那种，都是按照年份去做的分区。

# 2. 你有用过表分区么，如果一个月的分区就有几千万怎么办

答：有的，之前也有碰到过这种情况，我们当时的解决办法，就是每个月做一个分区，然后用地区做一个子分区，这种方法会好一些。另外就是分表，一年一个表，用原表名拼接年份作为新表名，然后新的表按照每半月一个分区/或者每周一个分区这样子。

# 3. 表分区有哪些，一般是用在什么场景下？

答：我知道的有范围分区，列表分区，散列分区，组合分区，

范围分区：用指定的分区键决定的范围进行分区，最为常用，分区键常采用日期。

列表分区：某列的值只有几个，可以采用列表分区。

散列分区：通过指定分区数量或编号来均匀分布数据的一种分区类型，分区数量常采用2的N次方；当列的值没有合适的条件时，采用散列分区；

组合分区：范围分区和列表分区组合；范围分区和散列分区组合，分区中的分区被称为子分区；

# 4. 哪种分区类型使用的最多？为什么？

答：看具体情况，

答：看具体情况，  

1. 用要素进行数据的过期化处理，那么范围分区基本上是唯一的选择；

如果需要进行数据的过期化处理，可以使用以下方法：1. 如果需要数据的均匀分布，那么可以考虑使用 HASH 分区；

如果需要数据的均匀分布，那么可以考虑使用列表分区；

如果数据的值可以很好地对应了某个分区，那么我们就可以用该数据作为回归模型的基点。再结合性能的影响因素，来最终确定使用哪种类型的分区。

在上面的原则基础上，再结合性能的就

# 5. 你们做的分区表占的比例大概是多少？

答：没有细数过，但是分区表不多，因为大的表没有多少，我们一般两千万以上的表做的分区表，真要说的比例的话，大概占常用表  $10\%$  左右的样子。

# 6. 谈谈表分区，常用哪些分区类型

答：分区表，原理其实就是像我把一个数据文件按照某个标准拆分成几份，然后加快一个查询，物理上是分块存储的，逻辑上还是一个完整的表，我知道的分区类型：

有范围分区，一般按照日期就是每年一个分区，还有列表分区，一般都是按照地区去做一个分区；还有哈希分区，这个不太了解，还有一份组合分区，就是可以按照日期每年一个分区，再用地区做子分区。

# 7. 分区表创建的基本语法是什么？

Create table 表名

（字段名，字段类型……）

partition by range(字段名)

(partition子分区名valueslessthan（某个日期））

# 8. 怎么去查看表格分区的信息？

select * from USER_TAB_PARTITIONS a where a.table_name = '表名';

# 9. 对分区字段进行where筛选和直接读取分区内容，谁快一些？

select * from fq_empt_list where depnto=10;

select * from fq_empt_list partition(d10); 一分区更快些

# 索引&视图

# 1. 怎么查看一个表格有哪些不同的索引？

select * from userIndexes where table_name='表名';

# 2. 索引怎么创建？

CREATE INDEX index_name

ON table_name (column1, column2, ...);

# 3. 索引的原理，种类，优缺点

答：（1）索引的原理，主要是将索引列，进行一个排序，然后加上对应的ROWID，生成一个索引表，查询或筛选该列的时候，会从索引表查询或筛选对应的数据，根据对应的ROWID，去查询整行的数据，达到加快速度的目的。我的理解就和目录是一样的，加快的速度主要来自于目录的页码，也就是ROWID,因为ROWID它是一个屋里位置。

(2)索引的种类主要有普通索引, 组合索引, 带函数的索引, 唯一索引。

(3) 索引的优点, 主要是为了加快查询或筛选, 当然用索引列关联或者排序的时候, 也会快一点。缺点就是会降低增删改的速度, 因为数据记录的增删改也需要同步更新这个索引表, 比没有所以的时候要慢一些。

# 4. 主键索引和唯一索引的区别

答：主键是不能自己独立创建，和主键约束一起创建的，唯一可以和约束一起创建，也可以单独创建，主键约束会自带一个唯一索引，主键列要求非空且唯一。唯一索引列，要求当前列的值必须是唯一的，但是可以为空。

# 5. 你在索引选择上，会选择什么？索引的使用方法所以是越多越好吗？

注：摘明白是问什么情况下建索引，还是建索引时怎么来考虑索引种类。

答：一般经常查询或者筛选的列，从大表中查询的记录比较少的情况，需要建索引，在索引的选择上，看具体的情况，如果没有添加函数或转换，创建普通索引即可，否则就可以加一个带函数的索引；如果该列的值都是唯一的，也可以考虑唯一索引；如果查询或筛选的列有多个，也可以按照优先顺序去创建一个组合索引。

索引并不是越多越好，因为其会降低增删改的效率，因为数据记录的增删改也需要同步更新这个索引表，比没有所以的时候要慢一些。

# 6. 建立索引需要遵循什么原则？

答：索引并不是越多越好，因遵循以下原则：（选几条回答）

选择适当的列：索引应该建立在经常被查询的列上，这些列通常是经常用于筛选、排序或连接的列。选择具有高选择性（即不同值较多）的列作为索引列可以提高索引的效果。

考虑查询性能：索引应该根据实际查询需求来设计，以提高查询性能。例如，对于经常进行范围查询的列，如日期范围或价格范围，可以建立范围索引；对于需要快速匹配的精确查询，如主键或唯一约束列，可以建立唯一索引。

避免过多索引：过多的索引会增加存储空间和维护成本，并且可能降低写操作的性能。因此，应该根据实际需要选择建立索引的列，并避免不必要的冗余索引。

注意索引选择顺序：在多列查询中，索引选择的顺序也很重要。应该优先选择最常用的、最能减少数据集大小的列建立索引，以便快速缩小查询范围。（翻页）

定期维护和更新索引：随着数据的增删改，索引也需要进行维护和更新。定期对索引进行重建、重新统计和优化，以确保索引的有效性和性能。

综合考虑存储和查询需求：索引虽然可以提高查询性能，但也会增加存储空间和写操作的成本。在建立索引时，需要综合考虑存储空间和查询性能的平衡，根据实际需求进行权衡。

这些原则可以作为建立索引时的参考，具体的索引设计还需要根据数据库引擎、数据量、查询模式等因素进行具体分析和优化。

# 7. 不同的索引一般用在什么场景下？

- 唯一索引：当某列任意两行数据不相同，建立PRIMARY KEY主键和UNIQUE CONSTRAINT唯一约束时（索引自动建立）。

组合索引：当多列经常一起出现在WHERE条件中，创建索引。

位图索引：当一列有大量重复数据时建立。

函数索引：在WHERE条件语句中包含函数或表达式时建立。

# 8. 你在使用索引的时候，需要考虑的问题，就是怎么避免索引失败？

答：要考虑的情况有这几个：

第一个是使用创建索引时的列的形式，比如不加函数转换，或原来加了函数的就要带上；第二个就是不要对索引列进行一个算数运算；

第三个就是筛选的条件中,不要对索引列做一些隐式转换,比如索引列是数字的,那么等号后面的值基因不该加引号；

第四个就是模糊查询不要用通配符%开头：

第五个就是不用用not in和not exists;

第六个就是不要单独查询组合索引中非第一列的，基本上就是这些需要考虑的问题了。

# 9. 在分区表建索引和在普通表建索引有什么区别（全局索引和分区索引的差别）

注：全局索引、分区索引,普通索引都叫全局索引,分区索引是可以建立在某个分区的,一般分区表都会选择分区索引,因为后期的维护都会比较容易。

答：我的理解，普通表是建的全局索引，分区表可以建全局索引，也可以每个分区创建索引，这样只是一开始工作量比较大，全局索引，也就是说一个索引你建立在全表上，这样问题就出来了，当我对某个分区进行drop或者新建分区的时候，不得不对全局索引进行重建，效率就会变低；二分区索引，当我对表进行drop分区，或者create分区的时候，只需要在新的分区简历新的索引即可。

# 10. 哪些列该建立索引？哪些列不应建立索引？

建立索引：经常需要搜索、排序的列；关联字段；

不该建立索引：很少使用的列；只有很少数据值的列；定义为 TEXT, IMAGE, BIT 数据类型的列

# 11.truncate会导致索引失效吗？

答：truncate分区使全局索引失效，需要重建全局索引，使其生效，对本地（分区）索引没有影响。如果truncate整张表也是，全局索引失效，分区索引还是有效的。为什么？因为分区索引在TRUNCATE操作后，虽然是处于一种无法直接访问有效数据的状态。但当重新插入数据到相应分区后，分区索引将重新建立与有效数据的关联，并恢复其正常的数据访问功能。

# 12. 在 abc 三个字段创建了一个索引，我现在进行查询，条件是  $b = 00$ ，判断一下这个索引针对这个查询是否有作用？

答：没有作用，因为单独查询组合索引中非第一列的会导致索引失效，如果查询  $a = 00$  and  $b = 00$  就不会失效。

# 13. 在测试环境下有十万条数据，然后建立了联合索引，确实走了索引，但是在生产环境下是一百万数据，发现没有走索引。你怎么处理？

如果在生产环境下，索引没有被有效使用，可以考虑以下几个方面来处理：

优化查询语句：检查查询语句是否能够充分利用索引，可以通过 EXPLAIN 或其他查询执行计划分析工具来查看执行计划，确认是否存在索引未使用的情况。如果发现索引未被使用，可以尝试重写查询语句，使用合适的查询条件、避免函数包装等操作，以便数据库能够正确选择并使用索引。

重新评估索引设计：在生产环境下，数据量增加可能导致索引失效或性能下降。这时候可以重新评估现有的索引设计，根据查询的实际需求和数据分布情况，考虑修改或添加新的索引。可以通过分析查询日志、查询执行计划等手段来确定需要优化的查询，并结合数据库系统提供的索引分析工具来找到最佳的索引设计方案。

考虑分区或分片：如果数据量非常大，索引无法有效支持查询，可以考虑使用分区或分片的方式来拆分数据并创建相应的索引。这样可以将查询操作分散到多个子表或子集群上，提高查询性能。

# 14. 索引失效的话你一般怎么去定位？

答：一般都是看代码，看一下哪些地方用到了索引列，使用形式，再看一下索引的名字和创建时的格式，尽量不要使用会失效的写法；

也可以通过执行计划去定位索引是否失效：

(1) 在执行查询语句时，可以加上“explain plan for”语句来生成执行计划。  
(2) 执行“select * from table_name”语句。  
(3) 运行“select * from table(dbms_xplan.display.format  $\coloneqq$  '\*ALL')；”命令，查看执行的计划。

(4) 查看执行计划，定位哪个索引失效。

(4) 查看执行计划, 定位中单击“索引”。执行计划中会显示每一步使用的索引和执行次数等信息, 如果执行次数很少, 则说明索引失效。可以根据这些信息定位哪个索引失效, 然后考虑优化该索引的使用方式, 或者重新设计相应的查询语句。

# 15. 索引中本地索引和全局索引的区别是什么？

13. 索引中除索引外，还有一种检索方式，即本地索引（也被称为分区索引）不能创建唯一索引的类型，因为每个小的索引只能保证每个分区的数据是唯一的，不能保证整个表格的数据是唯一的。

分区的数据是二的，不能用三个分区来表示。分区的数据进行识别和判断。

# 视图

# 1. 什么是视图？优点缺点是什么？

答：视图就是一张或多张表上的预定义查询。

作用：减少子查询的复杂性；提高运行效率；可以仅提供视图数据，提高数据安全性。视图以定义的方式存储在数据库中，不占用表空间。

# 2. 只读视图和物化视图是怎么用的？

答：视图，我们平时工作还是有用到的，一般都是只读视图，是为了保护原表的数据不被修改，所以采用只读视图，否则可能会因为一些误操作，修改了原表的数据，物化视图，平时没怎么用过，但是知道一点，一般的视图不会保存数据，物化视图可以保存数据，一般用来备份，或者记录历史某一个点的数据，相当于快照。

# 3. 视图什么时候会用？

在实际的数据库应用中，视图常常被用于处理复杂查询和数据访问安全性等问题（运维方面接触做得比较多）。

限制访问权限：通过创建视图，可以限制用户只能访问指定的数据列和行，从而实现数据访问权限的控制。例如，可以创建一个视图，只允许用户查看自己的相关数据，而不是整个表中的所有数据。

- 将敏感数据存储在一个视图中, 可以保护这些数据不被直接查询, 从而提高数据的安全性。例如, 可以创建一个视图, 将经过加密处理的敏感数据存储在其中, 同时只允许特定的用户访问该视图。

# 执行计划

# 1. 你一般看执行计划，会关注哪些点？

答：看执行计划，主要关注关联机制和扫描表的方式。关联机制主要有嵌套循环，哈希，排序合并三种。扫描表的方式。主要是看施工走了合适的索引，还有就是看一下扫描的记录条数和占用的资源。

# 2. oracle中执行计划主要看什么？

在 Oracle 中，执行计划是指查询的执行过程，包括数据的获取、连接、筛选、排序等操作。执行计划可以帮助我们优化 SQL 查询，提高查询性能。

在执行计划中，我们需要关注以下几个重要的指标：

查询类型：查询类型通常是由操作符和操作数字等组成，表示查询的操作类型，如全表扫描（Full Table Scan）、索引扫描（Index Scan）、嵌套循环连接（Nested Loops Join）等。

访问方法：访问方法有两种，即序列式访问和随机访问。序列式访问是按照数据的物理顺序逐一读取数据，而随机访问则是通过索引或者其他方式跳跃读取数据

访问对象：访问对象是执行查询的对象，可以是表、索引、视图等。

数据量估算：数据量估算是指查询过程中返回的数据行数或者数据量大小，可以通过执行计划中的Cost和Rows字段来估算。

# 3. 按 F5 后看到了执行计划的分类, 它有哪几种类型? 这些执行计划里哪种是最优的? 其次, 他们的顺序是什么?

在执行计划中，最优的执行计划通常是根据查询条件、索引、表的大小和统计信息等因素来确定的。最优执行计划的选择是由查询优化器负责的，其目标是选择执行代价最低的执行计划。在一般情况下，以下顺序可以作为执行计划的优先级参考：

使用唯一索引扫描（Unique Index Scan）：如果查询条件可以使用唯一索引来定位满足条件的行，那么这是最优的执行计划。

范围扫描（Range Scan）：如果查询条件可以使用索引的一部分来满足范围查询条件，这比全表扫描更有效。

索引扫描（Index Scan）：如果查询条件可以使用索引来满足条件，但需要扫描索引中的部分或全部数据。

全表扫描（Full Table Scan）：如果没有合适的索引可用，数据库将不得不扫描整个表来满足查询条件。

连接算法（Join Algorithm）：对于连接操作，嵌套循环连接（Nested Loop Join）可能会比哈希连接（Hash Join）更优，具体取决于表大小、索引和查询条件等因素。

排序（Sort）和聚合（Aggregate）：如果查询需要排序或聚合操作，这些操作会增加查询的执行代价。

# 4. 怎么看执行计划有没有走索引？

如果想要判断执行计划是否走索引，可以查看访问对象中的索引名称或者索引类型等信息。通常，如果执行计划中出现了 Index Scan、Index Unique Scan、Index Range Scan 等索引相关操作，就表示查询使用了索引。此外，还可以通过 Cost 指标来判断索引的使用情况，如果 Cost 值比较小，则说明查询性能较好，可能使用了索引等优化措施。

# 5. 表的扫描方式有哪些？优缺点是什么？

答：主要有全表扫描、索引扫描

优缺点：全表扫描：可直接读取表中所有行，适合数据量小或者数据量大返回结果多；

索引扫描：查询速度快，耗费资源少，先扫描索引得到对应的ROWID，然后通过ROWID定位到具体的行读取数据，适合数据量大，返回值少的表；

# 6. 大表和小表关联，先扫描那张表？

注意：把两张表最终需要关联的数据对比，大表适合做被探查表，因为可以减少从硬盘读取扫描表的次数。因为在oracle中，关联查询的执行顺序（即扫描哪张表）是由查询优化器决定的，其依据是选择最优的执行计划。可以通过查看执行计划或者使用优化器提示来指定先扫描哪张表。

# 7. 关联机制有哪些？

答：关联机制主要有嵌套循环，哈希，排序合并三种：

嵌套循环关联是指依次从驱动表中提取一条数据，便利被探查表，将匹配的记录放入待展示的缓存区中。

适用场景：其中一个数据集较小，另一个数据集较大，并且有一个可以用于快速查找的索引。

哈希关联计算出整张被探查表关联字段的哈希值，这些哈希值和整张被探查表一起放入缓存区，然后从驱动表逐条取记录，计算出挂把脸字段对应的哈希值，在意被探查表的哈希值匹配，匹配配上了在精准匹配每一条记录。

适用场景：两个数据集大小相对较大，并且没有适合嵌套循环连接的索引。

a. 排序合并关联是指将关联的 a 表跟 b 表分别进行排序,生成临时的两张表后,随机取一张表逐条抽取记录与另一张表匹配。

适用场景：两个数据集已经按照关联列进行排序。

选择合适的关联机制取决于数据集的大小、索引情况和排序状态等因素。在实际使用时，可以通过分析查询计划（Explain Plan）来确定 Oracle 数据库选择的关联机制。

# 8. 关联机制的运用优缺点

答：（1）嵌套循环：优点：适用广，占用内存小，展现快。缺点：需要不停从硬盘中读取扫描表，性能不好。（注意：把两张表最终需要关联的数据对比，大表适合做被探查表，因为可以减少从硬盘读取扫描表的次数。）

(2) 哈希关联：优点：性能好，匹配次数大大减少。缺点：只适用于等值关联，占用内存大。（注意：把两张表最终需要关联的数据对比，小表适合做被探查表，因为怕内存不够。如果缓存足够的前提下，大表适合做被探查表。）  
(3) 排序合并连接。优点：适合有索引的两张表或者不等关联。缺点：排序性能消耗大，占用资源占用内存大。

# 9. 假如给你一个执行计划你要怎么看，以你的经验做了那些优化呢？做了那些处理？

答：首先熟悉一下执行的先后顺序，缩进最多的先执行，同级别的上面的先执行。会关注关联机制，还有扫描表的方式，可以参考表的数据量或者是否是等值关联，来选择一个合适的关联机制，还有是否是走了索引，如果是全表扫描的，可以看下是否有索引，是否失效，或者没有走单列索引而走了组合索引等等。

# 10.怎么查看oracle运行日志？

答：Oracle数据库会将运行日志记录到日志文件中，但通常需要数据库sysdba用户高级权限，可以使用SQL*Plus命令查看日志文件，步骤如下：

首先登录到SQL\*Plus:sqlplus/nolog;

然后使用sysdba用户权限登录到数据库：conn / as sysdba

最后，使用以下命令查看Oracle数据库错误日志：

SHOW PARAMETERBACKGROUND_DUMP_DEST;

会显示存储日志文件的路径，可以使用操作系统的文件浏览器前往该目录查找日志文件，或者使用以下命令列出错误日志文件：

cd <trace_directory>

dir alert*.log

其中，<trace_directory>为你的跟踪文件目录。alert*.log为错误日志文件名的通配符，它将列出所有以“alert”开头和“log”扩展名结尾的文件。

# hints优化器

1. 什么是并行,如何通过并行增加数据处理速度？加多少并发？

2. 什么是并行，如何通过并行增加数据处理速度？并行就是采用 parallel 技术，把一个大的任务分为若干小任务，同时启用 n 个进程/线程，同时处理这些小任务，通过消耗 CPU 资源增加数据处理速度，适用 parallel 的两个条件：

(1) 大的任务, 如全表扫描大表, 小任务自己完成比派发省事  
2）系统有足够的资源（cpu/io）换句话说，并发是在系统资源充足，用户少的系统上，为了充分利用系统资源以提高任务处理速度而设计的一种技术。

# 2. Oracle 并行是不是越多越好。为什么？

答：并不是，因为会有上限。比如盖一个房子，一个人需要00天，但是00个人需要天，但是不可能400个人就只要小时。这是同样的原理，数据库服务器本身的资源和速度是有上限的。

# 3. 如何看 hints 是否被调用？Hints 原理

查看执行计划有个PX字段代表，在绝大多数情况下执行计划会选择正确的优化器，减轻了DBA的负担。但有时它也聪明反被聪明误，选择了很差的执行计划，使某个语句的执行变得奇慢无比。此时就需要DBA进行人为的干预，告诉优化器使用我们指定的存取路径或连接类型生成执行计划，从而使语句高效的运行；

# 4. 常见的 hints 优化器有哪些？

$/^{*} +$  PARALLEL（表名，并行数）\*/一指定开启多少个并行  
/\*+INDEX（表名索引名）\*/一指定索引  
/*+FULL（表名）*/一指定全表扫描  
/\*+USE_NL(表名1表名2）\*/一指定用NESTEDLOOP连接  
/\*+USE_HASH(表名1表名2）\*/一指定用HASH连接  
/\*+USE_MERGE(表名1表名2）\*/一指定用SORTMERGEJOIN（不等式运用）  
$/^{* + }$  LEADING(表名1表名2）\*/一指定表1作为驱动表  
/\*+APPEND \*/一数据直接插入到高水位上面(与insert连用)

# 5. 怎么使用优化器让索引失效？

注：如果不知道怎么用 hints来做，直说就好了，但是可以用代码来让索引失效

答：（1）我只知道用 hints 强制走一个索引，失效还真的没用过，但是可以用代码让它失效，比如 amt+0=5000，dt+0=sysdate，name||''=scott。  
答：（2）让索引失效其实就是走全表扫描，用 hints来做就可以，在select后面加上  $\ast +\mathrm{FULL}$  （表名）\*/就可以了  
答：（3）Oracle的 hints中NO_INDEX关键词是用来指定不适用那些索引，select/ $+\text{no_index}$  （表名。索引名）\*/

# 6. 数据库的并发控制，怎么处理？

答：Oracle中的 hints 里面，可以用 parallel 关键词来做并发，之前有一个 insert 语句比较慢，我加了一个并发在 select 语句中，但是 insert 后面忘记加了，insert into xxxx

select/*+parallel（a）*from xxx a;没有什么效率上的提升，修改后insert/*+parallel（xxxx,4）*/into xxxx select/*+parallel（a）*/*from xxx a;

# 7.怎么开启并行？开启-8，为什么是-8行？

答： $/^{*} +$  parallel（xxxx,4）\*/，因为这个并行不是越多越好的，会有一个上限，有时候开的多了反而会降低效率，浪费数据库的资源。

# 8. 什么是高水位插入，怎么实现高水位插入？

答：高水位插入，举个例子，有个箱子，放了一层的货物，拿出来一些，再往里面放的时候，一般都会填上一层的空缺，高水位就是从二层开始放，之前放过的位置不放了。怎么实现的话，用 hints中的append关键词，在insert后面加上/\*+append\*/就可以达到想要的效果。

# Oracle性能优化

# 1. Oracle 内部执行机制（底层原理）

1）语法分析，分析语句的语法是否符合规范，衡量语句中各表达式的意义。  
2）语义分析，检查语句中涉及的所有数据库对象是否存在，且用户有相应的权限。  
3）视图转换，将涉及视图的查询语句转换为相应的对基表查询语句。  
4）表达式转换，将复杂的SQL表达式转换为较简单的等效连接表达式。  
5）选择优化器，不同的优化器一般产生不同的“执行计划”  
6）选择连接方式，ORACLE有三种连接方式，对多表连接ORACLE可选择适当的连接方式。  
7）选择连接顺序，对多表连接 ORACLE 选择哪一对表先连接，选择这两表中哪个表做为源数据表。  
8）选择数据的搜索路径，根据以上条件选择合适的数据搜索路径，如是选用全表搜索还是利用索引或是其他的方式。  
9）运行“执行计划”。

# 2. Oracle性能优化

答：性能优化，我这边还是有遇到过一些性能优化的问题的，也自己总结了一套东西，首先我会考虑下表结构，主要看下是否有分区，或者是否有合适的索引，再或者尝试一下分表；

接着看下计算和关联的逻辑顺序问题，如果有几个表关联，可以先把小的表关联起来再关联大的表，另外有涉及计算的，可以考虑先进行一个计算，再进行关联，比如之前汇总产品的交易额，我都是先在交易表进行sum求和以后，用产品表left join进行关联；

然后就是考虑执行计划，看一下这个关联的机制，是嵌套循环还是哈希，根据情况去调整一个合适的关联机制，另一个就是看一下扫描表的方式，是否走了合适的索引；

最后就是考虑一下代码，比如用一些更加高效的代码，比如 merge into 替换 update，truncate 替换 delete，子查询比父查询数据量多很多，可以用 exists 替换 in。这是我平时工作里对性能优化问题的一个解决经验。

# 3. 常用的SQL语句调优？

1）避免索引失效；  
2）多使用COMMIT提交数据；  
3) in.not in 用 exist, not exist 替换，少用 JOIN;  
4）进行步骤拆分，使用临时表；使用WITH..AS语句，对某段SQL建立临时表；  
5) DISTINCT 性能最差的去重，尽量用 group by 替代  
6）使用SQL内置函数加快查询速度，例如使用decode函数替换casewhen；  
7）避免在select后使用'*’符号；  
8) 连接和分组时先使用 where 条件过滤  
9) 避免笛卡儿积；  
10）删除全表数据时TRUNCATE替换DELETE；

# 4. 在数据量大的情况下用 Group by 和 order by 会占用磁盘空间，怎么优化？

答：过程原理要解决这个问题就必须先了解 group by 的执行过程原理，其过程使用到了使用到临时表和排序，简单理解为的，先按字段分组完成后（临时表），分组完再进行排序，如果数据量太大，内存临时表大小到达了上限（控制这个上限的参数就是 tmp_table_size），会把内存临时表转成磁盘临时表，占用大量的磁盘空间。这些都是导致慢 SQL 的直接因素。

解决思路：既然它默认会排序，我们不给它排是不是就行了？临时表是影响groupby性能的X因素，我们是不是可以不用临时表？

解决方案：group by 后面的字段加索引；order by null 不用排序（有些不适合使用索引）；尽量只使用内存临时表可以适当调大 tmp_table_size 参数，来避免用到磁盘临时表；使用 SQLBigInt_RESULT（如果预估数据量比较大，我们使用 SQLBigInt_result 这个提示直接用磁盘临时表。MySQL 优化器发现，磁盘临时表是 B+树存储，存储效率不如数组来得高，因此会直接用数组来存）

# 5. 有两张很大的表关联，怎么做才能关联性能好？

1. 首先要建立适当的索引。SQL 在索引字段不要加函数, 保证索引起效。如果是复合索引注意在 SQL 的顺序。如果已经存在索引, 建议你先重建索引, 因为大数据表的索引维护到了一个阶段就是乱的, 一般建议重建。建立好的一般可以获得几十倍的速度提升。  
2. 最大数据量的表放在最前,最小的表放在最后面。SQL是从最后面开始反向解析的。  
3. 其次是要把最有效缩小范围的条件放到 SQL 末尾去。尤其是主键或者索引字段的条件。  
4. 为两个表设计合理的表分区，然后分别对应关联两个表的分区数据，再用union all把各个连接结果叠加起来；

# 6. 大表和小表连接如何进行优化？

答：将小表作为驱动表，大表作为匹配表，关联字段建索引。

# 7. distinct and group by 哪个效率高一些？

注：这个问题需考虑具体执行过程原理是怎样运行的，理解了之后就知道了怎么来答答：分两种情况，一种是关系型数据库中，如mysql中，groupby在执行过程中会进行隐式的排序，如果有索引存在，两者效率几乎等价，如果没有索引，在8.0版本前是，groupby需要进行排序，则distinct效率高，之后版本取消了隐式排序，效率也是一样的；在hadoop中是groupby效率高，Hadoop并不怕数据有多大，怕的是数据倾斜，使用distinct，在shuffle阶段到一个reducer里面，造成数据倾斜，groupby则不会；

# 8. in and exists 的区别，哪个效率高：

答：两者主要在进行扫描时有差异，当EXISTS子查询找到一个匹配项时，它可以立即返回结果并终止查询。这就是为什么在使用EXISTS子查询时，它可能比IN子查询更高效的原因之一。相比之下，IN子查询会扫描整个子查询结果集，并将其与外层查询进行比较。这意味着即使在找到第一个匹配项后，它仍将继续扫描整个子查询结果集，这可能会导致性能损失。

# 9. exists 一定比 in 效率高吗？

答："EXISTS"和"IN"这两种方式在查询性能方面并没有绝对的优劣之分，

一般来说，对于较大的数据集和复杂的查询条件，使用"EXISTS"子查询可能会比使用"IN"子查询更高效。这是因为"EXISTS"子查询只需要确定是否存在匹配行，而不需要返回实际的匹配值。相比之下，“IN”子查询需要返回所有匹配的值，这可能导致更多的数据传输和处理开销。

然而，在某些特定情况下，使用"IN"子查询可能比"EXISTS"子查询更高效。例如，当子查询的结果集较小且可以在内存中完全加载时，使用"IN"子查询可以避免对外部表的重复扫描，提高查询性能。

此外，数据分布和索引也会对查询性能产生影响。如果表中的数据分布较为均匀，并且针对查询条件的列上存在合适的索引，那么使用"IN"或"EXISTS"都可能获得较好的性能。

# 10. 用 merge into, 很容易遇到的问题是什么？

答：merge into 是根据一个结果集来批量替换另一个结果集，他们之间就需要一个关联的字段，那这个关联的字段是不能够修改的，可能会比 update 要局限一些，但是效率上来说，比 update 要快很多。

# 11. and and or 哪个性能好？

答：这涉及到sql的执行顺序问题，例如A and B，是先判断B，成立了才去判断A，这样的设计就隐含了一个查询性能优化问题。从尽量减少判断量这个角度出发，如果让容易为假的条件写放右边那么判断量就会减少（一假必假）。or的情况恰好跟and相反，把容易为真的条件写放or右边（一真必真）；

# 12. 有大量的 or 的时候应该要怎么优化？

答: 有大量 or 的时候, 如果有条件中存在索引列的时候, 可以用 union all 代替, 效率高, 因为 or 会导致走全表扫描, 如果没有, or 比 union all 快, 因为前者查询引擎会一次性完成指令分析。如果 or 条件中的值是有限且已知的, 可以使用 IN 语来替代 OR。如果特定值是连续的数值范围, 如 90-100 , 可以改用 bwteen...and 语句

# 13. 大量嵌套的子查询对性能影响比较大，可以考虑以下几种优化方式：

使用 JOIN 代替子查询：尽可能使用 JOIN 等连接方式替代子查询，因为 JOIN 可以更好地利用索引，提高查询效率。

使用WITH语句（或者称为公用表表达式）重构子查询：使用WITH语句可以将多个子查询重构为一个语句，提高代码可读性和维护性。

把子查询中的常量移出：一些子查询中可能存在被多次调用的常量，可以将这些常量单独提出来，避免重复计算。

使用临时表：有些情况下，可以将子查询的结果存储在临时表中，再进行后续操作，从而避免重复计算。

确保索引的正确使用：对于子查询中的表，需要确保索引的正确使用，从而减少查询的扫描次数。

缩短查询范围：对于大量嵌套的子查询，可以考虑优化查询范围，如增加过滤条件、合理使用分区等，从而缩小查询范围，提高查询效率。

# 14. 对脚本优化之前的排查步骤，除了和同事沟通是否进行了事务操作没有提交的情形外你怎么查？

答：这种针对一个脚本跑了很多没有结果，没有查询出来，那么一般都会考虑是锁表了，停止之后，重新执行一下看看，如果还是那么久，可以找运维组，看一下是否有表被锁住。也可以用 create as 创建一个一样的来源表，使用新的表跑一下这个脚本。

# 15. 对脚本的优化整体从哪几个方面来考虑？

答：我个人工作里，一般都是从4个方面考虑的，结构，关联顺序，执行计划中的关联机制和报表方式，具体关键词替换，（具体参考问题）

影响插入语句性能的原因

答：第一，就是目标表数据量很多，第二，目标表经常进行一个insert和delete的操作。导致有大量的日志文件，可以将目标表进行truncate一次，第三，就是目标表建立了大量的索引，第四，插入语句中的select子句查询很慢。

答：解决方案：看下能不能做一个历史表出来，将目标表只存放短期数据，相当于把原来的表拆分成多个表。可以将目标表 truncate 一次。先将目标表的索引失效，或者删除，插入完成后在重新恢复即可。具体的去优化 select 语句（方法参考问题）。除了这 4 点外，还可以考虑并发或者高水位插入等等都是可以的。

# 16. 给一条 SQL 语句, 你要怎么提高查询速度。理论+具体业务处理举例

注：参考问题的四个方面。

注：（案例）：结构方面，之前有一个项目中，它的交易表数据量很大，每个月新增几千万（3000-8000都可以），我们考虑用分区表，就是每个月一个分区，然后按照地区做了一个子分区，再以时间创建分区索引，当然也有考虑过每年建一个实体表，然后内部按照周去做分区，但是感觉很复杂，就选择了方案

注：（案例）：关联顺序，之前有个小需求，就是查每个地区的交易金额，但是表结构很奇怪，它的交易表中没有地区，而是通过客户信息表中国所在的地区编号来关联地区表得到的。但是交易表有几亿，客户表也有千万级别，最后还要关联一个地区表，我是这样做的，先在交易表中计算每个客户的金额，然后关联客户表，在按照地区ID去计算每个地区的金额，最后关联地区表。

注：（案例）：关于执行计划，之前有个项目中，开发环境的脚本都是走了一个单列的索引，但是上了生产环境，就变慢了，后来看了执行计划，发现走了组合索引，才注意到，就是这个列有两个索引，一个单列的一个组合的，后来用 hints 优化器强制固定走哪个单列的索引。

注：（案例）：关于代码，刚入行的时候，还是小白一个，当时有一个表5000W的数据，想清空，用了delete，结果半小时也没结果，后来发现天润查特可以一秒钟删除。后来优化代码中，发现有些update语句很慢，改了其中的子查询语句，也还是慢，后来查资料发现Oracle有merge into这种语法，改了之后，时间就成倍的减少。

# 17. 有 1000 条 sql 语言, 怎么定位哪句 sql 出现缓慢?

答：一般我们的存储过程中也会有多条sql语句，有delete, insert, update，多条的情况下，一般都是在语句的前后打印一个当前时间，通过对比这个运行前后的时间差，来判断每个SQL的运行时间，找到一些比较突出的，进行一个优化就可以了。

# 18. 在 1000 行 sql 语句里面，你该怎么优化，计算的指标涉及的数据量很大怎么解决？

答：首先需要理清楚最终的结果集，也就是需求是什么；

其次，看一下有多少个表，每个表的数据量，每个表的数据内容是干什么的，表间的关系，以及和最终出的结果集有什么关系；

第三个，再去看有没有一些操作是比如先关联，后进行一个计算或者聚合的，或者是不是重复用大表依次关联很多个小的表，或者有些是不是可以先进行筛选或者聚合的，然后再进行关联；  
第四个，主要可以看一下大的表有没有分区或者索引，索引是否失效；

第五个，有没有用一些效率非常低下的关键词，比如 order by，或者某个子查询比较大，用了 in，或者去重用了 distinct，或者有一些用了 union 的。

# 19. Oracle 数据库数据量大的时候是怎么查询的？你见过数据量最大是多少？

答：大的时候，一般考虑分表，或者分区，见过大的，有几亿的，可能有7-8亿的。

# 20. 说两个你 SQL 优化的例子？

实例一：UNION SQL 在运行时先取出数个查询的结果，再用排序空间进行排序删除重复的记录，最后返回结果集，如果表数据量大的话可能会导致用磁盘进行排序。实际大部分应用中是不会产生重复的记录，推荐采用 UNION ALL 操作符替代 UNION，因为 UNION ALL 操作只是简单的将两个结果合并后就返回。

实例二：Exists示例：当有A、B两个结果集，当结果集B很大时，A较小时，适用exists，如：SELECT  $\ast$  FROMaWHEREEXISTS(SELECT1FROMbWHEREa.COLUMN  $=$  b.COLUMN)；

当结果集A很大时，B很小时，适用in,如：

SELECT * FROM a WHERE a.COLUMN IN (SELECT b.COLUMN FROM b)

# 21. 怎么保证自己的代码数据唯一性，和主键唯一？

答：（嗯~营造个思考过程）代码唯一性，可以通过统计全字段去重后数据量和表中数据量，不一致说明不唯一，添加主键约束或者唯一约束，在hive一般没有特别限制主键唯一的，建表的时候也不会特别指定，一般需要sql去限制，如groupby等

# 22. 一个 psql 脚本很慢，如何分析到底哪里慢

通过日志，记录每步耗时，分析到底哪一步慢，然后找到最慢的 sql 部分，看到底是查询慢还是更新慢，如果查询慢，看 select 后面是否调用了其他的自定义函数，注释函数，再查性能是否提升，如果调了自定义函数之后变慢，则需要优化自定义函数的性能；如果仍然很慢，这个时候就要分析数据量的变化和执行计划，主要看数据量的变化情况，是否突然数据增量比较快，导致数据量暴增；还可以通过日志看，脚本执行的效率是否越来越慢，又分为数据量变得越来越大或者有人改过代码。数量多了，之前同步的方式可能需要更改；如果有人为了优化性能，建了更多的索引导致插入数据越来越慢，这时候需要平衡查询和更细的性能，有可能需要增加服务器的配置

# Linux系统&shell

# 1. 熟悉Linux系统和shell脚本吗，讲述自己知道的Linux命令

注：此类问题，都是判断是否熟悉linux系统，说明自己对于linux系统的搭建和使用，简单介绍一些命令的功能和关键字。

答：Linux 系统有使用过，自己也有搭建熟悉过一些基本的命令，比如文件的创建 touch、目录的创建 mkdir、删除 rm、移动文件或目录，也可用于重命名 mv、查看 cat、less、more、head、tail，还有解压压缩，用户权限一类的 chmod、chown。还有安装的 rpm、yum 这些。

# 2. vi编辑器有多少种模式？

命令模式：Vi的默认模式。在该模式下，可以执行编辑器的各种命令，如插入、删除、复制、粘贴、搜索等。按下Esc键可以从其他模式切换到命令模式。

插入模式：在命令模式下，按下“i”键进入插入模式。在该模式下，可以输入文本内容。

底行模式：在命令模式下，按下“:”键进入底行模式。在该模式下，可以输入各种底行命令，如保存文件、退出编辑器等。

# 3. 会写 shell 脚本么，对于 shell 的熟悉程度

答：shell 知道简单的命令。主要用来封装 hql 语句实现定期执行数据导入、转换、分析等操作，定时调度任务等（可以捕获和处理 Hive 命令执行过程中的错误，例如语法错误、连接错误等。同时，可以将执行日志记录到文件中，便于故障排查和后续分析。）

# 4. shell怎么传参

接收参数:采用$0,$1,$2..等方式获取脚本命令行传入的参数,值得注意的是,$0获取到的是脚本路径以及脚本名

!/bin/bash

echo “脚本$0”

echo “第一个参数$1”

echo “第二个参数$2”

# 5. 批量杀死名字带 abc 的进程

pkill-f abc

注:请注意,在执行此命令之前,确保你有足够的权限来杀死这些进程。如果你没有足够的权限,可以使用 sudo 命令来获取管理员权限: sudo pkill -f abc

# 6. Linux怎么查看具体的进程信息

ps aux：显示所有进程的详细信息，包括用户、CPU 使用率、内存占用等。

ps -ef：显示所有进程的信息，类似于 ps aux。

ps -e：显示所有正在运行的进程。

# 7. Shell怎么定时设置

crontab -e

crontab -1

8.例如：01\*\*sh/opt/room/restart.sh

说明1：01***，即每日凌晨1点执行脚本 #crontab语句格式，分时日月周命令

9. 每五分钟执行一次 task.sh 脚本并打印日志到 home 下的 task.log 文件？  
   $/5 * * * *$  date >> /home/task.log; /usr/bin/bash /home/task.sh >> /home/task.log 2>&1

10. shell 编程-定时删除（30天）文件？

#!/bin/sh  
find /opt/soft/log/-mtime +30 -name "*.log" -exec rm -rf {} \;

分析：/opt/soft/log/#为要定时删除文件的文件目录

-mtime +30 #为大于30天以上

-name"*.log" #为文件名称模糊匹配*

-exec #执行

rm -rf {} \; #删除上面匹配到的文件

11. shell 脚本怎么输出日志？

Shell脚本可以使用echo命令来输出日志信息，例如，在脚本开始时可以使用以下命令创建日志文件，并在后续命令中使用重定向符号将输出信息写入日志文件：

指定日志文件路径

LOGFILE=/tmp/my.script.log

然后，在脚本执行过程中，可以使用以下命令将输出信息追加到日志文件中

echo "Script started at $(date)" > $LOGFILE #信息拼接时间写入到日志中

echo "Starting backup..." >> $LOGFILE

执行备份操作

backup_command >> $LOGFILE

echo "Backup completed." >> $LOGFILE

#在脚本结束时，可以输出结束信息并关闭日志文件

echo "Script finished at $(date)" >> $LOGFILE

exec 1>&-2>&#其中，exec 1>&-2>&命令表示关闭标准输出和标准错误输出，相当于关闭日志文件。

12. 如何把 10 万行数据按照每 1000 行切割成 100 个小文件？

后缀为字母：split -l 1000 -a 2 1.txt a_

或后缀为数字：split -l 1000 -a 3 -d 1.txt a_

13. Sed命令：

sed 是一种流式文本编辑器，用于对文本进行模式匹配和替换。它按行处理文本，并根据指定的规则进行匹配和操作。主要用途包括查找和替换文本、删除或插入行、文件重定向等。基本用法：sed 's/模式/替换/g' filename

常见选项：

‘-i’：编辑文件，并将结果写回到原文件中。  
‘-e’：将脚本作为 shell 命令运行。  

- n'：不打印模式空间中的行。

常见脚本命令：

-s：替换指定的字符串，通常以`s/old/new/'的形式使用。如`sed 's/foo/bar/'将会将每行首次出现的"foo"替换为"bar"。  
-d：删除指定的行。如sed '3d' a.txt 将会删除第三行；

`sed /pattern/d a.txt` 查找包含 "pattern" 的行，并将其从输出中删除，如果你希望直接修改源文件，可以使用 -i 选项，sed -i '/pattern/d' filename

- p：打印指定的行。如sed -n '10p'将只打印第十行。

常见示例：

`sed 's/foo/bar/' file.txt`：将文件中每行的第一个"foo"替换为"bar"。  

- sed '3.5d' file.txt : 删除文件中第三到第五行的内容。  
- `sed -i 's/foo/bar/g' file.txt': 将文件中每行的所有 "foo" 替换为 "bar", 并将结果写回原文件, g 表示全局替换, 没有 g 表示替换每行的第一个 'foo', -i 表示修改源文件  
- `cat file.txt | sed 's/foo/bar/'：将文件中每行的第一个 "foo" 替换为 "bar",并将结果打印出来。

14.怎么替换一个xx.txt文本中的“，”为“；”？

sed -i 's/,/;/g' xx.txt

-i 选项表示直接修改源文件，而不是将结果输出到标准输出。

$^\# \mathrm{s} / , / ; / \mathrm{g}^{\prime}$  是sed的替换操作符，其中：

s表示替换操作。

，是被替换的字符串。

；是要替换成的字符串。

g表示全局替换，即一行中的所有匹配都会被替换。

如果你不想修改原文件，而是将结果输出到标准输出，你可以去掉 -i 选项。

如果你不需要全局替换，可以在命令中省略g选项。默认情况下，sed命令只会替换每行中的第一个匹配项。

15. awk 命令：

是一种强大的文本分析工具，用于从文件或输入流中提取和处理数据。它按行处理文本，并根据指定的字段分隔符将每行拆分为多个字段。awk 使用一种类似编程语言的语法，可以执行条件判断、循环操作、数据计算等功能。

常见选项：

-F：指定文件中字段分隔符。如`-F`表示以冒号为字段分隔符。

- v：设置变量值。如`-v var=value`表示设置变量“var”的值为“value”。

常见脚本命令：

- print : 打印指定的内容。如 `print $1` 表示打印第一个字段。  
- printf：使用指定的格式打印内容。  
  -if/else：条件判断。

常见示例：

`awk '[print $1]' file.txt `：打印文件中每行的第一个字段。  
`^awk -F: ['print $1']' file.txt': 以冒号为字段分隔符,打印文件中每行的第一个字段。  
`awk -v var=value '{print var, $1}' file.txt` : 打印文件中每行的第一个字段前添加变量的值。  
``cat file.txt | awk '{print $1}' `: 和 sed 类似, 将文件中每行的第一个字段打印出来。

# 16. 用哪个命令查找某个文件下的某个关键字

答：如果是用 vi 或 vim 打开文件了，查找方法是：在末行模式输入 '/关键字'：

如果是文件是在当前文件夹目录下,且没有打开,查询方法是:cat 文件名|grep “关键字”； 如果是在某个目录下的多个文件中查找内容中包含的关键字,查找方法是：grep -r “关键字” 目录;

# 17. 将某目录下以 a 开头的文件压缩到 a.tar.gz

答：find -name “a*” | xargs tar -zcvf ../ a.tar.gz

# 18. 将某目录下所有 txt 文件压缩到 a.tar.gz

答：find -name“*.txt”|xargs tar -zcvg ../a.tar.gz

# 19. 常见其他命令

已知程序名称查看进程号：ps -ef | grep 程序名称或 ps -aux | grep 程序名称；杀死进程：kill -9 进程编号；

查看某用户的所有进程：top -u [用户名]；

查看进程所属用户：psu 进程编号：

查看当前 cpu 使用率等相关信息：top #以实时更新的方式显示信息，按下 q 键退出

# 20. 有个文件需要周一早上8点运行，用linux语句怎么运行？（不能借助kettle）

crontab

08**1

# Hadoop

# 1. Hadoop 的构架

使用hdfs进行数据的存储

使用mapreduce进行数据的计算

使用yarn进行资源的调度

使用hive编写sql语句

使用 mysql 存储表格的元数据结构

# 2. 你对Hadoop的了解有多少，跟oracle的区别是什么？

答：它是基于HDFS分布式存储和MapReduce并行运算，通过将mysql/oracle等数据库中的表映射成hive上的一张表，使用hql语句进行相关的增删改查操作，转化成MapReduce程序运行，其次是两者处理的数据大小也有区别，Oracle一般处理千万级别的，hive的话一般都是以亿来衡量的，以上这是我对hadoop的一些理解；

# 3. Hadoop 三大主键，HDFS 是怎么存储数据的。说一下你对 MapReduce 的理解？

答：hdfs 负责存储，mapreduce 负责计算，yarn 负责资源管理。hdfs 中有 nameode，主要负责记录原数据，就是目录结构，文件的属性，大小，副本数等等，还有 secondarynamenode，主要是做一个监控和对 namenode 的快照，还有 datanode 就是负责各个节点的数据存储，默认数据块大小是 128M，副本数是 Mapreduce，有 map 阶段和 reduce 阶段，map 阶段是一个任务拆分的阶段，reduce 阶段汇总结果，我知道的是终将你的结果会写进磁盘。所以效率上来说没有做到实时，会相对慢一些

# 4. hdfs 里面，读取数据流程原理是什么（下载文件）？

读取数据整个过程大致：

1. 客户端申请某个位置的文件或者数据  
2. namenode响应申请，并且将文件和数据所在的datanode节点信息列表返回给客户端  
3. 客户端根据节点信息去向datanode申请数据的读取  
4. datanode响应成功给客户端

5. 客户端开始申请读取 block1  

6. datanode 返回 block1 的数据  
   7.持续申请后面的其他block数据  
   8.datanode持续的返回剩下的其他数据

7. hdfs 里面，上传数据流程原理是什么（上传文件）？

写入数据：

1）客户端要申请写入一个数据  
2）namenode审核文件和数据的合法性  
3) namenode返回允许的响应  
4）客户端开始申请写入  
5）namenode返回datanode的节点信息  
6）客户端找到datanode开始申请写入数据  
7) datanode 同意进行数据写入  
8) 客户端开始上传数据

8.1 datanode开始向其他的datanode申请备份  
8.2 其他的datanode同意备份  
8.3开始备份  
8.4 备份完成

9) datanode 回应客户端表示写入成功
6. mapreduce 的原理和流程是什么？

input: 先获取在计算过程中所需要的数据

split：对大的数据进行切割的操作，将数据分成一块块的

map：数据的映射。将分配好的数据，给到不同的进程去运行

shuffle: 每个进程单独的对自己拿到的数据进行计算（对每个数据进行数据拆分（分区）并且进行排序，保存在硬盘上，从硬盘上读取数据，并且进行计算，结果再次排序）

reduce: 将单独的数据进行总体汇总的计算过程

initialize: 将计算结果输出

7. shuffle的基本流程？

数据进行分区 sort排序一存入硬盘一从硬盘读取数据一聚合计算等等一数据排序

8. HDFS如何保证数据的一致性？

数据一致性：

一般来讲，datanode与应用交互的大部分情况都是通过网络进行的，而网络数据传输带来的巨大问题就是数据是否原样到达。为了保证数据的一致性，HDFS采用了数据校验和(checkSum)

机制。创建文件时，HDFS 会为这个文件生成一个校验和，校验和文件和文件本身保存在同一空间中。传输数据时会将数据与校验数据和一同传输，应用收到数据后可以进行校验，如果两个校验的结果不同，则文件肯定出错了，这个数据块就变成无效的。如果判定无效，则需要从其他 datanode 上读取副本。

9. Hadoop体系中有哪些数据处理引擎，他们之间的差异有哪些？

mapreduce：进程级别的计算引擎、计算过程比较稳定，但是消耗资源比较多，基于硬盘进行计算的

spark：线程级别的计算引擎，过程没有mr稳定，但是消耗资源少，基于内存计算的

flink：流式计算引擎，可以支持非常大的用户并发计算

10. Hadoop 平台，你用过和知道哪些不同的组件？

离线的部分：sqoop yarn hdfs mapreduce hive

实时的部分：flume(日志信息的收集)kafka(消息队列的处理)hbase(一种列式存储的数据库)

spark(基于内存的计算引擎) flink(流式处理的计算引擎)

11.hdfs里面，常用的操作命令有哪些？上传、下载、合并等等。

hadoop fs -put（上传）：hadoop fs -get（下载） hadoop fs -appendToFile（合并）

12. 你们使用的hadoop是什么环境什么版本的？

hadoop 开源版 3.1.1 ; Hive 开源版 3.1.2 ; MySQL 5.7 ; Sqoop: 1.4.7

hadoop cdh 版本 5

# 大数据Hive问题

1. Hive下如何查看数据表信息，说说你知道的？

答·方法1：查看表的字段信息：desc table_name;

方法2：查看表的字段信息及元数据存储路径：descextendedtable_name;

方法3：查看表的字段信息及元数据存储路径：desc formatted table_name;

备注：查看表元数据存储路径是，推荐方法3，信息比较清晰。

2 加载数据到数据库的表，使用什么方法？

load data inpath 'hdfs路径' into table 库名. 表名 partition(分区字段=分区值);  
load data local inpath 'linux 路径' into table 库名. 表名 partition(分区字段=分区值);  
hadoop fs -put linux 文件的位置和名字 表格在 hdfs 的路径

# 3. HIVE常用函数

# 1）日期函数

from_unixtime 时间戳转日期；

unix_timestamp 获取时间戳；

current_date 当前日期；

to_date 转日期;  
datediff(string 结束时间，string 开始时间) 返回结束日期减去开始日期的天数 返回天数；

year 获取年；

date_format(current_date(), 'MM.dd') 按指定格式返回的时间 date 如：date_format("2016-06-22", "MM-dd") = 06-22

获取当前时间：select from_unixtime(unix_timestamp()) current_timestamp

# 2）字符函数

regexpextract正则表达式解析函数；

regexp_replace('hello,world'，‘o|l'，‘e')；正则表达式替换

substr, substring 字符串截取

reverse 字符串翻转函数

trim 去空格

split 分割字符串函数

instr  $(xx,x)$  返回数字没有返回0

# 3) 类型转换函数

cast(expr as) 将 expr 转换成 type 类型如：cast("1" as BIGINT) 将字符串 1 转换成了 BIGINT 类型，如果转换失败将返回 NULL

# 4）字符串查找函数：

locate

语法：locate(查找内容，字段列名字）返回值：int

说明：返回字符串 substr 在 str 中从 pos 后查找，首次出现的位置，找不到返回 0 instr('源字符串', '目标字符串')

数据类型转换 cast

# 4. Hive中常用的系统函数有哪些？

date_add(str, n)、date_sub(str, n) 加减时间

next_day(to_date(str), 'MO') 周指标相关, 获取 str 下周一日期

date_format(str,'yyyy') 根据格式整理日期

last_day(to_date(str)) 求当月最后一天日期

collect_set(col) 收集数据返回一个以逗号分割的字符串数组

get_json_object-jsdata, $ .object') 解析 json, 使用 $$ object' 获取对象值
NVL(str, replace) 空字段赋值, str 为空返回 replace 值; 两个都为空则返回 null

# 5. Hive导入数据的五种方式

1. Load 方式，可以从本地或 HDFS 上导入，本地是 copy，HDFS 是移动

本地：load data local inpath '/root/student.txt' into table student;

HDFS: load data inpath '/user/hive/data/student.txt' into table student;

2. Insert方式，往表里插入

insert into table student values(1,'zhanshan');

3. As select 方式，根据查询结果创建表并插入数据

create table if not exists stu1 as select id,name from student;

4. Location方式，创建表并指定数据的路径

Create external if not exists stu2 like student location '/user/hive/warehouse/student/student.txt';

5.Import方式，先从hive上使用export导出在导入

import table stu3 from '/user/export/student';

# 6. hive怎么更新数据效率更高？

最高的方法，使用hadoop fs将数据下载下来，然后使用Linux命令更新之后再上传。

# 7. 如果某个数据有问题，如果更新这个数据？

1. -get这个文件，修改文件本身，删除hdfs原文件，在-put上传上去  
2. 通过select语句修改数据本身写入另一个表，然后删除原表，重新命名新表  
3. 如果是 orc 类型的分桶表格，通过打开 ACID 的方法，进行数据的 update 更新

# 8. Hive导出数据的五种方式

1. Insert方式，查询结果导出到本地或HDFS

Insert overwrite local directory '/root/insert/student' select id, name from student;  
Insert overwrite directory '/user/ insert /student' select id, name from student;

2.Hadoop命令导出本地

hive>dfs -get /user/hiveWarehouse/student/ 000000_0 /root/hadoop/student.txt

3.hive Shell命令导出

$ bin/hive -e 'select id,name from student;' > /root/hadoop/student.txt

4. Export导出到HDFS

hive> export table student to '/user/export/student';

Snoop导出

# 9. 写出Hive中split、coalesce及collect_list函数的用法（可举例）？

split将字符串转化为数组，即：split('a,b,c,d'，'，') => ["a","b","c","d"]。

coalesce(Tv1, Tv2, ...) 返回参数中的第一个非空值；如果所有值都为 NULL，那么返回 NULL。collect_list 列出该字段所有的值，不去重 => select collect_list(id) from table。

# 10. 什么是hive内部表？

10. 什么是 hive 内部表？内部表是指 Hive 自己创建并管理的表，数据存储在 Hadoop 分布式文件系统中的某个目录下，类似于关系数据库中的物理表。内部表的优点包括：

数据处理速度快，因为数据存储在HDFS中，可以使用MapReduce并行计算框架进行高效处理；可以使用Hive的优化器进行查询优化，支持多种查询语言，比如SQL、HiveQL等；内部表支持数据压缩和分区存储等特性，具有更好的存储优化和管理能力。

# 11. 什么是hive的外部表？

外部表是指 Hive 中定义但不直接管理的表，实际数据存储在 Hadoop 分布式文件系统中的某个目录下。外部表的优点包括：

节省HDFS存储空间，因为数据存储在外部，可以将数据集中存储并按需读取；

可以使用不同的数据格式存储数据，并支持数据共享和交互；

处理大量数据时，外部表可以提供更好的性能和可扩展性，比如可以使用Hadoop eco-system中的Spark、Impala等工具进行数据处理。

# 12. Hive 内部表和外部表的区别？

创建表时：创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，加一个external关键字，仅记录数据所在的路径，不对数据的位置做任何改变。

删除表时：在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据（表结构），不删除数据。这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。在实际应用中，我们需要根据数据的特点和业务需求来选择内部表或外部表。如果数据量较小、

需要频繁更新,可以使用内部表；如果数据量较大、需要分析和共享,可以考虑使用外部表。 当需要将外部数据源与 Hive 数据进行关联查询时,可以考虑使用外部表。

# 13. Hive 中的压缩格式 TextFile、SequenceFile、RCfile、ORCfile 各有什么区别？

答：了解的，主要有textfile、sequencefile、orfile、rcfile、parquet等等项目上常用的  
是orfile,接下来可以答orfile的特点，没问就不回答；

textfile 存储空间消耗比较大，并且压缩的 text 无法分割和合并 查询的效率最低，可以直接存储，加载数据的速度最高；

sequencefile 存储空间消耗最大, 压缩的文件可以分割和合并, 查询效率高, 需要通过 text 文件转化来加载;

orfile, rcfile 存储空间最小，查询的效率最高，需要通过 text 文件转化来加载，加载的速度最低；

parquet 格式是列式存储，有很好的压缩性能和表扫描功能：

备注：SequenceFile, ORCFile(ORC), rcfile 格式的表不能直接从本地文件导入数据，数据要先导入到 textfile 格式的表中，然后再从 textfile 表中导入到 SequenceFile, ORCFile(ORC)，

# 14. ①你们项目中用的是什么存储格式？

答：orfile，因为查询效率快，存储空间小

② orfile orc 的话解压之后数据也很大，为什么说它查询效率快，存储空间小？那 text file 不需要压缩不是更快吗？

答：这是因为 ORC 是 Hive 中一种优化的列式存储格式，它被设计用于提高查询性能并减少存储空间的占用，ORC 文件在解压之后的数据大小与其他非压缩的存储格式（如文本文件）相比可能会更大，这是因为 ORC 文件采用了一些压缩和编码技术来优化存储空间。然而，尽管解压后的数据大小可能大，但 ORC 在查询性能和存储空间方面仍然具有优势，这是因为：

列式存储：ORC将数据按列存储，而不是按行存储。这使得Hive在查询时可以仅读取和解析所需的列，而不必读取整个行。这种列式存储方式可以显著减少磁盘I/O，提高查询性能。

列式压缩：ORC采用了多种压缩算法，如LZ0、Snappy和Zlib等，可以根据数据特征选择最适合的压缩算法。由于列式存储的特点，相同或相似的值通常连续出现在一列中，这种连续性可以提高压缩效率。因此，ORC文件可以在存储时有效地减小数据的大小。

列式索引：ORC支持列式索引，可以加速数据的快速定位和过滤。通过在ORC文件中构建和使用索引，Hive可以跳过不相关的行和列，只读取满足查询条件的数据，从而提高查询性能。综上所述，尽管ORC文件在解压后可能会占用较大的存储空间，但它通过列式存储、列式压缩和列式索引等技术，可以在查询性能和存储空间方面提供较好的优化效果。这使得ORC成为了Hive中常用的存储格式之一。

# ③但是相比 orfile, textfile 不需要压缩查询效率不是更快吗？

③在压缩格式中，使用非压缩的文本格式（如TextFile）在插入和查询数据时可能会更快。这是因为非压缩格式的数据存储在磁盘上是以原始文本形式存在的，不需要进行解压操作，因此可以减少解压的开销。

当使用非压缩的文本格式存储数据时，插入操作通常会更快，因为数据可以直接写入文件，无需进行压缩和编码。而在查询操作中，由于数据以原始文本形式存储，直接读取和解析数据也相对较快。

然而，需要注意的是，非压缩的文本格式可能会占用更多的存储空间。由于数据没有经过压缩，其大小通常比压缩格式要大。这可能导致占用更多的磁盘空间和网络传输带宽。

因此，在选择存储格式时，需要根据具体场景和需求综合考虑。（如果对数据的存储空间要求较高，并且对查询性能的要求相对较低，那么非压缩的文本格式可能是一个合适的选择。如果对存储空间有一定的容忍度，并且对查询性能有较高的要求，那么压缩格式如ORC可能更适合。）

# 15. hive 中表 A 的存储格式 textfile 要转换成其他格式, 比如说 orcfile, hql 应该怎么实现?

可以使用 ALTER TABLE 语句来修改表的存储格式: ALTER TABLE A SET FILEFORMAT ORC; (注, 这个操作将会改变表 A 的存储格式, 并且已经存在的数据文件将会被重新组织和转换为 ORC 格式。因此, 在执行这个操作之前, 请确保你已经备份了重要的数据, 以防意外发生。)

# 16. 对 Hivesql 熟练吗，了解多少？说说插入数据和删除数据的理解。

16. 对 hivesql 然后再行修改，将 hivesql 的语法差不多，我之前有一些临时支援的项目用的 hivesql，基本掌握 hivesql 的语法和函数，知道有内部结构表，外部表，分区表，也知道数据文件存储在 hdfs 上面，原数据在 mysql 当中，hivesql 中一般是不支持数据的修改和删除的，但是可以通过 overwrite 进行一个覆盖写入，比如用空表覆盖原来的表代替删除，或者用新的结果集覆盖写入表中，进行一个修改

# 17.比如为什么不支持直接删除和更新，只能通过覆盖写入实现？

17. hive 为什么不支持直接删除和更新,只能通过建立一个 (注：在 Hive 中,不支持直接删除和更新操作的限制适用于所有类型的表,包括内部表和外部表。)

答：这是因为 Hive 是基于 Hadoop 的分布式数据处理框架，设计初衷是为了支持大规模数据的批处理和数据仓库查询。在这种场景下，Hive 更关注数据的读取和查询性能，而不是实时数据的更新和修改。因此，Hive 在设计时选择了一种简化的数据模型，即将数据视为不可变的，并采用追加写入（Append-only）的方式来处理数据。

(以下是一些具体原因,不问不答,或者是面试官不满意答案的时候可以说说,不要机械式照人之念,多营造自己的思考过程和自己的话语)

以下是一些原因解释为什么Hive不支持直接删除和更新操作：

数据存储方式：Hive中的表通常以文件的形式存储在HDFS中，而HDFS是一种追加写append-only）的文件系统，不支持直接修改已有的文件内容。因此，无法直接在已有文件中删除或更新数据。

数据处理模型：Hive 采用的是类 SQL 的查询语言 (HiveQL), 并且借助 MapReduce 等批处理框架来执行查询。在这种模型下, 删除和更新操作通常需要对整个数据集进行扫描和修改, 这在大规模数据上会导致性能问题。相反, Hive 更适合进行批量插入和查询操作。

数据保留和可追溯性：Hive 的设计目标之一是保留和追溯大规模数据的历史变化。通过将数据以追加写的方式存储，并保留历史版本，可以更好地支持数据审计、数据分析和数据回溯等需求。

# 18.hive支持直接在有数据表格里进行批量插入操作吗？（外部表不支持）

答：可以通过INSERT INTO语句实现批量插入数据到已有的表如：INSERT INTO table target_table SELECT * FROM source_table;

也可以选择性地插入特定的列或应用过滤条件：

INSERT INTO table target_table (col1, col2, col3) SELECT col1, col2, col3 FROM source_table WHERE condition（条件）;

(需要注意的是, 插入选择操作会将源表的数据追加到目标表中, 不会删除或替换目标表中已存在的数据。因此, 在执行插入操作之前, 应该确保目标表的结构与源表相匹配。

通过插入选择操作，你可以在Hive中方便地进行批量插入操作，将数据从一个表复制到另一个表，进行数据迁移或合并等操作。)

# 19.hive sql和Oracle的区别，hive相比sql有什么优势？

答：语法上没有什么太大的区别，就是函数会比 oracle 多一些，数据类型也多了 array 和 map 类型，其他的差别就很多，数据文件存储在 hdfs 上面吧，原数据在 mysql 中，执行引擎使用的 mapreduce。数据量少的时候执行效率比 oracle 要低，但是可以执行一些很大的数据。oracle 无法执行不了的数据查询，最主要的就是可以解决一些很大的数据量，oracle 无法实现查询。

# 20. hive表连接和其他数据库有什么区别？

hive使用left semi join替换子查询嵌套；表连接不能使用on后面的非等值查询。可以用where

内部资料，查版必究

# 21. 说说你对hive中join和left semi join(左半连接)的了解？

答：LEFT SEMI JOIN 的限制是，右侧表只能在连接条件（ON 子句）中引用，而不能在 WHERE 或 SELECT 子句等中引用。left semi join 只能查询左表字段，不能查询右表字段。left semi join 只能展示两个表能够关联上的数据。大多数情况下 JOIN ON 和 left semi on 是结果是相同对等的；但如果 A、B 两张表进行连接，B 表中存在重复的数据，两种连接方式的结果不一致，当使用 JOIN ON 的时候，A、B 表会关联出两条记录，应为 ON 上的条件符合都返回回来；而是用 LEFT SEMI JOIN 当 A 表中的记录，在 B 表上产生符合条件之后就返回，不会再继续查找 B 表记录了，所以如果 B 表有重复，也不会产生重复的多条记录。

# 22. hive 里面经常用到的几个不同的排序分别是什么，有什么区别？

order by 是整个表格的数据当成一个进程进行整体的排序, 只有一个 reduce 工作

sort by 每个 mapreduce 中进行排序，一般和 distribute by 使用，且 distribute by 写在 sort by 前面。当 mapred.reduce.tasks=1 时，效果和 order by 一样；

cluster by 只能对该列进行升序排序，当 distribute by 和 sort by 的字段相同时，可以使用 cluster by 代替

- distribute by 类似 MR 的 Partition，对 key 进行分区，结合 sort by 实现分区排序，本身没有排序功能，需要和 sort by 一起（可以结合桶表使用，给桶中的数据排序。）

# 23.hive中怎么给数据打标签

使用分析函数进行数据的统计，然后可能需要对数据 case when 进行判断。

# 24. 使用过Hive解析JSON串吗？

答：Hive处理json数据总体来说有两个方向的路走：

a. 将 json 以字符串的方式整个入 Hive 表, 然后通过使用 UDF 函数解析已经导入到 hive 中的数据。比如使用 LATERAL VIEW json_tuple 的方法, 获取所需要的列名。

b. 在导入之前将 json 拆成各个字段，导入 Hive 表的数据是已经解析过的。这将需要使用第三方的 SerDe。

# 25. 简述Hive主要架构及解析成MR的过程

注：Hive元数据默认存储在 derby数据库，不支持多客户端访问，所以需要将元数据存储在MySQL中，才支持多客户端访问。主要架构如下：

# Hive解析成MR的过程：

Hive 通过给用户提供一系列交互接口，接收到用户的指令 (sql 语句)，结合元数据 (metastore)，经过 Driver 内的解析器，编译器，优化器，执行器转换成 mapreduce（将 sql 转换成抽象语法树 AST 的解析器，将 AST 编译成逻辑执行计划的编译器，在对逻辑执行计划进行优化的优化器，最后将逻辑执行计划转换成 mapreduce），提交给 hadoop 中执行，最后将执行返回的结果输出到用户交互接口。

# 26.hive的sql语句是怎么运行的

26. hive 的 sql 语句是那么运行的?
    hive 窗口中编写 sql 语句, 然后会去 MysqI 里面进行表格结构和位置的查询, 如果表格结构没有问题, 那么就会将 sql 拆分成不同的关键字, 然后调用 mapreduce 对应关键字的 java 脚本模板, 如果 mr 的模板文件运行没有问题, 就会调用 yarn 进行资源的申请和分配, 然后使用 mr 进行数据的计算, 最后将计算的结果再回显到 hive 数据的窗口中。

# 27. Hive 的两张表关联，使用 MapReduce 怎么实现？

如果其中有一张表为小表，直接使用map端join的方式（map端加载小表）进行聚合。如果两张都是大表，那么采用联合key，联合key的第一个组成部分是join on中的公共字段，第二部分是一个flag，0代表表A，1代表表B，由此让Reduce区分客户信息和订单信息：在Mapper中同时处理两张表的信息，将join on公共字段相同的数据划分到同一个分区中，进而传递到一个Reduce中，然后在Reduce中实现聚合。

# 28.hive怎么动态的筛选出今天之前的数据？

可以使用（内置函数）current_date()函数获取当前日期，并与表中的日期字段进行比较。如：SELECT * FROM your_table WHERE date_column < current_date();

需要注意的是，current_date()函数返回的日期没有时间部分（即时分秒为0），因此与日期字段进行比较时，确保日期字段只包含日期部分，而不包含时间。

# 29. Hive 与传统数据库的区别

答：Hive和数据库除了拥有类似的查询语言外，无其他相似

存储位置：Hive数据存储在HDFS上。数据库保存在块设备或本地文件系统

数据更新：Hive不建议对数据改写。数据库通常需要经常修改。

执行引擎：Hive通过MapReduce来实现。数据库用自己的执行引擎

执行速度：Hive执行延迟高，但它数据规模远超过数据库处理能力时，Hive的并行计算能力就体现优势了。数据库执行延迟较低

数据规模：hive大规模的数据计算。数据库能支持的数据规模较小。

扩展性：Hive建立在Hadoop上，随Hadoop的扩展性。数据库由于ACID语义[wh1]的严格限制，扩展有限

# 30. hive中有索引吗？

答：Hive 在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些 Key 建立索引。Hive 要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。建了索引不会全表扫描，由于 MapReduce 的引入，Hive 可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势，我记得 hive 新版本是直接把索引废弃了（数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了 Hive 不适合在线数据查询。）

# 31. Hive 的函数：UDF、UDAF、UDTF 的区别？

UDF：单行进入，单行输出

UDAF：多行进入，单行输出

UDTF：单行输入，多行输出

# 32. 对 sqoop 和 flume 熟悉吗？

答：sqoop 是一个抽数工具，适用于关系型数据和大数据平台之间的数据交换，类似于 kettle，没有图形页面，都是通过 linux 命令来实现的。Flume 是一个实时的日志采集工具，主要是针对一些日志文件，将文件抽取到 hdfs 或者 hive 中进行一个处理。

# 33. 介绍下项目上常用的 hivesql 的工具是什么？

注：HUE，是运营和开发hadoop应用的图形化用不界面，Hue程序被整合到一个类似桌面的环境，以web程序的形式发布，对于单独的用户来说不需要额外的安装，只需要登入一个网站，类似于牛客网的那个页常用的面。可以执行sql，可以看结果，也有类似plsql左边的目录树，可以看到hive库中的表，也可以看到hdfs中的文件，还可以关联spark，oracle，mysql。是一个集成工具。

DBeaver，集成式的安装工具，类似于parsing，可以连接各种各样的数据库或者hive。

# 34. Hive中查看日志的方式有哪些？

# 1、使用CLI命令参数：

在运行Hive CLI时，可以将-v或—verbose选项作为命令行参数传递给CLI，以启用更详细的项目志记录输出。例如：

$$
h i v e - v - e " S E L E C T * F R O M m y _ {t a b l e} ; "
$$

此命令将运行Hive查询并显示更详细的日志记录输出，例如输入、输出、执行计划等。

# 2、查看日志文件：

默认情况下,Hive将运行日志记录到$HIVE_HOME/logs/<username>/<hive.log文件中,其中<username>是当前用户的用户名。你可以使用以下命令查看该文件的内容:

$$
\begin{array}{l} \text {l e s s} \quad \text {S H I V E} \quad \text {H O M E} / \log s / \langle \text {u s t r a n e m e} \rangle / \text {h i v e}. \log \\ \# \text {注 意 ，} \\ S H I V E _ {\text {H O M E}} \text {是 H i v e 的 安 装 目 录 。} \\ \end{array}
$$

# 3、使用Web UI查看日志：

Hive 提供了一个 Web UI，可以使用它来查看运行日志和其他信息。要使用 Web UI，请执行以下步骤：

启动Hive服务并确保Web UI已启用。

在Web浏览器中输入http://<hive_server>:/port>/

其中<hive_server>是Hive服务器的主机名或IP地址，<port>是Web UI端口（默认为10002）。在左侧菜单中选择“Query”，然后选择要查看日志的查询。单击“Logs”选项卡，将显示查询的运行日志。

以上是三种查看Hive运行日志的方法，可以根据需要使用其中任意一种。

# HIVE分区&分桶

# 1. Hive中分区的概述

分区就是对某列有相同的数据或者某一个数据范围的数据进行分类，在查询时就可以针对分区查询，不必进行全表扫描，对分区表执行insert时，表名后要跟partition子句。

# 2. Hive如何实现分区？

建表：create table tablename(col1 string) partitioned by (col2 string);

添加分区：alter table tablename add partition(col2=' 202101');

删除分区：alter table tablename drop partition(col2='202101');

# 3. 如何删除分区呢？

alter table 表名 drop partition(分区字段=’ 分区值 ‘);

load data inpath '路径' into table 库名. 表名 partition(分区字段=分区值);

insert overwrite table 表名 partition(分区字段=分区值) select * from 另一个表名;

# 4. 一个分区表，一个非分区表，如何用非分区表中的字段作为分区表的分区值？

直接做动态分区就好了，使用 insert overwrite table 分区表 partition(分区字段) select xxxx from 非分区表；

# 5. 动态分区和静态分区的区别？

1. 静态分区可以load data和insert两种方式添加数据，动态只能通过insert的方式添加数据  
2. 静态需要自己指定分区的内容，动态是通过查询结果自动指定和分配的  
3. 动态需要打开动态分区和非严格模式的开关

set hive.exec.dynamict.partition=true;  
set hive.exec.dynamict.partition_mode=nonstrict;  
4. 一般情况下都是使用静态分区，动态的处理数据的速度相对慢一点。

# 6. hive 分桶概述

分桶表是对列值取哈希值的方式，将不同数据放到不同文件中存储。对于hive中每一个表、分区都可以进一步进行分桶。由列的哈希值除以桶的个数来决定每条数据划分在哪个桶中。

# 7. 什么是分桶表

分桶表, 比普通表或者分区表有着更为细粒度的数据划分。举个例子, 每天产生的日志可以建立分区表, 每个分区在 hdfs 上就是一个目录, 这个目录下包含了当天的所有日志记录。而分桶表, 可以进一步对当天的日志按用户划分成多个文件。划分的依据是用户 id 取 hash, 然后对分桶数量求余, 每个分桶文件在 hdfs 上是一个独立的文件。

# 8. 说说对Hive桶表的理解？

桶表是对数据某个字段进行哈希取值，然后放到不同文件中存储。

数据加载到桶表时，会对字段取hash值，然后与桶的数量取模。把数据放到对应的文件中。物理上，每个桶就是表(或分区）目录里的一个文件，一个作业产生的桶(输出文件)和reduce任务个数相同。

桶表专门用于抽样查询，是很专业性的，不是日常用来存储数据的表，需要抽样查询时，才创建和使用桶表。

# 9. 什么时候可以使用分桶表

分桶表最主要的使用场景是优化大表和大表的join，其主要原理如下：

(1) 如果大表和大表使用 MapReduce 的普通模式, 会在 reduce 端 shuffle, 那就非常可怕,一个是慢, 另一个是容易出异常;

(2) 而分桶表将大表的数据划分成一个个小块，分别在 Map 端做 join。

(2) 而分桶表有类集的数据结构, 一个数据表可以包含多个数据表。如果两个数据表中元素之间之所以可以这样, 是因为分桶表在建表的时候, 需要指定分桶的字段, 对这个字段值取 hash 后, 对分桶的个数取余数获得一个值, 根据这个值将数据放到不同的桶里去。

对桶的个数取系数值一个值，再把这个值与表中的数据进行比较，如果相同 key 的数据都在一个桶里，在表和表关联的时候就不需要去扫描整个表，只需要去扫描对应桶里的数据即可。

(3) 由于不同的数据落到哪个桶是由分桶个数决定的，所以做 Join 的两个分桶表的桶个数必须是相等或者成倍数；  
(4) 分桶表的每个桶必须要排序, 这样可以更高效的做 map join。  
（4）分桶表的每个桶必须要分组，这样，SMB map join 就会把每个桶的每个元素放到 SMB map join。这样的 join 称为 SMB map join（Sort Merge Bucket Map Join），核心思想是大表化成小表，分而治之。

# 10. 开启支持分桶

set hive.enforce.bucketing=true;

默认：false；设置为true之后，mr运行时会根据bucket的个数自动分配reduce task个数。（用户也可以通过mapred.reduce.tasks自己设置reduce任务个数，但分桶时不推荐使用）

注意：一次作业产生的桶（文件数量）和reduce task个数一致。

场景：小表关联大表时效果明显（Map Side Join），会把小表数据通过DistributedCache分发到各个Map Side，然后加载到内存和每一个Map任务处理的大表进行JOIN，这样就不必要去做Reduce JOIN。JOIN时是按照桶来JOIN的，大大减少了JOIN的数量

示例：create table t1(id int，name string) clustered by (id) into 8 buckets stored in cc=cc|th|properties('transactional'= 'true');

关键字clustered声明划分桶的列和桶的个数，这里以id来划分桶，划分8个桶。Hive会计  
算id列的hash值再以桶的个数取模来计算某条记录属于哪个桶。

# 11. 分桶操作

莫忘记开启分桶支持：sethive.enforce.bucketing=true;

往分桶表中加载数据：

insert into table bucket table select columns from tbl;或者

insert overwrite table bucket_table select columns from tbl;

桶表 抽样查询

select * from bucket_table_tablesample(bucket 1 out of 4 on columns);

TABLESAMPLE 语法：

TABLESAMPLE (BUCCKET x OUT OF y)

x：表示从哪个bucket开始抽取数据

y：必须为该表总bucket数的倍数或因子

当表总bucket数为32时

TABLESAMPLE (BUCKET 3 OUT OF 16), 抽取哪些数据? 共抽取 2 (32/16) 个 bucket 的数据, 抽取第 3、第 19 (16+3) 个 bucket 的数据

TABLESAMPLE (BUCKET 3 OUT OF 8),抽取哪些数据？共抽取4（32/8）个bucket的数据，抽取：3，11,19,27

TABLESAMPLE (BUCKET 3 OUT OF 256), 抽取哪些数据? 共抽取 1/8 (32/256) 个 bucket 的数据, 抽取第 3 个 bucket 的 1/8 数据

# 12. 分桶的作用是什么？数量你是如何决定的

答：作用是进行表格连接的查询加速；加快表格的抽样查询。总的文件大小/(block size*2) = 100分桶数量

# 13. 为什么分桶可以对表格的联合查询加速呢？

答：因为分桶可以减少表格笛卡尔积的数量。

# 14. 分区和分桶的区别？

1. 分区是通过 partitioned by（新字段）进行分区设置的，分桶是通过 clustered by（已有字段）进行分桶设置的  
   2.分区是以文件夹的方式分开保存数据的，分桶是分成不同的文件保存数据的  
2. 分区是在查询分区字段的时候加快查询速度，分桶是对分桶字段进行表格的联合查询的时候进行联合查询加速的

# 大数据性能优化

# 1. 说说你对hive优化器的理解

Hive的优化器是自带的功能，无需手动开启。当你在Hive中提交查询时，优化器会自动处理查询，并尝试优化查询计划以提高性能。优化器的一些基本优化策略是默认启用的，例如列剪裁和谓词下推。然而，有些高级的优化策略可能需要手动设置相关参数才能生效。例如，数据倾斜处理和统计信息收集可能需要手动配置相关参数来启用

# 2. Hive 优化器主要负责那些优化？

2. Hive 优化器主要参数见表 10-1。在 Hive 中，优化器负责优化查询计划以提高查询性能。Hive 的优化器主要有以下几个方面的优化：

优化：查询重写：优化器会尝试将用户提交的查询重写为更高效的形式。例如，它可以将复杂的查询转换为等效的更简单的形式，或者利用索引或分区等数据组织方式来加速查询。

转换为等效的更简单的形式，或者利用并行或分立等效电路。例如，列剪裁：优化器会尝试只选择查询所需的列，而不是全部列。这可以减少磁盘I0和网络传输开销，并提高查询性能。

- 销，并提高查询性能。
- 谓词下推：优化器会尝试将过滤条件下推到数据源，以减少需要处理的数据量。这样可以减少10开销，并提高查询性能。
- 业务模式优化：此技术查处理数据倾斜。

10 开销，并提高查询性能。数据倾斜处理：当数据在集群中不均匀分布时，优化器会尝试使用一些技术来处理数据倾斜。例如，它可以将数据按键重新分区或使用样本数据来估计倾斜因子，并根据情况进行数据重分布。

布。统计信息收集：优化器需要准确的统计信息来做出更好的优化决策。因此，它会收集关于表、列和分区的统计信息，如行数、数据分布等。这些统计信息可以用于选择更好的查询计划。

# 3. Hive优化有哪些？

3. Hive优化有哪些?
- hive的性能优化，一般要先判断是否有数据倾斜，如果有数据倾斜，找到倾斜的原因，一般比较常见的都是因为数据分布不均匀或者出现大量空值，可以采用拼接或填充随机值，打散数据分布，避免大量重复值落在同一个reduce中，从资源角度也可以加大reduce的个数如果排除数据倾斜依然没有什么区别，会从几个方面去考虑：

首先是检查表的分区和文件格式，是否有分区等

首先是检查表的分区和文件格式，是否符合现行会计准则的要求。第二点，就是一个计算和关联的逻辑顺序问题，如果有几个表关联，可以先把小的表关联起来，再关联大的表，另外有涉及计算的，可以考虑先进行一个计算，再进行关联。比如之前汇总产品订单表，我们一般都是在交易表进行sum求和以后，用产品表leftjoin进行关联。

品的交易额，我都是先在交易表进行sum求和项后，再用sum求和项来计算。如果两个字段中同时有多个字段，那么第一个就是第i个字段，以此类推。第三点，考虑一下代码，第一个就是层层过滤数据，减少每个阶段的数据量，只保留自己需要用到的字段；第二个是如果用到多个窗口函数，多个窗口函数尽量在同一层执行；第三个，用一些更加高效的代码，比如distinct，order by，union等等，可以考虑使用group by去重，或者使用distributeby+sort by来替换order by，这是我平时工作里对性能问题的一个解决经验。

# 4. 小文件产生的原因

(1) 每次 insert 插入产生小文件。  
2) 分桶产生小文件  
(2) 分输产生电流；(3) 系统本身产生的文件。比如每天的财务数据，数据量不是特别大的

# 5. 小文件处理方法

在 map 执行前合并小文件，减少 map 数

（后面操作没问就不说）

CombineHiveInputFormat 具有对小文件进行合并的功能（系统默认的格式）

set mapred.max.split.size=112345600;

set mapred.min.split.size.per.node=112345600;

set mapred.min.split.size.per.rack=112345600;

set hive. input. format  $\equiv$  org.apache.hadoop.hive.ql.io CombineHiveInputFormat;

这个参数表示执行前进行小文件合并，前面三个参数确定合并文件块的大小，大于文件块大小128m的，按照128m来分隔，小于128m，大于100m的，按照100m来分隔，把那些小于100m的（包括小文件和分隔大文件剩下的），进行合并。

# 6. Hive 小文件过多怎么解决？

使用hive自带的concatenate命令，自动合并小文件

调整参数减少Map数量

减少 Reduce 的数量

使用hadoop的archive将小文件归档

ALTER TABLE/PARTITION XXX CONCATENATE:

# 7. Hive如何优化提高查询效率？

# 1) 开启 FetchTask

一个简单的查询语句，是指一个没有函数、排序等功能的语句，当开启一个Fetch Task功能，就执行一个简单的查询语句不会生成MapReduce作业，而是直接使用FetchTask，从hdfs文件系统中进行查询输出数据，从而提高效率。

(2) 合并中间表：将众多的业奇中相同的中间结果集, 扎取到  $n$  个元素

# 3）合理使用分区表

外部表、分区表，结合使用，采用多级分区。数据采用存储格式（textfile、orfcfile、parquet）或者数据压缩(snappy)。

1. 明细数据我们一般采用按天分区, 对于特别大的表, 可以采用子分区, 每个分区其实对应到 HDFS 上就是一个目录。数据存储方式我们可以采用 parquet 列式存储, 同时具有很好的压缩性能; 2.

# 4) 合理设置reduce个数

# 5) 开启并行执行

并行执行，意思是同步执行hive的多个阶段，hive在执行过程，将一个查询转化成一个或者多个阶段。某个特定的job可能包含众多的阶段，而这些阶段可能并非完全相互依赖的，也就是说可以并行执行的，这样可能使得整个job的执行时间加倍。

# 6) 优化 sq1

# 8. HIVE里模糊查询性能很差该怎么优化？

避免使用通配符开头的LIKE查询:LIKE%xxx'会使索引失效,导致扫描所有数据。可以尝试使用前缀索引,例如使用SOUNDEX函数。

使用正则表达式(REGEXP)进行模糊匹配时，如果有明确的字符串内容，尽量避免使用“点号”来表示任意字符，因为它会匹配所有字符(包括回车符、制表符等)，可以用具体的字符替代。使用正则表达像：像“%xxx”SOUNDEX

如果需要频繁地进行模糊查询，可以考虑将数据转换成全文搜索引擎（如Apache Solr或Elasticsearch）中的格式，在搜索引擎上进行查询，然后再将结果合并到Hive中。

对于特定的查询需求，可以考虑建立倒排索引（Inverted Index），以便更快地进行模糊查询。  
对于使用分区表（Partitioned Table）来限制查询范围，减小查询数据量。

优化查询语句，如只查询必要的列，使OR代替多个LIKE操作等。优化查询语

# 9. 数据倾斜

原理是数据资源分布不均匀，大量的相同值（空值、重复值）落在同一个reduce节点上，导致运行不过来，其他运行完成的节点就需要等待这个进程运行完成才能整体结束。

直接表现是：reduce的进程在日志里面卡在了  $99\%$  的部分不动了。

# (1) 怎么解决数据倾斜的现象呢？

设置 mapred.map.tasks; mapred.reduce.tasks;

在句子中尽可能少的使用去重统计

- 3.1 使用hive里面的优化器 /*+mapjoin(小表的名字）*/

- 3.2 调整表格的前后顺序，hive 里面，永远读取 Join 前面的表格

3.3对表格设置分桶

1）使用hive里面的万能开关，来解决异常数据的问题。

set hive.groupby.skewdata=true; 一万能方法

set hive.map.aggr=true; 一任务执行中合并文件的大小

set hive.mapTask;  
set hive.merge size per task=256000000;一调整每个map处理数据量的次数，

set hive.mergc:3120;p57;  
set mapred.map.tasks=10; ——这里设置的是每一个map要处理的map数量。

(2) 填充空值。用字符串+随机值进行空值的填充。  
3) nvi(被计算的列，concat('rand_'，rand()）

# (2）数据倾斜场景怎么解决

# ① 空值引发的数据倾斜

解决方案：

第一种：可以直接不让 null 值参与 join 操作，即不让 null 值有 shuffle 阶段  
第二种：因为 null 值参与 shuffle 时的 hash 结果是一样的，那么我们可以给 null 值随机赋值，这样它们的 hash 结果就不一样，就会进到不同的 reduce 中：

# ② 不同数据类型引发的数据倾斜

解决方案：

如果key字段既有string类型也有int类型，默认的hash就都会按int类型来分配，那我们直接把int类型都转为string就好了，这样key字段都为string，hash时就按照string类型分配了：

# ③ 不可拆分大文件引发的数据倾斜

解决方案：

这种数据倾斜问题没有什么好的解决方案，只能将使用GZIP压缩等不支持文件分割的文件转为bzip和zip等支持文件分割的压缩方式。

所以，我们在对文件进行压缩时，为避免因不可拆分大文件而引发数据读取的倾斜，在数据压缩的时候可以采用bzip2和Zip等支持文件分割的压缩算法。

# ④ 数据膨胀引发的数据倾斜

解决方案：

在Hive中可以通过参数hive.new.job.grouping.set.cardinality配置的方式自动控制作业的拆解，该参数默认值是30。表示针对grouping sets/rollups/cubes这类多维聚合的操作，如果最后拆解的键组合大于该值，会启用新的任务去处理大于该值之外的组合。如果在处理数据时，某个分组聚合的列有较大的倾斜，可以适当调小该值。

# ⑤ 表连接时引发的数据倾斜

解决方案：

通常做法是将倾斜的数据存到分布式缓存中，分发到各个Map任务所在节点。在Map阶段完成join操作，即MapJoin，这避免了Shuffle，从而避免了数据倾斜。

# ⑥ 小表、大表 Join

将 key 相对分散，并且数据量小的表放在 join 的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用 Group 让小的维度表（1000 条以下的记录条数）先进内存。在 map 端完成 reduce。

# ⑦ 大表 Join 大表

1）空KEY过滤

有时 join 超时是因为某些 key 对应的数据太多，而相同 key 对应的数据都会发送到相同的 reducer 上，从而导致内存不够。此时我们应该仔细分析这些异常的 key，很多情况下，这些 key 对应的数据是异常数据，我们需要在 SQL 语句中进行过滤。例如 key 对应的字段为空。

有时虽然某个 key 为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在 join 结果中，此时我们可以表 a 中 key 为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的 reducer 上。

# ⑧ 确实无法减少数据量引发的数据倾斜

解决方案：

这类问题最直接的方式就是调整reduce所执行的内存大小。调整reduce的内存大小为

调整reduce的内存大小使用mapreduce.reduce.memory.mb这个配置。

# 10. 数据倾斜具体处理过程案例

(1) 你刚提到了数据倾斜通过拼接随机值, 拼在主表还是副表, 怎么拼接的?

答：在进行表关联时，如果一张表是大表，另一张表是小表，通常只需要对大表进行拼接随机值操作，而不需要对小表进行相同的操作。这是因为大表的数据倾斜问题更常见，而通过拼接随机值可以更有效地解决大表的数据倾斜。

拼接过程：可以用concat函数拼接随机值函数RAND()

CONCAT（要拼接的列，'_'，CAST（RAND() * 1000 AS INT））；

注：使用CONCAT函数将名字和随机值拼接在一起，并使用CAST函数将随机值转换为整数类型。通过乘以1000，我们可以获取一个在0和1000之间的随机整数。

(2) 在小表里面不用进行拼接随机值操作, 那么只在大表进行拼接, 新生成的列去做成连接条件, 怎么能跟小表进行关联起来, 因为关联条件都不一样呀?

答：当只在大表中进行拼接随机值操作时，确实会出现关联条件不一致的问题，可以考虑将小表复制到大表所在的节点上：如果小表的大小相对较小，可以将小表的副本复制到大表所在的节点上。这样，在进行拼接随机值操作之后，可以直接使用新生成的列作为连接条件进行关联。（这种方法需要保证小表的副本能够完全适应大表所在节点的内存。）具体过程如下（不问就不用说）：

将小表副本复制到大表所在的节点上：

INSERT OVERWRITE DIRECTORY'<temporary_directory>' SELECT \*FROM small_table_copy;  
将小表副本加载到大表所在的节点上：

LOAD DATA INPATH'<temporary_directory>' INTO TABLE big_table;

最后表关联条件为

From big a JOIN small b ON a. id = b. id AND a. new_col = CONCAT(b. age, CAST(RAND() * 1000 AS INT);

11. mapjoin 优化器的原理是什么？（Map Join 是 hive 的一种优化技术）

注：此操作只能用与等值连接，通常是左连接和内连接才会考虑开启。

答：具体来说，MapJoin将一张小表（也称为“小表”或“右表”）完全加载到内存中，而另一张大表（也称为“大表”或“左表”）则通过Map任务按需进行处理。这样，在Map阶段，Hive可以直接从内存中获取小表的数据，并与大表进行关联操作，而无需通过Shuffle和Reduce过程。

12. Map Join 需要开启吗？怎么开启？

答：默认情况下MapJoin是关闭的，可以使用以下HQL语句来启用MapJoin模式

SET hive. auto.convert. join=true;

SET hive.mapjoin.smal1table.filesize  $= 25000000$  （默认25M以下是小表）；一设置小表的大小阈值，指定小表的大小（以字节为单位）。当小表的大小小于该阈值时，将会启用MapJoin模式。

模式。  
SELECT /\*+ MAP JOIN（小表别名）\*/要查询的列

FROM大表

JOIN小表AS小表别名

ON条件；

# 13. Fetch抓取

Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。

例如：SELECT * FROM employees; 在这种情况下，Hive 可以简单地读取 employee 对应的存储目录下的文件，然后输出查询结果到控制台。具体设置过程如下：

打开Hive客户端或其他Hive查询工具执行以下语句：

set hivefetch.taskconversion  $\equiv$  more;

more：表示设置为更高效的数据转换方式（如列式存储）对数据进行转换；

none：不进行数据转换，返回原始的数据格式；

minimal：使用更小的数据格式（如二进制）进行转换，以减少网络传输和内存占用。

备注：另外，可以通过设置hivefetch.taskconversion.threshold属性来调整Fetch操作触发的阈值。该属性指定了当查询结果的行数超过阈值时，才会触发Fetch操作。默认情况下，阈值为10000行。

# 14. 谓词下推

通常情况下, Hive 查询会先读取整个表的数据, 然后应用过滤条件进行筛选。但是, 通过谓词下推优化, Hive 可以将过滤条件提前执行下推到数据源扫描操作中, 只读取满足条件的部分数据, 而不是读取整个表的数据, 从而减少了读取的数据量。通过谓词下推优化, Hive 可以大大减少不必要的数据读取和处理, 提高查询性能。这对于大规模数据集和复杂查询非常有帮助。打开谓词下推设置如下:

hive  $>$  sethivetimize.ppd  $\equiv$  true; #谓词下推，默认是true，（有时候会关闭）

# 15. 本地模式

大多数的 Hadoop Job 是需要 Hadoop 提供的完整的可扩展性来处理大数据集的。不过, 有时 Hive 的输入数据量是非常小的。在这种情况下, 为查询触发执行任务消耗的时间可能会比实际 job 的执行时间要多的多。对于大多数这种情况, Hive 可以通过本地模式在单台机器上处理所有的任务。对于小数据集, 执行时间可以明显被缩短。可以通过设置 hive.exec_mode.local.auto 的值为 true, 来让 Hive 在适当的时候自动启动这个优化:

#开启本地mr

set hive.exec_mode.local.auto=true;

#设置local mr的最大输入数据量，当输入数据量小于这个值时采用local mr的方式，默认为134217728，即128M

set hive.exec_mode.local.auto.Inputbytes.max=50000000;

对设置localmr的最大输入文件个数，当输入文件个数小于这个值时采用localmr的方式，默认为4

set hive.exec_mode.local.auto-input.files.max=10;

# 16. 并行执行

Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。

通过设置参数hive.exec(parallel值为true，就可以开启并发执行：

set hive.exec(parallel=true; //打开任务并行执行，默认为false

set hive.execiardish.hive.set hive.execiardish thread.number=16; //同一个sql允许最大并行度，默认为8  
当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来

(建议在数据量大, sql 很长的时候使用, 数据量小, sql 比较的小开启有可能还不如之前快)。

# 17. 严格模式

它是指Hive可以通过设置防止一些危险操作：

# 1）分区表不使用分区过滤

将hive严格的checks.no.partition.filter设置为true时，对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。

# 2）使用orderby没有limit过滤

将hive严格的checks.orderby.no limit设置为true时，对于使用了order by语句的查询，要求必须使用limit语句。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reduceer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reduceer额外执行很长一段时间(开启了limit可以在数据进入到reduce之前就减少一部分数据)。

# 3）笛卡尔积

将hive严格的checks cartesian product设置为true时，会限制笛卡尔积的查询。关系型数据库中执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。但Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。

# 18.怎么设置map？

设置 map 数量：可以使用 set 命令来设置 mapreduce.job/maps 属性，从而调整 map 数量。例如，将 map 数量设置为 10：set mapreduce.job/maps=10；

# 19.怎么设置reduce?

类似地，可以使用 set 命令来设置 mapreduce.job.reduce 属性，从而调整 reduce 数量。例如，将 reduce 数量设置为 5：set mapreduce.job.reduce=5；

备注：其他设置如设置 map 任务内存为 2048 MB: set mapreduce.map.memory.mb=2048;

需要注意的是，通过HQL语句设置的属性值只在当前会话中生效，对其他会话不产生影响。如果需要全局生效，可以在Hive配置文件（如hive-site.xml）中进行相应的配置。

# 建模问题

# 1. 对数仓和数据集市是否了解？

答：数仓是企业级的一个数据仓库，记录所有的数据，是所有部门取数的统一来源，数据集市是部门级的数据仓库，只有某个部门的一些数据，一般都用于不同行业的业务部门会有一个数据集市，但是这些数据集市经常需要互相之间进行数据验证，而且数据没有搞笑的利用，无法形成闭环。

# 2. OLTP/OLAP的区别

注：OLAP是分析的OLTP是事务操作的

答：这个还是很容易区分的，OLTP是事务操作型数据库，主要是前端业务系统的一种，操作次数多，大部分是新增的，要求响应快。OLAP是分析型数据库，主要是数据仓库，给分析人员用的，操作次数少，大部分是查询，要求相应速度相对来说没那么快，可以有一定的延迟性。

# 3. 说说你对建模的理解？

答：会使用图形化的界面来表示表格和表格之间的关系，以及表格本身的内容。这个表示数据之间的表达关系的图，就叫做ER图。

怎么去设计某个功能需要的表格,以及中间的小表,每个表格有哪些不同的关系等等,这个就是数据建模的过程

# 4. 说说对建模的思路？

下面是对数据库中建模的理解和建模的思路：（可挑几点去答）

理解问题领域：在进行数据库建模之前，需要深入理解业务需求和问题领域。与甲方业务人员沟通，了解业务流程、数据流动、关键概念和规则等。这有助于更好地把握需求，抽取出核心实体和关系。

识别实体和维度：根据业务需求的理解，识别出需要存储的实体和其属性。实体可以是现实世界中的人、物、事件等，属性是描述实体特征和状态的信息。例如，在一个学生管理系统中，学生可作为一个实体，属性包括学号、姓名、年龄等。

定义关系和关联：分析实体之间的关系,确定它们之间的关联方式。 关系可以是一对一、一对多或多对多的关系。通过定义适当的关联关系,可以建立实体间的联系,并便于查询和操作。 例如,在学生管理系统中,学生和课程之间可能存在多对多的关系,可以通过一个中间表来表示学生和课程的关联。

设计表结构和字段：根据实体的属性以及关系的定义，设计数据库中的表结构和字段。每个实体对应一个表，表中的字段对应实体的属性。合理地选择字段的数据类型和约束，确保数据的完整性和一致性。在设计表结构时，还需要考虑数据的规模、性能需求和扩展性。

确定主键和外键：在表中确定主键和外键，用于唯一标识实体和建立关联关系。主键是用于唯一标识表中记录的字段，外键是用于与其他表建立关联的字段。通过主键和外键的定义，建立表之间的关系，并确保数据的完整性和一致性。

优化性能和规范化：在建模过程中，需要考虑数据库的性能和规范化。可以通过合适的索引设计、分区、冗余数据的消除等手段来提高查询和操作的性能。同时，遵循数据库规范化原则，将数据组织为符合规范化要求的形式，以避免数据冗余和更新异常。

# 5. 数据建模的具体过程

(1) 先要分析当前的功能模块的核心功能，以及这个核心功能和其他模块之间的业务关系，分析他们的业务是哪种业务模型（1-1 1-多 多-1 多-多）  
(2）将他们的关系，编辑到业务模型中  
(3) 将业务模型转换成逻辑模型，查看在转换的过程中，中间还需要哪些不同的业务表格进行数据的存储和中转  
(4) 给中间表格进行其他的必要字段的添加  
(5) 给添加完字段信息的逻辑模型，转换成物理模型，最后再查看下有没有需要修改的字段和名字等等直接在工具中，编写对应的 sql 语句，在数据库中运行就可以了

# 6. 比如说：我想看到银行总的贷款金额，同时也想看到各个支行的贷款金额以及详细贷款难详细信息，你要怎样去设计表的结构？（确实事实是贷款事件，往上面添加维度信息关联即可）

答：总贷款表：贷款ID：唯一标识贷款的字段。

贷款金额：记录总贷款金额的字段。

支行表：支行ID：唯一标识支行的字段。

支行名称：支行的名称。

支行贷款表：贷款ID：关联总贷款表的贷款ID字段。

支行ID：关联支行表的支行ID字段。

贷款金额：记录支行贷款金额的字段。

# 贷款详细信息表：

贷款ID：关联总贷款表的贷款ID字段。

客户ID：标识贷款客户的字段。

贷款类型：记录贷款类型的字段，如个人贷款、房屋贷款等。

贷款日期：记录贷款发放日期的字段。

贷款利率：记录贷款利率的字段。

还款计划：记录贷款的还款计划,可以是关联的表或存储结构化数据的字段。

客户表：客户ID：唯一标识客户的字段，用于关联其他表。

客户姓名：记录客户的姓名。

客户联系方式：记录客户的联系方式，如电话号码或电子邮件地址等。

其他客户属性：根据业务需求添加与客户相关的属性，如地址、年龄、性别等。

员工表：员工ID：唯一标识员工的字段，用于关联其他表。

员工姓名：记录员工的姓名。

员工职位：记录员工的职位或角色，如经理、柜员等。

员工联系方式：记录员工的联系方式。

其他员工属性：根据业务需求添加其他与员工相关的属性，如部门、入职日期、薪资等。

# 7. 建模用什么，建模有几种，怎么建模？

答：建模一般是由专门的同事负责或者项目经理主导来做，这中间我也有参与，有ER模型和维度模型，一般用的维度建模，维度模型又有星型模型和雪花模型，主要考虑的是业务需求（指标）怎么落地实现，划分好主题，确定好取数口径，数据源从哪里取，中间也涉及到系统筛选、表级筛选、字段筛选的过程，面试官，这是我对建模的一些理解。

# 8. 数据建模用的哪些模型

# 星型模型

星形模式(Star Schema)是最常用的维度建模方式。星型模式是以事实表为中心，所有的维度表直接连接在事实表上，像星星一样。星形模式的维度建模由一个事实表和一组维表成，且具有以下特点：

a. 维表只和事实表关联，维表之间没有关联；  
b. 每个维表主键为单列，且该主键放置在事实表中，作为两边连接的外键：  
c. 以事实表为核心，维表围绕核心呈星形分布。

![](images/bdbfcf4b3eca331a3f2c5d612aade2059e2c078156b6574355a4bedc89fb74c6.jpg)

# 雪花模型

雪花模式(Snowflake Schema)是对星形模式的扩展。雪花模式的维度表可以拥有其他维度表的，虽然这种模型相比星型更规范一些，但是由于这种模型不太容易理解，维护成本比较高，而且性能方面需要关联多层维表，性能比星型模型要低。

![](images/b1d4906c73a0246e704ab5c303552f79bdb239cd0c112d019007576d8536e604.jpg)

# 星座模型

星座模式是星型模式延伸而来，星型模式是基于一张事实表的，而星座模式是基于多张事实表的，而且共享维度信息。前面介绍的两种维度建模方法都是多维表对应单事实表，但在很多时候维度空间内的事实表不止一个，而一个维表也可能被多个事实表用到。在业务发展后期，绝大部分维度建模都采用的是星座模式。

# 9. 维度建模和关系建模的区别是什么？

关系模型严格遵循第三范式（3NF），较松散零碎，物理表数量多，数据冗余程度低。由于数据分布于众多的表中，这些数据可以更为灵活地被应用，功能性较强。主要应用于OLTP系统中，保证数据的一致性以及避免冗余，所以大部分业务系统的表都遵循第三范式。但是在大规模数据，跨表分析统计查询过程中，会造成多表关联，这会大大降低执行效率。

维度模型通常以某一个事实表为中心进行表的组织，主要面向业务，特征是可能存在数据的冗余，但是能方便的得到数据，主要应用于OLAP系统中，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果

注：最主要区别是有没数据冗余以及应用侧重场景不同

# 10. 维度建模必须注意考虑的因素有哪些？

维度建模是重点解决用户如何更快速完成分析需求，同时还有较好的大规模复杂查询的响应性能，更直接面向业务。

技能，更直接地表明了数据的正确性。因此并没有严格遵守三范式，存在数据冗余，数据量越来越大，但是数据展现要获得成功，就必须建立在简单性的基础之上，时刻考虑如何能够提供简单性，以业务为驱动，以用户理解性和查询性能为目标。

注：每个建模人员考虑的因素都不一样,依据具体的业务场景而定,但把上述内容答出来,足以应对。

# 11. 什么是代理主键和业务主键？

代理主键是纯数字为了记录数据的变量情况，而业务主键是基于业务系统，可能是数字也有可能是字符，主要是为了确保唯一的记录。

比如 订单编号 对应不同的状态，那么 订单编号 就是业务主键 无论订单状态如何变化，订单编号都是不变的。

在数仓中为了记录订单每次变化的情况，我们可以通过代理主键（可以是自从生产的一个序列的序号纯数字）来记录每次状态的变化情况。

这一点跟业务系统的业务主键不一样，通过代理主键作为事实表的外键，对事实表的字段分类（维度，度量）可以做到比较清晰

# 12. 知道表结构吗？有设计过表吗？

（1）确定表名。表名要确保其唯一性，表的名称要与用途相符，简略、直观、见名知意。  
（2）确定字段名称。字段名长度小于64个字符；字段名可以包括字母、汉字、数字、空格和其他字符；字段名不可以包括句号(。）、感叹号(!)、方括号([])和重音符号(、)；字段名不可以以先导空格开头。  
（3）确定字段类型。设计数据类型，满足字段的不同需要。  
（4）确定字段属性。如字段大小、格式、默认值、必填字段、有效性规则、有效性文本和索引等。  
（5）确定表中唯一能识别记录的主关键字段（主键）

# 13. 三范式有哪些，三范式与反三范式的优缺点

答：范式有第一到第六范式。常用第一到第三范式。（自己记三范式的概念），三范式优点：数据结构清洗，数据无冗余。缺点：关联效率低。查询慢（都是相对的）。反三范式，数据冗余，关联效果高。查询快。

三范式就是设计数据库的规则，就是为了建立的冗余较小、结构合理的数据库。

设计数据库时必须遵循一定的规则，这些规则就称为范式

满足最低要求的范式是第一范式（1NF）：原子性、做到列不可拆分

在第一范式的基础上进一步满足更多的规范要求称为第二范式(2NF)：一张表只能描述一件事

在此基础上满足更多的规范要求称为第三范式（3NF）：消除传递依赖

意思删除只要其中2列相关连可以得到的信息都需要删除

如：单价*数量可以得到总数额，那么总数额这一列信息就需要删除

一反三范式

反范式值得是通过增加冗余或重复的数据来提高数据库的读性能，但是浪费存储空间，节省了查询时间（以空间换时间）

# 14. 项目中表的构建都是符合三范式的吗？

答：（有思考）不一定，有一些会为了加快表间关联的速度，做一些冗余数据，但是大部分都还是按照三范式的。

# 15. 表结构设计和字段类型你们是根据什么因素设计的？

答：表结构设计一般参考三范式，也会考虑以下因素：

业务需求：了解业务需求是设计数据库表的基础。明确业务实体、关系和操作，以及数据的结构和约束。

数据模型：根据业务需求创建逻辑数据模型，包括实体、映射关系和属性。使用实体关系图（ER）（如图）来可视化和理解数据模型。

表结构设计：基于数据模型设计数据库表结构，包括表的名称、字段、数据类型、约束（主键、外键、唯一性等）以及默认值等

数据完整性：定义适当的主键、外键和约束条件，以确保数据的完整性和一致性。

查询需求：根据预期的查询需求，选择适当的索引、分区等技术来提高查询性能。

数据量和存储需求：根据预计的数据量和存储需求，选择合适的数据类型和存储方案，以确保效率和可扩展性。

数据安全性：考虑数据的安全性需求，如敏感数据的加密、权限管理等。

具体的设计决策还会受到项目需求、技术造型和团队经验等因素综合考虑。

# 16. 为什么使用星型模型建模？

注：问星型模型的优缺点

答：我的理解，星型模型，数据虽然有一些冗余，但是不多，可以减少表的关联，加快查询的速度，是一个用空间换时间的操作，后期也方便维护。

# 17. 物理模型的设计需要遵循哪些规则，简单说下这些规则

注：模型+范式

答：我觉得应该要遵循制定好的建模模型，比如是星型模型还是雪花模型，然后装结构满足三范式即可。（简单介绍星型模型和三范式的概念）

# 18. 什么是事实表什么是维度表，它们的区别是什么，举例说明。

答：事实表就是你要关注的内容，用于记录一个发生事件，它是用来存储主题的主干内容的，事实表一般是没有主键的，数据的质量完全由业务系统来把握。比如申请表，审批表，消费记录表，还款记录表，利润表，收入表；维度表就是看待事实事件的一个角度，维度表一般是有

主键的,比如地区表,部门表,产品表,客户表。 但是从另一个方面来说,客户表也可以作为一个事实表,因为新增一条记录,就代表有一个新 客户,可以从地区,职业,年龄等维度进行一个分析。 (事实表如客户信息表：客户编号、姓名、性别、年龄、证件号码、联系电话、教育程度、婚 烟状况、户籍省/市/区

合同信息表：合同编号、客户编号、放款金额、期限、放款时间、合同到期日、合同状态、贷款利率、提前还款违约利率、逾期利率

逾期信息表：首次逾期日期、逾期次数、累计逾期天数、贷款用途、还款方式、约定还款日、产品名称、产品编号）

# 19. 维度与各粒度之间应该是什么关系？

答：粒度是维度的一个种类，比如日期维度，它的粒度就有年、季度、月、周、天、小时等等。

# 20. 主题维度指标标签的理解

答：主题是一个类别，有两种方式，第一按照业务不同划分主题，比如个人存款主题，对公存款主题，第二按照事件的参与结构划分主题，比如当事人，协议，事件，地区；维度是看待事件的角度，比如日期，地区，客户，产品等等，指标是一些在事件中的一些有价值的数值类数据，也包括一些统计值，比如金额类的，逾期天数，逾期率；标签，我个人理解，就和衣服的吊牌，商品的标签差不多，给某些事件中的构成部分加一个特定的词汇，比如之前疫情，有些地区就会有一个高风险地区的标签，对用的就还有比如优质客户。

# 变化维、退化维、一致性维度、维度退化

# 21. 什么是退化维(Degenerate Dimensions)

答：退化维的定义一般来说事实表中的外键都对应一个维表，维的信息主要存放在维表中；但是退化维仅仅是事实表中的一列，这个维的相关信息都在这一列中，没有维表与之相关联。比如：发票号，序列号等等。

# 22. 那么退化维有什么作用呢？

a、退化维具有普通维的各种操作，比如：上卷，切片，切块等。  
b、如果存在退化维，那么在ETL的过程将会变得容易。  
c、它可以让 group by 等操作变得更快。

维度退化：将维度退化到事实表中，减少事实表和维度表的关联

# 23. 缓慢变化维

它的提出是因为在现实世界中，维度的属性并不是静态的，它会随着时间的流失发生缓慢的变化。这种随时间发生变化的维度，一般被称为缓慢变化维；并且把处理维度表的历史变化信息的问题称为处理缓慢变化维的问题，有时也简称为处理SCD的问题。

# 处理缓慢变化维方法通常分为3种：

a、直接覆盖原值  
b、添加维度行-拉链表  
c、添加属性列-变化列

# 24. 一致性维度

在多维体系结构中，没有物理上的数据仓库，由物理上的数据集市组合成逻辑上的数据仓库。而且数据集市的建立是可以逐步完成的，最终组合在一起，成为一个数据仓库。如果分步建立数据集市的过程出现了问题，数据集市就会变成孤立的集市，不能组合成数据仓库，而一致性维度的提出正是为了解决这个问题。一致性维度的范围是总线架构中的维度，即可能会在多个数据集市中都存在的维度，这个范围的选取需要架构师来决定。一致性维度的内容和普通维度并没有本质上区别，都是经过数据清洗和整合后的结果。一致性维度建立的地点是多维体系结构的后台（Back Room），即数据准备区。

在多维体系结构的数据仓库项目组内需要有专门的维度设计师，他的职责就是建立维度和维护维度的一致性。在后台建立好的维度同步复制到各个数据集市。这样所有数据集市的这部分维度都是完全相同的。建立新的数据集市时，需要在后台进行一致性维度处理，根据情况来决定是否新增和修改一致性维度，然后同步复制到各个数据集市。这是不同数据集市维度保持一致的要点。

在同一个集市内，一致性维度的意思是两个维度如果有关系，要么就是完全一样的，要么就是一个维度在数学意义上是另一个维度的子集。例如，如果建立月维度话，月维度的各种描述必须与日期维度中的完全一致，最常用的做法就是在日期维度上建立视图生成月维度。这样月维度就可以是日期维度的子集，在后续钻取等操作时可以保持一致。如果维度表中的数据量较大，出于效率的考虑，应该建立物化视图或者实际的物理表。这样，维度保持一致后，事实就可以保存在各个数据集市中。虽然在物理上是独立的，但在逻辑上由一致性维度使所有的数据集市是联系在一起，随时可以进行交叉探察等操作，也就组成了数据仓库。

# 25. 如何优化表的结构

答：尽量满足三范式，字段要求不能折，每个字段都和主键有关，并且一个表只记录一件事，比如，一个表记录了学生号，学生姓名，课程编号，分数。那这个表就记录了两件事，学生信息和课程分数。

# 26. 什么是数据血缘？

20. 什么是数据二元？  
    跟数据血缘相关的概念还有数据地图数据地图等等，其实描述的都是同一个意思，所谓数据血缘一句话总结就是各种数据之间的关联关系；这里面的关联关系呢，不仅仅指表与表之间的数据转换关系，同时还包括数据从数据源开始经过一系列的计算，中间存储最终生成业务结果，过程中要经历的调度方式，计算方式和存储方式等等，数据血缘说到底就是在数据处理过程中对与之相关的各种关系梳理；

那数据血缘到底有什么作用呢？

那数据血缘到底有什么不同？

# 数仓分层

# 为什么要对数据仓库分层？

用空间换时间，通过大量的预处理来提升应用系统的用户体验（效率），因此数据仓库会存在大量冗余的数据。如果不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗过程，工作量巨大。

通过数据分层管理可以简化数据清洗的过程，因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，把一个大的黑盒变成了一个白盒，每一层的处理逻辑都相对简单和容易理解，这样我们比较容易保证每一个步骤的正确性，当数据发生错误的时候，往往我们只需要局部调整某个步骤即可。

# 注：看你简历中是否有些大数据项目，传统数仓分层是ods-dw-dm,大数据的是ods-dwd-dws-ads，区分是否讲的当前项目。

答：我这个项目是 oracle 数据库，分成主要是这样的，首先是 ods 层，也叫贴源层，主要是对业务系统的数据做一个备份，不做任何的处理，除此之外会更改表名加上数据库层名，方便数据血缘的追踪。然后是 dw 层，主要两个功能，第一对 ods 层的数据做清洗（主要清洗的内容：空值（是否取默认值），异常值（具体看业务需求，删除整条记录还是按什么规则修改），日期类型的统一，枚举值的统一，也就是码值的统一（比如证件类型，产品类型，地区编码）。第二个就是对过滤的数据进行一个简单的汇总，做一些宽表出来，为下一层提供数据聚合的依据和支持。然后是 dm 层，主要是根据 dw 层的一些宽表计算相应的一些指标或者同比环比数据，最后是报表层，使用帆软报表将 dm 层的数据做一个可视化的展示）

答：我这个项目是hive的，分成是这样的，首先是ods层，也叫贴源层，主要是对业务系统的数据做一个备份，不做任何的处理，然后就是dwd层有效数据层，对ods层的数据做一个清洗（主要清洗内容：空值（是否取默认值），异常值（具体看业务需求，删除整条记录还是按什么规则修改），日期类型的统一，枚举值的统一，也就是码值的统一（比如证件类型，产品类型，地区编码），然后dws层轻度汇总层（中间层），就是对过滤的数据进行一个简单的汇总，做一些宽表出来，未下一层提供数据聚合的依据和支持。然后是ads层结果层，主要是根据dws层的一些宽表计算相应的一些指标或者同比环比，然后将这些数据导入到mysql数据库中，最后是报表层，使用帆软报表将mysql的数据做一个可视化的展示）

# ODS层

从不同系统过来的数据是怎么处理呢, 就是 a 系统过来的品牌 id 是 a, b 系统过来的品牌 id 是 b, c 系统过来的品牌 id 是 c, 这些品牌都是一个品牌, 这些你要怎么处理?

答：ods 层我们不做处理，在 dw 层会将这些品牌重新进行码值编排，做到统一。

讲讲你在ods层都做什么？怎么做？

答：ods 层主要是对业务系统进行一个抽数，我们这边就是不做任何处理，只对表名做修改。将业务系统的表结构，转换成 oracle 的数据类型和对应的长度，然后表名使用 ods_系统简称源表名，这样看见表名就知道是哪一个业务系统的数据，因为 oracle 的表名长度最长是 30，

有一些会超长，我们一般都是尽量不改变源表名的意思去缩短，并且会将这些对应关系写进excel 进行一个记录，以及数据抽取的频率（每天/每个月）和方式（增量/全量）

# DWD层

从ods层到dw层经过了一定的数据清洗后，怎么保证数据的质量，例如A表的A字段跟B表的B字段应该是一模一样的，另外一种场景A表A字段跟B表的B字段进行一个运算，再到B表的A字段，怎么保证中间的数据质量问题（怎么保证数据不会出错）

注：此类问题是企业的通病，一般通过一些工作的流程，去保证数据在开发之前的准确性。开发之前，需求串讲，反串讲，设计的评审，开发过程中，细心，开发完成后，代码评审和测试。答：这种问题，我觉得我之前的项目组做的是比较好的，所以我们的代码质量是比较高的，因为我们在一开始会有需求串讲，澄清需求的内容，之后呢，我们会有反串讲，就是我们开发来讲自己负责的需求内容，然后业务来听，确认我们是理解需求的，然后我们开发会做一个简单设计文档，就是针对每个需求的一个实现步骤，比如第一步做什么，第二步做什么，然后内部会简单的开会听一下其他人的需求实现步骤，做一个优化，其次就是在开发过程中，不断的完善之前的开发方案，做到最好。开发完成后也会进行一个测试和互测，来保证数据不会出错。

# ods层到dw层是怎么实现的，一般做哪些操作呢？

答：dw 层，主要两个功能，第一对 ods 层的数据做一个清洗（主要清洗内容：空值（是否取默认值），异常值（具体看业务需求，删除整条记录还是按什么规则修改），日期类型的统一，枚举值的统一，也就是码值的统一（比如证件类型，产品类型，地区编码），第二个就是对过滤的数据进行一个简单的汇总，做一些宽表出来，这些宽表是一个字段比较多的事实表，尽可能的包含多个维度，每个维度以最小粒度，进行一个汇总，为下一层提供数据聚合的依据和支撑。

# DW层是做什么的,粒度维度如何划分,每一层都是做什么的怎么做?

答：dw 层主要是做清洗（内容参考上个题答案），简单的汇总到宽表中（参考上个题），粒度取各个维度的最小粒度，主题还是按照我们之前项目有的模板，就是一个金融的十大主题。

# 数据清洗主要做什么。如何进行数据清洗，清洗过哪些数据？

数据消元主要做什么。可选选项有：  
答：对ods层的数据做一个清洗（主要清洗内容：空值（是否取默认值），异常值（具体看业务需求，删除整条记录还是按什么规则修改），日期类型的统一，枚举值的统一，也就是码值的统一（比如证件类型，产品类型，地区编码）

# 数据清洗的原则，規則是什么？

在数据清洗过程中，有一些常见的原则和指导方针，可以帮助确保数据的质量和可用性。以下是一些常见的数据清洗原则：

(1)数据集中没有缺失值,尽量填充或处理缺失值,以保持数据的完整性。

完整性：确保数据集中没有缺失值，尽量填充或处理缺失值，以保持数据的完整性。一致性：确保数据在整个数据集中的一致性，比如相同类型的数据应该使用相同的单位和格式。

准确性：确保数据的准确性，通过验证和校对来自不同来源的数据，纠正错误和异常值。

唯一性：确保数据集中没有重复的记录或实体，去除重复数据以保持数据的唯一性。

规范化：将数据转换为统一的格式和单位，以便于比较和分析。

# 什么是宽表，如何来理解？

宽表呢，属于事实表中的一种，从形状上来看，它很宽，意思是它包含的字段数量比一般的的普通表要多很多的字段，那更多的字段就意味着包含着更多的数据信息，更多的数据信息就意味着能够提供更完整的业务价值，同时宽表的实现在效率上会比普通表来得更快，因为普通表在数据仓库建设时需要考虑到维度拆分，满足数据库的范式原则等等，对于同一个业务需求来说宽表直接就可以提供满足需求的所有维度，而普通表一般情况下则需要通过各种维度的join操作之后才能达到这个目的，而join操作，我们知道一定要触发shuffle的，而这个shuffle往往会成为效率的绊脚石，同时也会容易引发数据倾斜，大数据不怕数据量大，就怕数据倾斜，那么会导致你的整个数据分析流程变得缓慢，照这么说了，宽表这么好，是不是所有的数据库表都要设计成宽表，答案是否定的，宽表虽然有诸多的优势，但是它的缺点呢也一样非常的明显，首先就是宽表因为把所有跟某个事件相关的维度都揉在了一起，这样就会导致其中一些变化缓慢的维度信息出现大量的数据重复，从而占据过多的存储空间，此外呢，因为一张宽表很多时候只是面向某一个业务需要就把相关的指标都揉到一张表里面，而如果需求稍微一变化，可能就得重新设计一张新的宽表，这样就导致单张表的灵活性很低，使用场景很杂，如果类似的需求很多的话，那么系统就会出现大量这种复用度很低的宽表给人感觉就是整个数仓的建设缺少了设计的而且常使以往下去，系统的开发效率也会变低，因为没有中间层的概念，所以也就没有模块复用这一说，宽表是一把双刃剑，它在带来数据处理效率提升的同时呢，也破坏了数据库建模的很多规范。

# DWS层

# DWS层宽表是怎么做的？

注：最好能准备一个宽表出来，说一些中间的维度，都是什么粒度的。

答：这些宽表是一个字段比较多的事实表，尽可能的包含多个维度，各个维度以最小粒度，进行一个汇总，为下一层提供数据集合的依据和支持。

# ODS层和DW层的数据有什么区别？

注：一定要了解ods层和dw层的基本概念，该层的操作。

答：首先ods层是作为一个业务系统的备份层，只是修改了表名，以ods_系统简称_源表名，其他的数据没有变化，自短板会修改数据类型和长度，DW层是一个数据仓库层，也是一个中间层，对数据进行一个清洗，保证数据是准确有效的，不会有一些无效的数据，另外会做一些简单的汇总，比如交易表，就会有日交易额表，那么对于下一层计算月交易额，季度交易额，都会加快速度，而不是都从一个很大的表中查询数据。

数仓项目是从0到搭建的吗？

答：是的，这个项目我是全程参与的，从需求的分析，串讲，反串讲，到最终的测试，上线。

# dm层如何保证数据质量，如何保证数据是正确的？

首先在开发阶段，我会仔细分析并理解需求，按照指标计算的公式完成进行开发。在这个阶段，如果我认为需求有问题或者我对需求有疑问会跟我们的产品经理进行沟通和确认；开发以后，先进行单元测试，单元测试的时候我会先按照计算公式进行手工计算，然后通过代码再算一遍，对比两次计算结果，如果计算结果一致说明没有问题。；之后，我们还有一个代码评审环节，可以进一步确认我们对需求的理解是否正确；进入测试阶段以后我们团的测试会进行充分的测试，对于他们发现的问题我会分析问题并解决，然后对发现的有效缺陷进行分析，看看是否在其他地方也有类似的问题，避免测试漏测，也可以提高我的代码质量。上线以后，对于我们团队的bug，我会进行分析，总结经验，避免以后再次发生类似的bug。

# ETL工具类问题

什么是 etl 英文名称是 extract—transform—load 的缩写, 用来描述将数据从来源端经过抽取,转换,加载至目标端的过程。

# 1. 常用到的一些工具

答：ETL类的，kettle、sqoop、flume；代码工具plsql、notepad++、Hue、DBeaver；报表工具finereport、永洪报表、powerBl、tableau等等。

# 2. 用什么工具抽取

答：抽取工具用过kettle、sqoop，flume

# 3. kettle 是一个开源的抽象工具，纯 Java 写的。

我们一般都是用来抽取源系统数据到hive或者Oracle。

我们一般都是用来抽取素材的。  
他有转换和作业两种文件格式，转换是ktr结尾的xml格式文件，作业是kjb结尾的xml格式  
他有转换和作业两种文件格式，转换是ktr结尾的xml格式文件，作业是kjb结尾的xml格式  
他有转换和作业两种文件格式，转换是ktr结尾的xml格式文件，作业是kjb结尾的xml格式  
他有转换和作业两种文件格式，转换是ktr结尾的xml格式文件，作业是kjb结尾的xml格式  
他有转换和作业两种文件格式，转换是ktr结尾的新文件，作业是kjb结尾的新文件  
他有转换和作业两种文件格式，转换是ktr结尾的新文件，作业是kjb结尾的新文件  
他有转换和作业两种文件格式，转换是ktr结尾的新文件，作业是kjb结尾的新文件  
他有转换和作业两种文件格式，转换是ktr结尾的新文件，作业是kjb结尾的新文件  
他有转换和作业两种文件格式，转换是ktr结尾的新文件，作业是kjb结尾的新  
件主要是对转换文件的依赖配置和调度配置一般都是使用转换和邮件，失败了会发邮件，然后  
做一个定时调度

# 4. kettle 的转换或者作业文件都保存在哪里？

kettle 我们会配置一个资源库，连接资源库即可，都保存在资源库中

（相当于把转换和作业文件保存在一个公共的数据库中，所有人都能访问）

# 5. kettle工具的运用

答：kettle 主要是用到了转换和作业两个不同的功能，使用转换进行 ETL 数据的处理，主要是处理表格或者数据等结构化的数据，或者接口这样的半结构化数据等，然后对数据进行格式的统一，和数据的有效筛选或者是数据的计算等等。然后将数据增量或者全量的保存到数据表格中。使用作业主要是进行脚本的定时任务的操作等，作业我们是在 windows 上进行编辑，然后上传到 kettle 的 linux 服务器，使用 crontab 结合 kettle 的指令进行数据的定时调度。

# 6. kettle转换常用组件

答：计算器，剪切字符串，字段选择，排序记录，去除重复记录，过滤记录，关联，列转行，合并记录、分组。

计算器：利用一系列的函数组合，来创建新的字段，并且可设置新的字段是否移除。

剪切字符串：剪切字符串可以对字段中字符串践行剪切，索引位置从0开始，例如：截取至索引位置6，也就是0345。

字段选择：字段选择有3个标签页，[选择和修改]，{移除}，{元数据}

选择和修改：可以实现对字段的改名。移除字段：设置字段是否要移除。元数据：可以修改字段类型。

注意：移除字段一定要在选择和修改页面包含，否则会报找不到字段的错误。

排序记录：按照指定字段对记录进行升序后者降序排列，可以对多个字段进行组合排序。

去除重复记录：去除重复记录之前需要排序。

过滤记录：对记录进行过滤，过滤的结果可以进行两个输出，符合条件的一个输出，不符合条件的另一个输出，可以添加多个过滤条件，并且可以选择条件之间的逻辑关系。

关联：从多个数据源中取值并进行关联。关联之前需要进行排序。

列转行：列转行之前也需要进行排序。

合并记录：合并记录可以对旧记录与新记录进行合并，并可对数据的新增，修改和删除进行标记。主要用于新旧数据的对比。

分组：分组就是按照一定的分类对数据进行分类统计,如按照班级对成绩进行统计,按照性别进行统计,班级和性别就是分组的依据。

# 7. Kettle性能调优

答：（1）调整JVM大小进行性能优化，修改kettle根目录下的Spoon脚本。  
（2）调整提交（Commit）记录数大小进行优化，kettle默认Commit数量为：000，可以根据数据量大小来设置Commitsize：000-5000  
（3）尽量使用数据库连接池

(4) 尽量提高批处理的 Commit size;  
(5) 尽量使用缓存, 缓存尽量大一些 (主要是文本文件和数据流)  
(6) kettle 是 JAVA 做的，尽量用大一点的内存参数启动 kettle;  
（7）可以使用sql来做的一些操作尽量用sql；Group。merge，stream lookup，split fields，这些操作都是比较慢的。想办法避免他们，能用sql就用sql。  
(8) 插入大量数据的时候尽量把索引删掉  
(9) 尽量避免使用update。delete操作，尤其是update，如果可以把update变成先delete，然后insert。  
（10）能使用truncate table的时候，就不要使用delete row 这种类似sql合理的分区，如果删除操作是基于某一个分区的，就不要使用delete row这种方式（不管是deletesql还是delete步骤），直接把分区drop掉，在重新创建。  
（11）尽量缩小输入的数据集的大小（增量更新也是为了这个目的）。  
（12）尽量使用数据库原生的方式装载文本文件（oracle的sqlloader。mysql的bulk loader和步骤）

# 8. sqoop 和 kettle 的区别

答：sqoop 是一款开源的工具，主要用于在 hadoop 与传统的数据库间进行数据的传递，kettle 是一款国外开源的 etl 工具，纯 java 编写，可以在 window。linux，unix 上运行，数据抽取高效稳定。

# 9. sqoop 和 kettle 取数性能对比

答：因为 sqoop 底层调用的是 mapreduce，所以小数据量性能会受到限制。千万级别及以下，kettle 有优势，亿级别的数据 sqoop 优势明显。

# 10. sqoop 抽取异常常见问题举例

答：指定了\n为squoop导入的换行符；mysql 的某个string字段的值如果包含了\n，则会导致squoop导入多出一行记录，squoop导入时删除string类型字段的特殊字符-hive-drop-import-delims Drops\n,\r,and \0 from string fields when importing tohive

答：sqoop 导入数据时间日期类型错误：hive 只支持 timestamp 类型。而 mysql 中的日期类型是 datetime。当 datetime 的值为 0000-00-00 00:00:00 的时候，sqoop import 成功，但是在 hive 中执行 select 语句查询该字段的时候报错，解决方法是在创建 hive 表时用 string 字段类型。

类型。  
答：sqoop 从 mysql 导入 hive 的字段名称问题：hive 中有些关键字限制，因此有些字段名称在 mysql 中可用，但是到了 hive 就不行。比如 order 必须改成 order_，下面板列出了我们发现的一些不能在借中使用的字段名称：order  $=$  order_，sort  $\coloneqq$  sort_，reduce  $\coloneqq$  reduce_，cast  $\coloneqq$  cast_，directory  $\coloneqq$  directory_

答：Sqoop 从 mysql 导入 hive 时 null 值不一致：Hive 中的 Null 在底层是以[\N]来存储，而 MySQL 中的 Null 在底层就是 Null，所以在导入的时候利用—null-string 和—null-non-string 来数据都转化成指定的类型，一般指定的话是按 hive 上的 null 值来指定，也就是 \N

11. 在项目上, 从银行那边的源系统用 sqoop 抽数到 hive, 在抽数结束后, 去对数据进行核验, 发现 hive 中的数据总比数据库那边多了几千行

答：原因是特殊字符引起，解决方式：sqoop添加语句--hive-drop-import-delims #将mysql中字符串中的特殊字符删除

12. Sqoop 数据导出一致性问题

注意：这个是导出一致性，不是导入！！！ 数仓是面向决策使用的，数据可以没有，但是不能出现错误 由于 sqoop 在写 sqoop 语言的时候，需要配置 num-mappers 数量，如果导入数据的时候，maptask 执行失败，那 map 任务会转到另一个节点里重新执行，可能会导致数据库里的数据重复。通过—staging-table 使用—staging-table 可以生成一个暂存表，就是先导出数据到暂存表，然后再把这个暂存表的数据移动到目标表 但是你生成暂存表时，必须需要保证暂存表是空的，所以就要—clear-staging-table 来清空暂存表。

staging-table表名

clear-staging-table

13. sqoop导入数据发生数据倾斜

可能是 map 任务分配不均，增加 map 任务的数量

--num-mappers 8 # 默认为 4，增加 map 任务数；这个数量不是越多越好

14. ads层是orfcfile等格式导出到mysql能成功吗？

不能，需要先导到 textfile 格式的表，再将这个表数据导出到 mysql

15. etl 如何保证数据准确性

统计源表和目标的数据量，看是否一致；抽样，手动计算结果和sql运算结果是否一致两张表的关键字段做minus计算；如果是hive中，因为hive没有主键约束，可以通过group by分组having>=2过滤，看是否有重复数据。

16. 抽数到ods的时候怎么保证与原始的数据一致，数据的完整性和一致性，怎么保证数据没问题？

答：第一个就是抽取完成后，将数据量与原系统作比较，看是否一致，第二个就是将抽取过来的数据进行随机抽样，与原系统做比较，这个范围可以稍微大一点。

17. 如果上亿数据抽数的时候出现问题怎么解决？

答：一般我们这种大的表，都是采用增量抽取的，日增量在百万级别，抽数的效率还是比价高的。

的，重新抽一次基本就可以了。

18. 什么是映射，什么是etl？

答：在 etl 过程中，原表和目标表的对应关系就是一种映射关系，etl，英文名称是 extract-transform-load 的缩写，用来描述将数据从来源端经过抽取，转换，加载至目标端的过程。

19. kettle 抽数一般遇到什么问题, 你们怎么解决?

答：kettle 在不同的数据库抽取数据时，有时会出现中文乱码问题，解决方法：如果数据设置正确仍然中文乱码，则可能是因为有的客户端数据库默认的编码集不是 utf-8，我们只需要在输出时对数据库进行编码设置，如果不行，可以打开 options，添加参数 characterencoding，设置值为 gbk/utf

20. 如果抽取的数据有重复，怎么解决？

答：在 kettle 的核心组件里, 有一个字段选择, 里面有一个去除重复记录(uniq rows (hashset))的控件, 在抽数的时候, 可以通过这个控件对数据进行去重。具体操作: 创建几个河西对象,分别是输入->表输入, 将数据根据字段排序, 数据去重, 将去重好的数据输出到另一个表中,这里需要注意, 数据去重前必须要根据相关字段排序。

21. etl 脚本开发以后，怎么运维？

答：我们会定期巡查，一般周做一次检查，检查 etl 脚本跑数是否正常，是否报错，分析脚本跑数时间，是否越跑越慢，在这种情况下，需要分析脚本和目标表，比如目标表是否后来建了索引，导致更新表的时候越来越慢，索引会影响更新的效率。如实是这种情况，可以先删除目标表的索引，更新完了以后在创建新的索引，这样就可以提高跑数效率。

22. 一张特别大的表, 几千万, 几亿的表, 怎么通过 etl 工具同步

答：在 etl 工具中，我们插入数据的时候，可以设置批量提交，比如 10 万记录提交一次，而不是一次性提交。另外如果特别慢的话，还要考虑是否锁表了。比如对目标表进行更新的时候，转换的状态一直不变，可能是目标表产生了排它锁，导致无法更新，这个要具体分析一下。

23. 数据同步以后, 源系统的表结构发生了改变, 比如源系统的表增加了字段, 你的 kettle 脚本是否会报错?

脚本是否会报错？答：不会。因为我们抽数的时候，对原表和目标表的每个字段都做过映射，所以源系统的表增

24. 有一张表下午 5 点才出数据, 一般的表早上十点就已经跑完了, 是什么原因导致表这么慢出数据 (从数仓同步数据, 本来在上 9 点就要出结果, 结果一直到下午 4 点才出结果)

答：首先我会看一下调度日志，再次检查一遍是否所有的前置 job 都跑完了，排除尹某一个前置 job 没跑完，导致目标表的 job 一直在等待。排除上面的问题后，在分析是否是性能问题，检查监控日志，看下跑数时间，如果确实是目标表更新慢，在进一步分析慢的原因。比如：检查是否是在目标表上建了过多的索引导致更细慢，另外，我们平时也要定期（每周或者没周对之前的跑数 job 进行检查）检查 job 是不是越来越慢，可能因为数据量过大导致的，数据量过大的原因可以通过创建索引或者通过分区表的方式进行优化，在分析问题的过程中我们要关注 sql 的执行计划，必要的时候可以使用优化器进行 sql 优化。

# 25. 有一个 job 前一天执行成功了，第二天再执行失败了。请分析失败的原因

答：一种是报错，一种是不报错，但是没有结果，报错，可以查看日志，分析错误原因。如果报主键冲突。说明更新的sql逻辑有问题，我们可以查下代码，更改原来的同步逻辑即可。不报错，看调度任务是否执行，日志中如果有开始，但是没有完成时间，说明一直在等待，这时候需要查询一下是否锁表，我会查下锁表的过程，然后杀死该进程。

# 26. ETL 测试主要包括什么？

答：：数据量测试。这个应该不用多说，从源头拿来多少，存到目标表里又是多少。是否一直，是否正确。：数据转换测试，主要是对数据格式的合法性进行验证，测试参考点主要包含：1)时间、数值、字符等数据的处理，是否符合数据仓库规则，是否进行统一的转换，是否有超出维表或者业务值域的范围。（2）是否捕捉字段空值，或者需要对空值进行替换为其他含义值的处理。（3）主键是否唯一（4）特殊符号或者乱码的处理规则（5）脏数据的处理。3抽样测试，在转换完成后要对转换之后的关键字段验证，还有原表和目标表的映射是否正确。4：etl加在策略测试。包含：全量加在（先清空在插入）和增量加载（目标表仅更新原表变化的数据）

# 27. 数据抽取是有专人负责么？

答：是的，有专门同事负责，但我也会一些。

# 28. 有没有做过前端的展示，报表开发可视化这部分工作是否考虑

答：有做过一些finereport的行市报表和交叉报表，制作一些简单的图样这样子。考虑的，个人对于这块也是觉得是有发展的，比较喜欢做这个，但是一直没什么机会。

# 29. 在源数据对接过程中会遇到那些问题？

大数据对接银行的数据库，一般是指与银行业务相关的数据库。常见的有 Oracle、DB2、SQL Server 等关系型数据库，以及 Hadoop 生态中的 HBase、Hive、HDFS 等非关系型数据库。在源数据对接过程中，可能会遇到以下问题：

1. 数据量大，传输效率低下

银行业务数据量通常很大，传统方式下使用FTP或HTTP方式进行数据传输速度较慢，容易导致传输失败。可以采用分布式文件系统（如HDFS）来存储和传输数据，提高传输效率。

银行业务数据通常来源于不同的子系统或厂商，数据格式差异较大，需要通过数据预处理和清

洗来统一格式。可以使用 ETL 工具进行数据转换和清洗，也可以使用编程语言如 Python 等进行数据处理。

# 3. 数据安全性

银行业务数据涉及敏感信息，需要进行数据加密和权限控制。可以使用SSL/TLS等安全协议进行数据传输加密，也可以使用Kerberos等认证机制进行权限控制。

# 4.性能问题

大数据平台处理大数据量时，需要考虑性能问题。可以通过优化SQL查询、调整数据分区和索引等方式进行性能优化。

# 5. 故障处理

传输过程中可能会出现网络故障或其他问题，需要进行故障处理。可以使用监控工具监控数据传输情况，及时发现并进行处理。可以使用日志进行故障排查。

综上所述，大数据对接银行的源数据在对接过程中可能会遇到多种问题，需要通过优化技术、安全机制和故障处理等手段进行解决。

# 项目描述类

# 1. 项目周期

注：项目的周期。面试官主要通过项目周期来判断项目大小（表的数量、数据级别）。报送类的一般都是6个月以内，后续每个月在报送前会留一个人在现场进行异常数据的处理。其他的，区分客户方的大小（业务、数据量、项目技术），人员的编排，一般都在6个月左右。答：有思考（停顿），这个项目大概是6个月，从xx年x月到今年x月。

# 2. 项目团队组成，这个项目有多少个人。分别的角色

2. 项目团队组成：这个项目组由5名成员组成，其中至少有1人是公司的员工。  
   注：面试官主要通过项目团队组成，区分项目的大小。区分你的公司是不是第三方数据服务公司，是（一般都不超过5个人），否（十几个人）

答: 我们开发组包括项目经理在内一共是 5 个人, 我和另外个人是负责坐数据开发, 还有一个同事是在做前端报表开发, 当然后期还会有其他的一些运维, 测试人员进场, 但是我们开发只会留一个在现场。

# 3. 项目背景是怎么样的？

注：客户方，项目介绍，客户为什么做系统，做这个系统有什么用答：这个项目是在xx银行的一个xx项目，主要针对xx块数据进行一个处理和分析，（之前客答：这个项目是在xx银行的一个xx项目，主要针对xx块数据进行一个处理和分析，（之前客

# 4. 指标数据在原系统哪个地方？

4. 指标数据在原系统哪个地方？答：针对信贷项目：客户信息，一般都来自于核心系统和信贷系统，信贷业务的管理数据都在信贷系统，包括申请，审批，授信，协议，逾期，但是放款和还款在核心系统。我自己也做了

这么久的银行业务系统，发现就是关于客户和账务处理的都在核心系统，关于授信业务都在信贷系统，其他的各个系统都负责自己的业务流程管理，比如申请，审批，后期的维护。

# 5. 你们指标是怎么分类的？

在数仓中，指标通常是按照不同的类别进行分类和管理，以方便数据分析师和业务用户进行数据查询和分析。以下是常见的指标分类：

事实指标：基于事实表的数据测量，通常表示业务处理中的数量、金额、百分比等，如销售金额、库存数量、订单数等。

维度指标：基于维度表的数据测量，通常表示维度数据的聚合值，如客户数量、产品种类数等。计算指标：通过运用计算公式或表达式得到的指标，通常表示数据的比率、百分比或增长率等，如毛利率、销售增长率等。

智能指标：通过机器学习、数据挖掘等技术自动生成的指标，通常用于分析复杂的业务场景和模式，如推荐引擎中的“猜你喜欢”功能。

衍生指标：通过对其他指标进行计算或转换得到的指标，通常用于数据分析和可视化，如对销售额进行年环比计算得到的“销售增长率”。

关键绩效指标：用于跟踪业务目标和关键绩效指标的指标，通常用于评估业务绩效和制定业务决策，如单个用户的消费金额、用户留存率等。

# 6. 用了哪些技术，平时用到的技术是 oracle, db, 还是大数据平台？

注：了解项目的技术

答：传统数仓：当前用的oracle数据库，用存储过程来做数据处理，用kettle抽数，用帆软做报表展示。大数据平台：当前项目用sqoop进行从业务系统到hdfs的数据抽取，用的hadoop中的hdfs做存储，用hive.sql来做数据处理，然后将数据导入mysql，用帆软做报表展示。

# 7. 你们项目经理是具体负责做什么？

答：业务逻辑的梳理（制作maopping文档），任务安排，人员编制，考勤，对接客户方的技术人员，疑难问题的解决。

# 8. 你们在项目中可以自己负责一整条线的开发任务吗？

注：面试官想要了解：你是不是只会做整个流程中的某一个阶段，比如只会做dw层，或者只会做报表。

答题思路：我的上一个项目是怎么分工的，如果是自己负责某一整条线的（我负责 xx 内容的数据处理，从 ods 层到 dw 层的数据清洗和转换加工，最后到 dm 层的数据指标的计算）。如果上一个项目不是这样的（我上一个项目是 xx 样子的，我负责某一层的数据处理。但是我又之前做过 XX 项目是这样的，我负责 xx 内容的数据处理，从 ods 层到 dw 层的数据清洗和转换加工，最后到 dm 层的数据指标计算）。

# 9. 系统给谁做的，小额贷款项目是哪家金融机构的，汽车金融项目是哪家金融机构的，风控管理系统是给谁做的？

注：这个问题是微众银行的二面问的。一定要在简历的项目名称中写出客户方的名字，自己要注意驻场的地址（熟悉当地的区域），在哪里，具体某座大厦几楼。大致了解附近的建筑。

# 10. 风控系统在整个业界都是比较成熟的，为什么会有契机会给你去做？

注：会有一些面试官经验很多，好奇你的项目，觉得你的项目过时了，回答大方一点（之前客户那边有一个老式的系统，但是原开发商倒闭了，没有进行维护了，所以我们等于是重新做了一个）。

# 11. 你们银行项目的主题分类

注：告诉他你知道的银行大部分项目主题分类。区分当前聊天的场景，是在聊所有项目，还是在聊你的某一个项目。

答：我们所有银行的项目，都是按照金融十大主题去建模的，分成当事人，资产，结构，产品，协议，事件，渠道，营销，财务，地区。但是一般的项目都会做一些个性化调整，具体要看客户方的一个项目需求和数据的分布情况。比如我做的xx项目，它的主题就分有xxx。

# 12. 项目中用到哪些维度表

注：和主题不太能区分开，相对的。

答：简单的维度表，比如说地区，时间，机构，产品，利率，汇率这种，一般的，比如产品类型表，产品补充信息表，部门表，复杂一点的维度表也有，比如产品信息缓慢变化维表，就是一种拉链表。

# 大类业务问题

1. 描述一下你遇到过最复杂的业务场景，描述一下比较复杂的指标，以及你是怎么解决的？注：一定要提前准备-3个复杂的业务场景，可以参考牛客网的困难类型题目，也可以准备一些复杂的计算公式（贷款损失准备金文档中有很多指标计算公式），一定要找到解决的办法，如果找到了，但是不算公式（贷款损失准备金文档中有很多指标计算公式），一定要找到解决的办法，如果找到了，但是不会解决一定要问。建议准备复杂的业务场景，因为他可能在你说一般的时候打断你，回到正题。答：业务场景复杂的，慢慢弄梳理，拆解步骤基本都可以解决，有一些印象比较深刻的，最早的时候，答：业务场景复杂的，慢慢弄梳理，拆解步骤基本都可以解决，有一些印象比较深刻的，最早的时候，是开始做这一行，有些语法不太懂的时候，碰到有两个XX表，一个是新的，一个是旧的，但是有一些数据刚开始做这一行，有些语法不太懂的时候，碰到有两个XX表，一个是新的，一个是旧的，但是有一些数据开始做这一行，有些语法不太懂的时候，碰到有两个XX表，一个是新的，一个是旧的，但是有一些数据开始做这一行，有些语法不太懂的时候，碰到有两个XX表，一个是新的，一个是旧的，但是有一些数据开始做这一行，有些语法不太懂的时候，碰到两个XX表，一个是新的，一个是旧的，但是有一些数据开始做这一行，有些语法不太懂的时候，碰到两个XX表，一个是新的，一个是旧的，但是有一些数据开始做这一行，有些语法不太懂的时候，碰到两个XX表，一个是新的，一个是旧的，但是有一些数据开始做这一行，有些语法不太懂的时候，碰到两个XX表，一个是新的，一个是旧的，但是有一些数据开始做这一行，有些语法不太

# 2. 对银行的这块业务除了贷款业务还有什么了解的？

答：那我介绍下银行的业务分类吧，银行业务主要分资产类业务，负债类业务，中间业务，资产类主要是信贷和信用卡，负债类业务主要是存款，借款，同业存放，中间业务：票据，理财，保险，期货，基金这些，大致知道有这么多业务，具体的只有了解一些概念。

# 3. 银行票据了解多少？说一下你所知道的票据业务知识？

答：之前项目上没有遇到过，了解得不多，我对票据的了解是：以支付金钱为目的的有价证券，出票人根据票据法签发的，由自己无条件支付确定金额或委托他人无条件支付确定金额给收款人或持票人的有价证券，分为汇票，本票，支票，这是我对票据业务的一些理解

# 4. 怎么定义表内外科目？

注：科目和课程的科目差不多意思。

答：表内表外指的是业务是否在资产负债表内，在资产负债表以内的就叫表内业务，否则就叫表外业务，有些表外业务可以转为表内业务，具体的我不太清楚，能在表内的，我知道的主要是资产业务（信贷和信用卡）和负债业务（存款，借款和同业存放）

# 5. 谈谈对银行业务的了解

答：银行业务主要分资产类业务，负债类业务，中业务，资产类主要是信贷和信用卡，负债类业务主要是存款，借款，同业存放，中间业务：票据，理财，保险，期货，基金这些，大致知道有这么多业务，具体的只有了解一些概念。

# 6. 业务流程是怎么样的？

答：（1）信贷业务流程。客户申请-审批-授信-合同签订-放款-生成账单-还款-清算-逾期-催收-诉讼。  
（2）信用卡业务流程：客户申请-审批-授信-合同签订-发卡-消费-账单-还款（分期）-清算-逾期-催收-诉讼。

# 7. 对公业务，你熟悉不？主要从哪里取数的？

答：项目没遇到过，但和同事朋友的交流沟通了解下，对公业务数据是分得很细的，我记得对公客户信息是存放在核心业务系统的，对公账号开户流程是以下几步

1、法人到现场提交资料（身份证原件、营业执照、公司规章等资料）；  
2、跟客户经理预约；  
3、填好申请表格签名；  
4、银行上传资料到央行，允许之后会下发开户许可证，一般当天是可以拿到的。  
5、银行客户经理会通知开户人取回证件、U质等：  
6、开户人可以对账户的余额进行操作，大致就是这样的流程。

# 8. 押品业务概念

押品是缓释银行信用风险的重要工具，也叫担保物，押品种类是押品管理的基础和前提，为了

便于管理，会将押品进行分类。目前分为金融质押品、应收账款、商用房地产和居住用房地产、其他押品四大类，其中：金融质押品有现金及其等价物、资金属、债券、票据、股票/基金、保单、保本型理财产品等；

应收账款有交易类应收账款、应收租金、公路收费权、学校收费权等；

- 商用房地产和居住用房地产有商业房地产、居住用房地产、商用建设用地使用权利居住用建设用地使用权、房地产类在建工程等；

其他押品有流动资产、出口退税账户、机器设备、交通运输设备、资源、资产、设施类在建工程、知识产权、采矿权等

# 9. 风险敞口”一般是指银行在向贷款人放贷时的“信任度”，以金额来衡量：

一般来说，如果银行给贷款人的无担保流动资金贷款是500万，风险敞口是500万；如果银行给出借人一张500万的“银行承兑汇票”，保证金为  $30\%$  ，那么风险敞口为  $500 \times (1 - 30\%) = 350$  万；

# 10. “信用证敞口”是“风险敞口”的一种：

一般信用证敞口就是银行给国内进口商开信用证的风险敞口，比如要求开证人交  $20\%$  的保证金，银行开  $100\%$  的信用证，那么信用证敞口就是  $80\%$  。

# 项目场景问题分析

# 1. 有没有做过分析和报表？

注：分析报表先说简单的，后确认分析的职责，要确定这里的分析指的是什么。答：报表我是有用过XXX工具（帆软report），有时候会帮同事做一些行式报表和交叉报表，我不太理解你说的这个分析指的是哪一块的，因为涉及到很多的一个分析工作，比如需求分析，就是拿到需求文档，分析需要哪些数据每一集公式怎么用sql实现，或者报表的数据展示有误，怎么分析问题出现在哪里，再或者从图表中获取一些数据变化的结论，做一个决策或者预测的

分析。你想要了解哪一块的内容呢。

需求分析答：去主场前，拿到需求文档，会先看自己的那一块内容，熟悉大致的指标，计算公式，有关的数据项，以及可能的表，公式怎么用sql简单的实现它。

# 2. 报表数据有误

答：大片有误的一般很少，因为会做测试，一般都是个别数据有问题，我记得之前出现过一些问题，XX 项目中，业务突然找到我说报表上面某个产品的总交易额不对了，少了 3000，我的想法是，代码一般不会有问题，如果有问题，那么不可能几百个产品就一个出问题。所以一般都是数据的问题，但是我从报表上看这个明细数据，自己加一遍是对的，但是去数据库查询就发现少了一条记录，后来发现这条记录是业务通过 excel 补录的，产品编号后面有一个空格，呆滞关联的时候关联不上，后来去掉了空格，在代码中加上了 trim。也通过领导和业务去反馈，上传 excel 文件注意前后不要有空格。

# 3. 一张千万数据量的表和一张抽取的新增的8万数据量的表，在不同的层里，怎么合并两张表？用sql方法？

如果千万的表格有分区，那么直接读取数据全量写入到对应的例如今天的分区中；如果是个普通的表格，那么可以使用 insert into table 进行数据的追加 select * from 库名. 表名

# 4. 如何快速定位数据发散的表？

1 查看表的大小：可以通过查询表的行数和列数来了解表的大小。如果表的大小远远超过其他表，则可能是存在数据发散的表。  
2 检查重复值：在表中查找重复值可以帮助确定是否存在数据发散问题。如果存在大量重复的记录，则表中可能存在数据冗余或误操作导致的重复数据。  
3. 分析字段分布：对于一些关键字段，比如日期、金额等，可以通过观察字段取值范围、分布情况等来判断是否存在数据发散的情况。如果某个字段的取值范围极大，或者分布异常，则可能需要进一步检查数据。  
4. 检查关联表：如果存在多个表之间的关联关系，可以检查其他表中的数据是否与该表中的数据相符合。如果其他表中的数据出现了异常，那么该表也可能存在数据发散的问题

# 5. 数据漂移

# (1) 1.1 定义

源数据抽取到ods层中，同一个业务日期数据中包含前一天或者后一天凌晨附近的数据或者丢失当天的变更数据。

# (2) 1.2 数据漂移出现的原因

通常落地数仓的 ODS 表会按时间切分做分区存储，实际上往往由于时间戳字段的准确性问题导致发生数据漂移。通常有四类时间戳：

modified_time: 数据库记录某条数据更新的时间。

log_time: 数据库日志记录某条数据更新的时间。

proc_time:具体业务过程发生时间。

extract_time: 数据记录被抽取时间。

1)同一条记录的数据抽取时间 extract_time 明显是晚于另外三个时间的,如果用这个字段切分,ODS 某个分区中的数据会包含前一天末尾的数据,并丢失当天末尾的数据。  
(2) 如果用数据库记录的更新时间 modified_time，前台业务系统手工订正数据时可能会遗忘同步更新该时间，导致该抽取的数据被遗漏掉。  
3）另外，由于网络或者系统压力问题，log_time或者modified_time可能会晚于proc_time，

导致数据漂移。

4) 如果我们直接使用 proc_time 时间进行切分, 这种情况仅仅对包含一个业务过程的 ODS 表有效果, 如果该表每条记录需要存储多个业务过程, 则用 proc_time 切分会丢失其他发生在当天的业务过程记录。

# (3) 1.3 处理数据漂移的方式

# ① 1. 多获取后一天的数据

既然很难解决数据漂移的问题，那么就在 ODS 每个时间分区中向前、向后多冗余一些数据，保证数据只会多不会少，而具体的数据切分让下游根据自身不同的业务场景用不同的业务时间 proc_time 来限制。但是这种方式会有一些数据误差，例如一个订单是当天支付的，但是第二天凌晨申请退款关闭了该订单，那么这条记录的订单状态会被更新，下游在统计支付订单状态时会出现错误。

# ② 2. 通过多个时间戳字段限制时间来获取相对准确的数据

(1) 首先根据 log-time 分别冗余前一天最后 15 分钟的数据和后一天凌晨开始 15 分钟数据, 并用 modified_time 过滤非当天数据, 确保数据不会因为系统问题而被遗漏  
2）然后根据log_time获取后一天15分钟的数据；针对此数据按照主键根据log_time做升序排列去重，因为我们要获取的是最接近当天记录变化的数据（数据库日志将保留所有变化的数据，但是落地到ODS表的是根据主键去重获取的最后状态的数据）  
3）最后将前两步结果数据做全外连接，通过限制业务时间proc_time来获取我们所需要的数据。

# 6. 每天同步的表有哪些，增量数据有哪些？

一般事实表都是增量的。比如借记卡的开户信息表，贷记卡合同信息表，交易信息表，存款信息表等。每天同步新开户，新合同，新的交易流水，新存款等。

# 7. 存储过程写的多吗, 一个版本写多少个, 一个有多少行, 写过最复杂的存储过程是什么, 存储过程中怎么记录日志, 找出异常?

存储过程中怎么记录日志，从何而来呢？存储过程又怎么管理呢，我主要就是写存储过程的。一个版本大概要写2,3个，每个7,8百行，最复杂的是统计交响，我主要就是写存储过程的。一个版本大概要写2,3个，每个7,8百行，最复杂的是统计交响，我主要就是写存储过程的。一个版本大概要写2,3个，每个7,8百行，最复杂的是统计交响，我主要就是写存储过程的。一个版本大概要写2,3个，每个7,8百行，最复杂的是统计交响，我主要就

# 8. 说说项目中做过的指标有哪些？用到的表有哪些？

8. 说说项目中做过的指标有哪些？用对数统计、加法统计、求和法统计。客户余额，日均余额统计。表有：客户交易明细和客户信息、币种、机构、贷款合同等表存款账户交易月汇总指标（根据存款账户不同的交易类型来统计交易笔数和交易金额）。表有：客户交易明细和客户信息、币种、机构等表。

9. 决策+预测分析答：（根据个人情况回答，没有就没有）这快没有做过，一般做开发多点，但是最近面试也有

被问到过，之前有个面试官问我，销售额下降  $50\%$  ，可能是哪些原因导致的。我的理解就是可能和疫情，同行竞争（新开的同行），之前的活动，经济市场。

# 10. 你们是如何获取客户的消费记录的？打印流水？

答：客户的消费记录是没有的（看不到的），只能看交易记录。

# 11. 知道消费类型要怎么用？怎么分析？

注：决策分析

判断每个消费类型的交易量和金额，也可以判断每个地区主要的一个消费类型，某些地区出产服装，制定一些营销的策略。也可以判断如果某些消费类型的金额和交易量都可以，比较多，也可以制定一些相应的策略。

# 12. 某个月的销量突然下降，从哪些方面去分析？

答：我的理解就是可能和疫情，同行竞争（新开的同行），之前的活动，经济市场。

# 数据治理

# 1、什么是数据治理？

答：数据治理是一种管理和控制数据的过程，旨在确保数据质量、保护数据安全、合规性和合理使用数据。它包括制定和执行数据管理策略、规则和流程，监控数据质量并进行纠正，确保数据安全和合规性，以及定义数据所有权和访问权限。数据治理是组织内数据管理的关键活动，帮助组织有效地使用数据，支持业务决策和运营活动。

# 2、数据治理的具体工作内容主要包括哪几个方面？（以下内容不用全部答）

1. 数据策略与规划：制定数据治理策略和规划，明确组织对数据的价值和需求，确定数据治理的目标和准则。  
2. 数据质量管理：确保数据的准确性、完整性、一致性和可靠性，通过数据清洗、数据标准化、数据验证等手段提升数据质量。  
3. 元数据管理：收集、记录和管理数据的元数据信息，包括数据定义、数据结构和数据关系等，以便更好地理解和使用数据。  
4. 数据安全与隐私保护：确保数据的安全性和隐私保护，包括制定数据安全政策、权限管理、数据分类和敏感数据保护等。

5. 数据架构与模型管理：设计和管理数据架构，包括逻辑数据模型、物理数据模型等，确保数据的结构和组织能够满足业务需求。  
6. 数据访问与共享管理：管理数据的访问权限，确保只有授权的人员可以访问和使用数据，同时促进数据的共享和交流。  
7. 数据治理组织与流程：建立数据治理组织和流程，明确数据治理的责任和流程，确保数据治理工作的执行和持续改进。  
8. 数据文档与沟通：编写数据文档，记录数据相关信息，以及进行数据沟通和交流，确保数据沟通、治理的透明和可理解性。  
9. 数据监控与治理评估：监控数据的使用情况和建设情况，进行数据治理评估和改进，确保数据治理的有效性和持续性。

总之，数据治理的工作内容涵盖了数据策略规划、数据质量管理、元数据管理、数据安全与隐私保护、数据架构与模型管理、数据访问与共享管理、数据治理组织与流程、数据文档与沟通、数据监控与治理评估等多个方面。这些工作内容旨在确保数据的质量和安全，并促进数据的有效管理和利用，实现组织的数据驱动决策和价值创造。

# 资产负债问题

# 1. 讲一下资产负债表，利润表，现金流量表的内容

注：此类问题主要针对会计财务管理类专业的同学提问较多。

注：此类问题主要是针对会计财务管理类、主营业务类、其他业务类等会计科目进行的。  
答：（1）银行资产负债表中主要有：现金，银行存款，存放中央银行款项，存放同业，结算备付金，存出保证金，交易性金融资产，买入返售金融资产，应收账款，应收股利，应收利息。（2）利润表中包含：营业收入，营业成本，税金及附加，销售费用，财务费用，管理费用，资产减值损失，公允价值变动损益，投资收益，营业外收入，营业内收入，  
(3）现金流量表中包含：经营活动，投资活动，筹资活动产生的现金流量及其各项目流入，流出的总额和净额。补充资料包括经营活动现金流量和投资现金流量。

# 2. 会计科目类别 举一个相关的做账的例子

2. 会计科目类别：负债类

3. 公司的电脑，桌子算是什么科目，公司每个月发的工资和别人欠公司的钱是记在什么科目？应收账款算资产还是负债？

答：电脑桌子是固定资产，发的工资，计提的时候增加放管理费用，增加应付职工薪酬，发的时候减少应付职工薪酬，减少银行存款。

# 信贷业务问题

# 1. 五级分类是什么？以及区分

答：正常，关注、次级、可疑、损失。

（1）正常贷款、借款人能够履行合同，能正常还本付息，不存在任何影响贷款本息及全额偿还能力的因素，银行对借款人按时足额偿还贷款本息有充分把握。  
(2）关注贷款，尽管借款人有能力还贷款利息，但存在一些可能偿还产生不利影响的因素，如这些因素继续下去，借款人的偿还能力受到影响，贷款损失的概率不会超过  $5\%$  。（3）次级贷款，借款人的还款能力出现明显问题，完全依靠其正常营业收入无法足额还贷款利息，需要通过处分资产或对外融资乃至执行抵押担保来还款利息。贷款损失的概率在  $30 - 50\%$  
（4）可疑贷款，借款人无法足额偿还贷款利息，即使执行抵押或担保，也肯定要造成一部分损失，只是因为存在借款人重组，兼并、合并、抵押物处理和未决诉讼等待定因素，损失金额的多少还不能确定，贷款损失的概率在  $50 - 75\%$  
(5) 损失贷款, 指贷款人已无偿还本息的可能, 无论采取什么措施和履行什么程序, 贷款都注定要损失。或者虽然能收回极少部分, 但其价值也是微乎其微, 从银行的角度看, 也没有意义,而且必要再将其作为银行资产在账目上保留下来, 对于这类贷款在履行了必要的法律程序之后应当立即予以注销, 其贷款损失的概率在  $75 - 00\%$  。

(关注、次级、可疑和损失类分别对应着逾期90天、180天、270天、360天)

# 2. 贷款核销是什么概念，贷后的话，贷款核销怎么处理？

答：贷款核销是“呆账贷款核销”的简称，这种行为是将贷款审核后销账，通常一笔贷款在未还清的情况下，经银行的核销部门认定确实无法将剩余贷款本金及利息收回时，采取的将贷款剥离至银行所属的资产处置中心，而不由经办行继续找借款人催收还款的情形成为核销。不是任何一笔不良贷款都可以核销，必须符合一定条件，央行对贷款核销有着严格的法律程序，必须时经过多努力认定人没有任何收回或者减少的可能性，已经成为呆账的不良贷款，当该笔贷款核销之后，会给予一定补偿，因此这种贷款分主体一般是享受国家政策扶植的国有企业。最后，并不是核销就是对银行的损失，这样做可以使银行不必总挂着不良贷款的包袱，可以冲掉一些死账，让银行整体的不良贷款率下降，这样银行的业绩看上去就会好看一些。

# 3. 抵押贷款和质押贷款的区别

打：我个人的理解，抵押贷的基本都是不动产，比如房子车子，质押是属于一些动产，比如股票，理财产品之类的。

# 4. 了解什么样的贷款业务及其种类

答：贷款主要分个人和对公，个人贷款分三大类。房贷，消费贷、经营贷。消费贷又分为教育贷，消费贷，医疗贷，旅游贷等等。对公贷款主要有固贷和流贷，就是固定资产贷款和流动资金贷款。

# 5. 合同、借据和抵质押合同（担保合同）之间的关系

答：合同是事先签订的，借据是放款时产生的，分批借款就有多比借据，抵质押合同或者担保合同是对借款合同中还款义务的保证。

# 6. 比如你签一笔合同, 什么时候回生成一笔借据, 那你的担保合同又是什么, 是挂在合同上还是挂在借据上, 把这个链路说一下, 借据和合同是一对多还是多对一?

答：合同是先签的。实际放款前才会有借据，可能分批借款就会有多笔借据。担保合同是对借款合同中还款义务的保证。合同是一，借据是多。

# 7. 逾期率是怎么算的？

答：逾期贷款率指的是逾期的贷款占全部贷款的比例，他是用于反映贷款按期归还的情况的，它是从是否按期还款的角度来反映贷款使用的效益情况以及资产风险的程序。加快逾期贷款率主要是为例促进银行对逾期贷款尽快的妥善处理。逾期贷款率的计算公式，可以按照期末余额和平均余额来计算。公式：期末逾期贷款率=期末逾期贷款的余额/期末贷款的总余额；平均逾期贷款率=全期逾期贷款的平均余额/全期贷款平均余额

# 8. 你们计算不良贷款率的最终结果只有一条数据么？

答：这个是根据行方的业务需求来定的，在这项目中，我主要负责当事人，区域和产品这几个答：这个是根据行方的业务需求来定的，在这项目中，我主要负责当事人，区域和产品这几个主题域，每个主题域都有相应的不良贷款率，比如当事人主题域，就有不同类型的客户的不良贷款率，像20—35高学历人群、世界500强员工、高龄女妇女等；区域的话，就比较好理解了，贷款率，像20—35高学历人群、世界500强员工、高龄女妇女等；区域的话，就比较好理解了，就是按照地区不同来就算各个地区的不良款率，一般以市级作为最小统计力度；产品的话，也就是按照地区不同来算，比如学历贷、白领贷、经营贷等等，可以统计每个产品的不良率，来是按照产品种类来计算，比如学历贷、白领贷、经营贷等等，可以统计每个产品的不良率，来是按照产品种类来计算，比如学历贷、白领贷、经营贷等等，可以统计每个产品的不良率，来是按照产品种类来计算，比如学历贷、白领贷、经营贷等等，可以统计每个产品的不良率，来是按照产品种类来计算，比如学历贷、白领贷、经营贷等等，可以统计每个产品的不良率，来是

# 流程问题

# 1. 理财业务数据来源，数据来源除了业务系统还有哪些？

答：因为客户方这边的理财产品主要通过核心系统和TA系统两个渠道销售，所以大部分的数据都来自于核心系统和TA系统，也有其他的excel补录的数据，比如活期宝产品是通过核心系统销售的每日开放的现金管理类产品，但该产品不是通过设置理财产品参数表的形式发售，所以部分报表需要手工录入，还有对公手工上存产品是通过分行直接手工上存客户理财投资叙做的理财产品。此类产品也不在核心系统中设置参数，所以也需要手工录入。

# 2. 选一个流程，遇到的问题，怎么解决

注：最好能够确认是什么流程，明确好，有很多种的流程，需求评审中的，还有开发中的，还有上线中的，

答：那我说一下，之前上线的时候，碰到一个问题，在开发环境，测试环境都没有问题，上线后，按照常规流程进行存储过程编译和跑数，最后在报表上就发现有一个项目的余额就为空，没有数字，当时也是立马和测试沟通、跟领导汇报，然后查找问题，后来发现上线时业务导入了一个项目明细信息表，这个项目的项目编号后面有一个空格，导致无法关联上，就临时修改了生产环境中的项目编号，重跑了存储过程，后面也在代码中加入了trim来避免以后出现这种情况，也有和领导提建议，就是建议业务或者运维那边向数据库中导入excel数据的时候检查一下前后是否有空格。

# 3. 接到需求你们项目组是怎么做的？

打：因为我们是这种第三方的数据系统服务公司，一般小的需求，都是客户方业务和技术加上我们这边的经理和开发人员简单开会，了解需求内容和最终形式，然后我们内部分工，简单说明实现方式和可行性，进行一个脚本开发，完了以后进行一个测试，和客户方进行对数，验证完成，然后在月底的版本变更中上线。大的需求，就是我们经理这边评估，和商务那边对接的，是看这期做还是下一期做，决定好了以后流程都是一样的。

做过这么多项目，有和甲方工作人员沟通过吗？什么情况下会和他们沟通？有问题怎么沟通的？

答：和业务人员沟通的话，还是有很多的，像平时需求串讲，反串讲，开发过程中碰到的一些问题，比如逻辑不清楚，公式问题，数据值缺失，枚举值缺失，等等一系列的问题，都会有招业务沟通。具体的要看问题的紧急程度，如果不是大的问题，或者不太着急的问题，一般我们都会每周联系相关业务开会讨论，解决本周碰到的问题。

# 4. 整理表的时候用什么方式去解决

答：我一般都是使用思维导图或者excel，将这些表分类，按照不同主题去存放，主要还是熟悉表的记录的内容和一些关键的指标数据，思维导图多一些，因为可以加上表间的关系。如果时间充足，可以用powerdesinger做五路模型，会更清楚一些，但是一般时间都不允许。

# 5. 项目的取数口径

注：数据最终格式，数据的公式一加工的逻辑

答：取数的口径一般都是和客户方的业务人员和技术人员一起开会沟通得出的。一般都是我们根据需求文档制作需要的表结构，然后由客户方的业务和技术人员进行一个填写取数口径，最终一起开会讨论可行方案（每个字段能否从系统中取得，能否都有对应的一个关系，证件类型能否和最终版个结果表的数值对上）

# 6. mapping 文档是什么？写过那些 mapping 文档

答：mapping 文档主要是记录目标表和来源系统，表、字段的关系，也包括一些指标的计算公式，还有枚举值的对应关系，编码和码值的对应关系，还有确认的人员、部门、时间

# 7. 怎么设计表的，怎么设计 mapping 的，表是谁设计的

这个主要是设计人员设计都，设计好后目标表的字段以及来源于哪些表

# 开发过程

# 1. 写代码中的经验

答：我的理解就是，一定要先弄明白需求文档或者设计文档，因为需求文档是客户给的，设计文档有时候不一定是自己写的，第二个就是开发之前，想好实现的逻辑顺序，有没有更好的方案，也会通过设计评审，其实也就是几个痛死互相卡卡实现的逻辑，第三个开发中，保持细心，布套初夏一些因为粗心犯的错误，少些单词或者单词拼错，注意代码的格式，别名，做规范一些，还有歇一歇注释，对后面代码的审阅也会有帮助

# 2. 回想一下你写存储过程，遇到什么问题？

2. 回想一下你写存储过程，遇到什么问题？答：这个问题有很多方面的，主要包括需求文档或者设计文档有问题，公式变更什么的，还有就是表或者字段找不到的，还有枚举值对不上的，设计文档中有5类，但实际有7类，这些都是开发过程中碰到的，当然还有一些查询语句在试运行的时候很慢，也会做一些优化。

# 3. 存储过程好用吗？怎么好用？用于过哪些业务场景？

3. 存储过程好用吗？怎么好用？用于这些业务的工具，包括哪些功能？答：好用。可以封装一些查询语句或者DML语句，保存在数据库中，方便以后的调用和维护。答：好用。可以封装一些查询语句或者DML语句，保存在数据库中，方便以后的调用和维护。我们一般用于同步数据，就是将几个表的数据关联起来，经过一些聚合或者转换得到一个结果。我们一般用于同步数据，就是将几个表的数据关联起来，经过一些聚合或者转换得到一个结果。

# 4. 你们项目的开发流程大致是怎么样的？

答：首先会有需求分析，主要是客户方的业务和技术人员还有我们项目成员，澄清需求的内容，然后会有反串讲，主要是我们去讲述对于需求的一些理解，确保我们的理解和客户方是一样的；然后进入设计阶段，主要完成 mapping 文档和简单的开发方案，就是针对每个需求的一个逻辑上的实现步骤，然后进行一个开发，测试，最后会给客户方的业务进行测试，没有问题就可以交付上线。

# 测试类

# 1. 有没有测试过，你们的自测一般是怎么测的？

答：有的，自测主要是测试一下数据的量和数据值的准确性。比如计算部门当月利润，那么有几个部门就应该有几条记录，首先记录数要对上，其次我会看一下来源表，筛选某个部门的收入和支出，然后自己手工加一下，看看数据的值和我用 SQL 计算出来的是否相同，一般都会多挑几个验证一下，其他的表也都是参考这个思路去做的。

# 2. 平时开发的数据指标测试怎么做，一般用多大的数据量测试？

答：（测试参考第8题），数据量的话，一般我饿开发环境都有数据，但是不多，只有几十万的数据，可能交易表多一些，一百来万。

# 3. 数据补录

数据质量检查过程中会经常发现源系统的字段缺失，导致数据报送不符合规范，影响数据分析结果，那往往需要业务人员进行补录，那补录最好的系统当然是在源系统，但往往许多系统设计时并没有考虑补录数据的需求，因此可以在全行建立一个补录系统，通过配置需要补录的字段、格式、检查规则以及后台系统及数据库自动产生补录界面，该补录界面可以被各系统进行集成，以便在各系统进行数据补录。

那对于一些数据应用系统的结果数据，如反洗钱上报的交易对手缺失，可以在数据应用系统中补录，对于补录的数据如有必要也可以回传给数据仓库进行数据补充，以便其它系统使用。

# 4. 数据质量校验分为以下方面：

1. 完整性

完整性是指数据的记录和信息是否完整、不缺失。数据的缺失包括数据记录的缺失（表行数异常）和记录中某字段信息的缺失（字段出现空值），检核对象是否存在为空。

2. 准确性

准确性是指数据记录中信息和数据是否准确、不存在错误或异常。例如, 出现负数, 答非所问数据, 特殊符号, 则明显是错误数据。

3. 一致性

对于不同的业务流程和节点，同一份数据必须保持一致性。不可以同一个意义的数据，出现两种形式。

4. 有效性

是否满足长度约束，值域约束，检核对象的码值是否在对应的码表内，取值范围，检查对象取值是否在预定的范围内。

5.及时性

及时性主要体现在最终应用层的数据可以及时产出。为保证及时性，需要确保整条数据加工链路上的每个环节都可以及时产出数据。

6.规范性

有没有遵循语法规则定义，时间格式定义为yyyy-MM-dd。

7. 唯一性

唯一性约束，同一客观实体在表中是否重复记录，是否出现两次以上。

# 5. 数据对不上怎么解决？

答：一般来说，我这边还比较少这种情况。因为准备工作做得比较好，当然也会有一些问题，有时候可能只有一两条对不上，有时候可能有大面积数据对不上，我简单说一下，一两条对不上，我的经验是数据本身可能有问题，大部分情况检查一下都是因为管理啊字段前后有空格，没关联上，或者有重复数据，导致发散了。大面积对不上，一般都是关联条件/筛选条件写错了或者多了少了，我记得之前还有一个事情，要筛选xx业务的数据，同事告诉我这个类型的业务编号是030，结果后来我怎么都对不上，最后发现这个编码是0030

# 6. 如何快速定位数据发散的表？如何排除，收敛动作？

A: 通过查询表的关联字段, group by 关联字段 having count (关联字段) >1 , 进行统计, 就可以快速判定是否有一对多的发散情况。

可以快速判定是否有一对多的发散性。  
收敛就是：如果你用 group by 关联字段 进行统计，然后统计金额的话，就会 group by 字段。  
相同的情况下多条记录的金额就会加在一起，合成一条记录。

# 备注：如何避免思路是

备注：如何避免思路走 1、表关联的条件对不对：工作中的 SQL 查询会用到很多表,很多表甚至一开始是陌生的,本 来你以为两个表的关联条件是用户 UID,实际上是订单 ID,or 用户 UID+日期 or 用户 UID+其他 条件。如果一开始表之间的关联条件不完整或者是错误的,这就会导致数据发散的现象。因此, 我们一开始要确定好表之间关联的正确逻辑。

2、表的数据是不是唯一的：如果确定了表关联的条件无效，在群组中，表数据会出现在所有表中。这也会出现一对多关联的情况。这个时候，可以把有重复数据的表先 group by，再去关联。

7. UT测试是什么，如何做，谁来做，你负责什么？答：UT测试是unit_test，单元测试，主要是测试当前脚本，谁开发测试，我就负责自己开发的脚本测试。

# 8. ST, UAT

答：ST是系统测试，system test，主要是测试人员对整个系统的一个些事，包括主流程的测试，各个脚本的测试，也包扣性能测试；UAT测试，主要是业务人员的测试，主要关注数据的准确性，也是测试阶段的最终确认环节。

# 上线类

# 1. 写的程序上线完了之后怎么去跑？

答：我们程序一般上线后，一般都是先跑维度表的数据，再跑事实表的数据。

# 2. 有没有关注跑批作业时长？

答：这个是有关注过的，尤其是项目测试期间和上线运行前期，跑批时间都是设定在凌晨12点至早上6点期间，一次，一般在10分-20分钟，得具体看数据量大小。

# 3. 你们项目上线怎么调度？

答：我们用 kettle 做抽数和脚本的调度，作业我们是在 windows 上进行编辑，然后上传到 kettle 的 linux 服务器，使用 crontab 结合 kettle 的指令进行数据的定时调度。

# 4. 上线的时候你在做什么？

答：上线的时候，我会再检查上线内容，然后等候运维那边的上线结果，以及上线后的试运行是否通过，有问题都会随时沟通，尽快解决，不耽误上线，当然我们上线的时候，基本没有出过问题。

# 5. 上线之后你们遇到了什么问题，能举例说下吗？

因为准备工作做的比较好，一般都不是代码问题，其他的问题，我想想，嗯...就是在之前的项目中也遇到过，就是代码也没有问题，测试没有问题，上线之后也没有问题，单过了一段时间，也没有报错，但客户反应说最新日期的结果表没有数据，之前的都有，刚开始我以为是调度出了问题，但通过查看日志，调度是有正常再跑的，后面通过结果表反推，一层层去定位，发现是某些时间的数据量突然增加，导致上游的还在跑数过程中，下游已经在开始跑了，导致最终的结果表为空，问题发现了，更改了调度间隔时间，由原来的半小时，改为1个小时间隔，解决了这个问题。

# 主要职责

# 1. 你在项目中负责做哪些内容？

注：根据你简历上的内容来说主要的内容，简历上可能写了 ETL，或者报表的，其他的都是附加的。

答：我在这个项目中，主要负责xx块的数据处理，就是编写SQL脚本（存储过程），（当然你写了ETL或者报表的，也可以加上，）这是一个主要的代码部门工作，还有就是沟通方面的，比如需求串讲，和反串讲。设计方面的，需要写mapping文档，写开发方案，就是每个需求的逻辑实现步骤。还有开发过程中出现的一些数据问题和字段映射问题，都要找业务或者我们的精力去沟通，还有代码的测试等等。

# 2. 开发负责哪个模板的开发？

注：自己要准备一些主题（或者叫做模板），简单的说就可以，但是要补充，我们的开发工作是主要的，但是不是全部的。

答: 我负责 xx 模块的开发, 这是我的一个本职工作, 但是不是全部的, 还有一些其他的类似沟通协调, 或者编写文档类的工作。

# 3.2 平时具体负责哪块的内容，答DW-ADS层这部分工作追问，负责需求调研还是开发的工作？

答：（回答负责的内容参考问题），需求调研和开发都会做一些，当然开发是本职工作，但是需要的确认和梳理，也还是要做的。毕竟不理解需求，就没办法开发。

# 4. 遇到数据质量问题你是怎么处理的？

4. 遇到数据质量问题你是怎么处理的?答: 一般都是和同事确认一下, 是不是真的是数据质量问题, 还是我个人问题, 然后和经理反馈, 具体看你是不是很着急, 不急情况下, 一般我们都是汇总问题之后, 和业务方去开会沟通, 馈, 具体看你是不是很着急, 不急情况下, 一般我们都是汇总问题之后, 和业务方去开会沟通, 怎么处理这个数据质量问题; 如果比较急, 或者质量问题比较明显, 设计范围不广, 都是可以直接找业务去确认问题, 然后得到一个解决方案的。大部分的解决方案都是通过业务重新给一个excel表格, 进行数据的一个补录或者修改。

5. 假设前端展示的数据是 9800 , 但是业务那边说是 00 , 这个问题是怎么处理? 答: 一般来说, 上到生产环境之后, 代码一般都是没有出现特别大的问题, 出现数据不对, 也只是个别数据出现问题, 一般我都考虑是数据质量本身除了什么问题。之前工作中也出现过, 只是个别数据出现问题, 一般我都考虑是数据质量本身除了什么问题。之前工作中也出现过, 只是个别数据出现问题, 一般我都考虑是数据质量本身除了什么问题。之前工作中也出现过, 只是个别数据出现问题, 一般我都考虑是数据质量本身除了什么问题。之前工作中也出现过, 只是个别数据出现问题, 一般我都考虑是数据质量本身除了什么问题。之前工作中也出现过, 只是个別数据出现问题, 一般我都考虑是数据质量本身除了什么问题。之前工作中也出现过, 只是个別数据出现问题, 一般我都考虑是数据质量本身除了什么问题。之前工作中也出现过, 只是个別数据出现问题, 一般我都考虑是数据质量本身除了什么问题。之前工作中也出现过, 只是个別数据出现问题, 一般我都考虑是数据质量本身除了什么问题。

# 6. 项目数据量一般是多少？

注：（中小型商业银行，每个月交易应该在千万级别，3000w-5000w，所以大的表有几亿的。客户表，在百万级，接近1000w。账户表就会更多。但是这是生产的数据，总共的表有多少张，常用表在60-80张，总共在300-500张）

# 7. 这个项目数据量有多大，用到多少张表？

注：第一种：告诉他整个项目一共有多少数据量（不好判断），所以说第二种：整个项目我没算过，但是大的表有几亿的，晓得表有几十万的都有。总共有常用的表可能就几十个。总共有大概三万白张表吧，具体也没有数过。

# 8. 工作的表数据有多少，你们所有表的数据量有多少，日增数据量多少，写的sql一般需要爬多久？

注：参考问题7,8,日增数据量大概在百万级别(100-300w都是可以的),SQL的运行时间,在开发环境都是很快的,几秒钟,在生产环境一般也就十分钟左右,慢一点的可能就半小时左右。

# 9. 问银行项目，DW和DM用到的什么表？

注：参考十个主题,每个主题准备 3-5 个表基本就可以了。

# 10. 讲一下增量表，全量表和拉链表。

答：增量表是指每天新增昨天的数据，或者每个月新增上个月的数据，主要适用于一些比较大的交易表，客户信息表，账户表；全量表就是每天能清空目标表，再将所有历史数据插入目标表，主要适用于一些比较小的表，或者不太会变动的维度表，基本上百万以下的表，都是全量表；拉链表就是会记录所有的历史变化数据，会有数据的有效标识，1是0否，会有数据的有限期限，开始时间和结束时间，主要适用于一些会做变更的数据记录，比如说产品信息，或者客户信息，订单信息等等。

# 11. 每一层数据同步如何同步的？

答：都是通过存储过程，封装 SQL 语句，将对应的查询结果集插入到下一层的表中。我们的数仓各个层都是在同一个数据库，所以比较方便。

# 12. 增量表、全量表优缺点。

答：增量表优点：效率高；缺点：历史数据（上个周期以前的）发生变更了。会不方便同步的。全量表优点：对于历史数据发生变更，也能在目标表提现出来，缺点是如果数据量比较大，速

# 13.怎么判定表全量/增量以及条件

答：如果这个表数据量比较大，每天新增有很多，比如交易表，适合增量的，如果数据量比较小，历史的数据也会变更，比如说产品信息表，产品账户表，适合做全量的。

# 14. 项目中哪些表是增量更新，哪些表全量更新？

答：事实表大部分都是增量的，比如用户的申请表，客户消费记录表，账单表，还款表，逾期信息表，等等，维度表基本都是全量的，比如说产品表，部门表，利率表等等都是全量的。

# 15. 有没有了解过数据质量

答：我对于数据质量的理解，有没有空值，异常值，重复记录。有没有缺失表或字段，确实挂架信息，比如确实交易时间，缺少客户身份证号码，那么这些在工作里都是会碰到，一般就找业务补录数据。

# 16. 项目中碰到的问题怎么解决？

答：我碰到的，比如需求分析因为一些业务专业词听不懂，导致整个会议听不太懂的，一般都是下去恶补专业名字，争取搞懂需求，不懂得也会拉着项目经理帮我梳理这些内容。Mapping中也有一些数据取不到的，业务系统中也没有，只能通过业务excel补录，开发中也会发现有些表，字段没有，或者关键信息缺失，缺少交易时间，缺少客户身份证号码；上线以后，有时候业务也回来找某个值不太对，参考前面报表中数值不对的问题。

# 开放性问题

离职原因，为什么想来上海（或者其他城市）

（自己整理最少2个）

上份薪资是多少？（根据年限来。不能乱说。跳槽一般涨薪1-2k）

上份薪资是多少？（根据平根来。不能说说。）  
你看我不是相关专业。是不是培训出来的。（我是有同学朋友做这块。他带我入行的。我自己也很喜欢这方面的工作。所以就一直做这块了）

现在有收到offer了么。(有的。目前面了1-2家了。有的还在等二面或者有一个在走流程。) 对于加班你有什么看法。(加班是可以接受的。之前我们项目组。需要上线的时候也经常加班)

你最快什么时候能入职。（随时可以入职的，3天到一周内）

你最快什么时候能入职。（随时可以入职的，你可以决定时间。）你的优势是什么。（工作中的优势，例如沟通，细心，自律，时间观念强，对行业的热爱。专业的优势等等。）

你未来的职位规划是什么。（后面想往管理层发现或者往技术岗更高层的职位）

接受加班吗？

为什么会选择来这里工作？
